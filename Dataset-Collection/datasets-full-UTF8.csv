Dataset,Paper Title,Year,Publisher,Paper Link,Project Link,Description,Size & Content,Indoor/Outdoor,Real/Synthetic,Modalities,No. Classes,Tasks,No. Papers,No. Benchmarks,Column1,checked?
12 Scenes,Learning to Navigate the Energy Landscape,2016,3DV,https://arxiv.org/pdf/1603.05772v1,https//graphics.stanford.edu/projects/reloc/,The dataset contains RGB-D data of 4 large scenes comprising a total of 12 rooms,4 scenes; 12 Rooms,indoor,real,iPad RBG camera; Structure.io depth sensor,0,RGB Relocalization; Hand Pose Estimation; Image Retrieval,38,0,,c
2D-3D-S (Stanford 2D-3D - Semantics Dataset),Joint 2D-3D-Semantic Data for Indoor Scene Understanding,2017,arXiv,https://arxiv.org/pdf/1702.01105v2,http//buildingparser.stanford.edu/dataset.html,"The 2D-3D-S dataset provides a variety of mutually registered modalities from 2D; 2.5D and 3D domains, with instance-level semantic and geometric annotations",6000 m2; 6 indoor areas; 3 different buildings; over 70;000 RGB images + depths + surface normals; semantic annotations; global XYZ images; camera information; registered raw and semantically annotated 3D meshes and point clouds,indoor,real,Matterport Camera (3 Structured-Light sensors) for RGB-D,13,Semantic Segmentation; Depth Estimation; Self-Supervised Learning; Visual Navigation…,141,8,,c
3D60 / 360D,OmniDepth: Dense Depth Estimation for Indoors Spherical Panoramas,2018,ECCV,https://arxiv.org/pdf/1807.09620v1,https//vcl3d.hub.io/3D60/,High quality 360o dataset with ground truth depth annotations,94098 renders; 23524 unique viewpoints; depth annotations,indoor,synthetic,using 2 computer generated datasets (SunCG + SceneNet) and two realistic ones acquired by scanning indoor buildings (Stanford2D3D + Matterport3D),0,Depth Estimation; Monocular Depth Estimation,17,0,,c
3D AffordanceNet,3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding,2021,CVPR,https://arxiv.org/pdf/2103.16397v2,https//andlollipopde.hub.io/3D-AffordanceNet/#/,3D AffordanceNet is a dataset of 23k shapes for visual affordance (= the ability to understand the ways to interact with objects from visual cues),56307 well-defined affordance information annotations for 22949 shapes covering 18 affordance classes and 23 semantic object categories,indoor,real,Raw 3D shape data are collected from the shapes in PartNet; 42 professional annotators for annotations,23,Affordance Detection,12,1,,c
3DCSR (3D cross-source point cloud registration dataset),A comprehensive survey on point cloud registration,2021,arXiv,https://arxiv.org/pdf/2103.02690v2,https//multimediauts.org/3D_data_for_registration/,Cross-source point cloud dataset for registration task,2 kinds of cross-source point cloud: (1) Kinect and Lidar; 19 scenes; 165 pairs of cross-source point clouds; (2) Kinect and 3D reconstruction (iPhone RGB camera): 2 scenes; 37 pairs of cross-source point clouds ,indoor,real,"Kinect, LiDAR; iPhone RGB camera",0,Point Cloud Registration,8,0,,c
3D-FRONT (3D Furnished Rooms with layOuts and semaNTics),3D-FRONT: 3D Furnished Rooms with layOuts and semaNTics,2021,ICCV,https://arxiv.org/pdf/2011.09127,https//tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset#,"large-scale, and comprehensive repository of synthetic indoor scenes highlighted by professionally designed layouts and a large number of rooms populated by high-quality textured 3D models with style compatibility",18968 rooms; 13151 furniture 3D objects with highquality textures,indoor,synthetic,built on about 60K professionally designed houses and 1M 3D CAD meshes,49,Interior Scene Synthesis; Texture Synthesis,7,0,,c
3D-FUTURE (3D FUrniture shape with TextURE),3D-FUTURE: 3D Furniture shape with TextURE,2021,IJCV,https://arxiv.org/pdf/2009.09633v1,https//tianchi.aliyun.com/specials/promotion/alibaba-3d-future,3D dataset that contains photo-realistic synthetic images involving industrial 3D CAD shapes of furniture with high-resolution informative textures developed by professional designers,20240 synthetic images; 5000 scenes; 9992 industrial 3D CAD shapes of furniture; high-resolution informative textures developed by professional designers; WordNet Taxonomy,indoor, synthethic,CAD models,34,Semantic Segmentation; Instance Segmentation; Texture Synthesis,42,0,,c
3DMatch,3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions,2017,CVPR,https://arxiv.org/pdf/1603.08182v3,https//3dmatch.cs.princeton.edu,The dataset contains 2D RGB-D patches and 3D patches (local TDF voxel grid volumes) of wide-baselined correspondences,2D RGB-D patches and 3D patches (local TDF voxel grid volumes) of wide-baselined correspondences.; validation set: 10000 pairs of RGB-D patches and their ground truth correspondence labels; test dataset: similar data for another 10000 pairs; except the ground truth correspondence labels have been left out,indoor,real,datasets used: SUN3D; 7-Scenes; RGB-D Scenes-v2;  BundleFusion; Analysis by Synthesis ,0,Point Cloud Registration; Low-Light Image Enhancement; 3D Feature Matching,170,3,,c
3D MM-Vet,ShapeLLM: Universal 3D Object Understanding for Embodied Interaction,2024,ECCV,https://arxiv.org/pdf/2402.17766v3,https//hub.com/qizekun/ShapeLLM,The 3D MM-Vet benchmark comprises 59 diverse Internet8 3D objects and 232 human-written question-answer pairs,59 3D objects; 232 human-written question-answer pairs,other,other,sourced exclusively from Turbosquid,0,3D Question Answering (3D-QA),4,1,,c
3DNet,3DNet: Large-Scale Object Class Recognition from CAD Models,2012,ICRA,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6225116&casa_token=ge8wIzslq10AAAAA:mh2TH1OKHHMyIG9TK3jAIl2Qt7UJbqzKC6se666XVnMjVK8dHvXPbiOfMQEk_eWU3H3qDJLtaot2&tag=1,https//www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/3dnet-dataset/,free resource for object class recognition and 6DOF pose estimation from point cloud data; 3DNet provides a large-scale hierarchical CAD-model databases with increasing numbers of classes and difficulty with 10; 60 and 200 object classes together with evaluation datasets that contain thousands of scenes captured with an RGB-D sensor.,A) CatJO: Basic Object Classes -> 360 common; simple; geometrically distinguishable 3D CAD models (e.g. apple; banana...); 1600 scenes of single objects on a flat surface in multiple poses and multiple instances per class; B) Cat50: Super-Classes -> CatlO database + forty additional classes including sub-categories (e.g. chair; armchair); C) CatJOO: Color -> adds objects with similar shape but can be distinguished using color (e.g. lemon; lime); CAT200: Size ->   cosnidering the real world size of objects (e.g. planes; toy- planes); models linked to WordNet structure,other,synthetic,"constructed by semi-automatically downloading models from Google's 3D Warehouse and various smaller, free online repositories for CAD models",200,Object class recognition and 6DOF pose estimation from point cloud data,4,0,,c
3DPW (3D Poses in the Wild),Recovering Accurate 3D Human Pose in The Wild Using IMUs and a Moving Camera,2018,ECCV,https://openaccess.thecvf.com/content_ECCV_2018/papers/Timo_von_Marcard_Recovering_Accurate_3D_ECCV_2018_paper.pdf,https//virtualhumans.mpi-inf.mpg.de/3DPW/,dataset in the wild with accurate 3D poses for evaluation,60 video sequences; 2D pose annotations; 3D poses obtained with the method introduced in the paper; Camera poses for every frame in the sequences; 3D body scans and 3D people models (re-poseable and re-shapeable); each sequence contains its corresponding models; 18 3D models in different clothing variations,outdoor,real,single hand-held (RGB) camera;  Inertial Measurement Units (IMUs),0,Pose Estimation; 3D Human Pose Estimation; Hand Pose Estimation; Human Pose Forecasting; Cross-Domain 3D Human Pose Estimation,383,5,,c
3DSSG,Learning 3D Semantic Scene Graphs from 3D Indoor Reconstructions,2020,CVPR,https://openaccess.thecvf.com/content_CVPR_2020/papers/Wald_Learning_3D_Semantic_Scene_Graphs_From_3D_Indoor_Reconstructions_CVPR_2020_paper.pdf,https//3dssg.hub.io,3DSSG provides 3D semantic scene graphs for 3RScan.,"1482 scans; 48k instances; 534 classes; 40 relationships; 478 scenes; annotations include
scan-to-scene-mappings and 3D transformations together with dense instance segmentation",indoor,real,extends 3RScan with semantic scene graph annotations,534,3D Scene Graph Generation; 3D Scene Graph Alignment,32,2,,c
3RScan,RIO: 3D Object Instance Re-Localization in Changing Indoor Environments,2019,ICCV,https://arxiv.org/pdf/1908.06109v1,https://waldjohannau.github.io/RIO/,A novel dataset and benchmark featuring 1482 RGB-D scans of 478 environments across multiple time steps,1482 RGB-D scans of 478 environments across multiple time steps; approximately 363k frames,indoor,real,RGB-D images recorded with a Tango mobile application,528,3D Semantic Segmentation; Scene Understanding; Point Cloud Registration; Scene Graph Generation; 3D Object Classification; Scene Change Detection; Predicate Classification,40,5,,c
4D-OR,4D-OR: Semantic Scene Graphs for OR Domain Modeling,2022,MICCAI,https://arxiv.org/pdf/2203.11937v1,https//hub.com/egeozsoy/4D-OR,4D surgical Semantic Scene Graphs (SSG) dataset,6734 scenes; recorded by 6 calibrated RGB-D Kinect sensors 1 mounted to the ceiling of the Operating Room (OR); with one frame-per-second; providing synchronized RGB and depth images; fused point cloud sequences of entire scenes; automatically annotated human 6D poses and 3D bounding boxes for OR objects; SSG annotations for each step of the surgery together with the clinical roles of all the humans in the scenes; e.g.; nurse; head surgeon; anesthesiologist,indoor,real,6 calibrated RGB-D Kinect sensors,0,medical application; 3D Object Detection; 3D Human Pose Estimation; Scene Graph Generation,8,1,,c
4DMatch,Lepard: Learning partial point cloud matching in rigid and deformable scenes,2022,CVPR,https://openaccess.thecvf.com//content/CVPR2022/papers/Li_Lepard_Learning_Partial_Point_Cloud_Matching_in_Rigid_and_Deformable_CVPR_2022_paper.pdf,hubhttps//hub.com/rabbityl/lepard,benchmark for matching and registration of partial point clouds with time-varying geometry,based on DeformingThings4D (= 1972 animation sequences with ground truth dense correspondence) -> random selection of 1761 animations + generation of partial point cloud scans by synthesizing depth images; train/valid/test sets: 1232/176/353; Point cloud pairs in the test sequence are split into either 4DMatch or 4DLoMatch based on an overlap ratio threshold of 45%,other,synthetic,based on DeformingThings4D,0,Point Cloud Registration; 3D Feature Matching; Partial Point Cloud Matching; 3D Point Cloud Matching,10,1,,c
7-Scenes,/,2013,Microsoft,/,https://www.microsoft.com/en-us/research/project/rgb-d-dataset-7-scenes/,The 7-Scenes dataset is a collection of tracked RGB-D camera frames,7 sequences; each sequence consists of 500-1000 frames; each frame consists of three files: Color (RGB; 24-bit; PNG); Depth (in mm; 16-bit; PNG; invalid depth is set to 65535); Pose (camera-to-world; 4×4 matrix in homogeneous coordinates; TXT),indoor,real,handheld Kinect RGB-D camera,0,Dense Tracking; Mapping; Relocalization; 3D Scene Reconstruction,6,0,,c
A Large Dataset of Object Scans,A Large Dataset of Object Scans,2016,arXiv,https://arxiv.org/pdf/1602.02481v3,http//redwood-data.org/3dscan/index.html,dataset of more than ten thousand 3D scans of real objects,over 10000 dedicated 3D scans of individual objects;  44 categories that have at least 44 scans each; largest category: car (sedans; SUVs; vans); other categories: chair; table; bench; utility vehicles; motorcycles; trucks; the average length of a scan is 77 seconds; yielding over 23 million RGB-D images in total,other,real,mobile scanning setups: netbook + RGB-D camera,44,Scene Understanding; 3D Reconstruction; Object Reconstruction,5,0,,c
A*3D,A*3D Dataset: Towards Autonomous Driving in Challenging Environments,2020,ICRA,https://arxiv.org/pdf/1909.07541v1,https//hub.com/I2RDL2/ASTAR-3D,The A*3D dataset is a step forward to make autonomous driving safer for pedestrians and the public in the real world,230K human-labeled 3D object annotations in 39179 LiDAR point cloud frames and corresponding frontal-facing RGB images; Captured at different times (day; night) and weathers (sun; cloud; rain); 39K frames; 7 classes; 230K 3D object annotations,outdoor,real,raw sensor data collected using the A*STAR autonomous vehicle; Two PointGrey Chameleon3 USB3 Global shutter color (RGB) cameras; Velodyne HDL-64ES3 3D-LiDAR,7,(3D) Object Detection; Autonomous Driving,37,0,,c
Aachen Day-Night,Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions,2018,CVPR,https://arxiv.org/pdf/1707.09092v3,https//www.visuallocalization.net,Dataset for benchmarking 6DOF outdoor visual localization in changing conditions,14607 images with changing conditions of weather; season and day-night cycles; 98 night-time query images  captured using a Google Nexus5X phone with software HDR enabled; handlabelled 2D-3D matches,outdoor,real,based on the Aachen localization dataset; Google Nexus5X phone (RGB+HDR),0,Pose Estimation; Camera Localization; Visual Localization,89,1,,c
ABC Dataset,ABC: A Big CAD Model Dataset For Geometric Deep Learning,2019,CVPR,https://arxiv.org/pdf/1812.06216v2,https//deep-geometry.hub.io/abc-dataset/,collection of one million Computer-Aided Design (CAD) models for research of geometric deep learning methods and applications, over one million Computer-Aided Design (CAD) models,other,synthetic,CAD models collected from Onshape,0,Object Detection; 3D Reconstruction; Single-View 3D Reconstruction; Physical Simulations; Region Proposal,96,0,,c
ABO (Amazon Berkeley Objects),ABO: Dataset and Benchmarks for Real-World 3D Object Understanding,2022,CVPR,https://arxiv.org/pdf/2110.06199v2,https//amazon-berkeley-objects.s3.amazonaws.com/index.html,large-scale dataset designed for material prediction and multi-view retrieval experiments,ABO contains product catalog images; metadata; and artist-created 3D models with complex geometries and physically-based materials that correspond to real; household objects; Blender renderings of 30 viewpoints for each of the 7953 3D objects (in glTF 2.0 format); camera intrinsics and extrinsic for each rendering;  category annotations for each 3D model; mapped to noun synsets under the WordNet taxonomy,indoor,synthetic,"originates from worldwide product listings, metadata, images and 3D models provided by Amazon.com",63,3D Reconstruction; Single-View 3D Reconstruction; Metric Learning,74,0,,c
ADE20K,Scene Parsing Through ADE20K Dataset,2017,CVPR,https://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Scene_Parsing_Through_CVPR_2017_paper.pdf,https//groups.csail.mit.edu/vision/datasets/ADE20K/,semantic segmentation dataset that contains more than 20K scene-centric images exhaustively annotated with pixel-level objects and object parts labels,more than 20K scene-centric images exhaustively annotated with pixel-level objects and object parts labels; totally 150 semantic categories; 20210 training images - 2000 validation images - 3000 testing images ,mixed,real,RGB images,2.693,Semantic Segmentation; Instance Segmenation; Image-to-Image Translation; Scene Understanding; Panoptic Segmentation; Scene Recognition….,1164,28,,c
AI2-THOR (The House Of inteRactions),AI2-THOR: An Interactive 3D Environment for Visual AI,2017,arXiv,https://arxiv.org/pdf/1712.05474v4,https//ai2thor.allenai.org,interactive environment for embodied AI,"4 types of scenes (kitchen; living room; bedroom; bathroom); each scene includes 30 rooms; each room is unique in terms of furniture placement and item types; 3,578 interactive objects for AI agents",indoor,synthetic,hand-modeled objects,0,Robotics; Visual Navigation; Imitation Learning,222,1,,c
aiMotive,aiMotive Dataset: A Multimodal Dataset for Robust Autonomous Driving with Long-Range Perception,2022,arXiv,https://arxiv.org/pdf/2211.09445v3,https//hub.com/aimotive/aimotive_dataset,multimodal dataset for robust autonomous driving with long-range perception,176 scenes; synchronized and calibrated LiDAR + camera + radar sensors covering a 360-degree field of view; data was captured in highway; urban; and suburban areas during daytime; night; rain and is annotated with 3D bounding boxes with consistent identifiers across frames; 26 583 annotated frames; more than 425k objects organized into 14 categories,outdoor,real,rotating 64-beam LiDAR; 4 cameras; 2 long-range radars; high-precision GNSS+INS sensor,14,3D Object Detection; Object Tracking,1,1,,c
AKB-48 (Articulated object Knowledge Base),AKB-48: A Real-World Articulated Object Knowledge Base,2022,CVPR,https://arxiv.org/pdf/2202.08432v1,https//liuliu66.hub.io/articulationobjects/,large-scale Articulated object Knowledge Base,2037 real-world 3D articulated object models of 48 categories,other,real,EinScan Pro 2020 for 3D scanning; Intel RealSense D435 for RGB-D multi-view snapshot,48,Articulated Object Understanding,7,0,,c
ALFRED (Action Learning From Realistic Environments and Directives),ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks,2020,CVPR,https://arxiv.org/pdf/1912.01734v2,https//hub.com/askforalfred/alfred,benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks,25743 English language directives describing 8055 expert demonstrations averaging 50 steps each; resulting in 428322 image-action pairs (directives contain both high-level goals like “Rinse off a mug and place it in the coffee maker.” and low-level language instructions like “Walk to the coffee maker on the right.”);  84 object classes across 120 different indoor scenes,indoor,synthetic,simulated environment; crowdsourced language directives,84,Visual language understanding,158,0,,c
AnoVox,AnoVox: A Benchmark for Multimodal Anomaly Detection in Autonomous Driving,2024,arXiv,https://arxiv.org/pdf/2405.07865v3,https//zenodo.org/communities/anovox/records?q=&l=list&p=1&s=10&sort=newest,large-scale benchmark for ANOmaly detection in autonomous driving, 1117 scenarios based on the mono sensor configuration; 76 for the stereo configuration; and 35 for the multi configuration; 24 different types of scenarios; 8 different regions and 14 weather and time of day presets; 178 different content anomalies in 5 different size classes: (tiny; small; medium; big;  huge); 6 super-classes (anomal; home; special; nature; falling; airplane); uses CARLA simularot,outdoor,synthetic,RGB camera; lidar; depth sensors,178,Anomaly Detection; Autonomous Driving,3,0,,c
A-OKVQA,A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge,2022,ECCV,https://arxiv.org/pdf/2206.01718v1,https//hub.com/allenai/aokvqa,crowdsourced visual question answering dataset composed of a diverse set of about 25K questions requiring a broad base of commonsense and world knowledge to answer,24903 Question-Answer-Rationale triplets; split into 17.1K/1.1K/6.7K for train/validation/test,mixed,real,RGB images from the 2017 partitioning of the COCO dataset,0,Visual Question Answering (VQA),142,1,,c
ApolloScape,The ApolloScape Open Dataset for Autonomous Driving and its Application,2018,TPAMI,https://arxiv.org/pdf/1803.06184v4,https//apolloscape.auto,large dataset consisting of over 140000 video frames from various locations in China under varying weather conditions," 143,906 video frames + corresponding pixel-level semantic labelling; 73 street scene videos from 4 locations in China under varying weather conditions",outdoor,real,acquisition system: Riegl VMX-1HA -> 2 VUX-1HA laser scanners; 1 VMX-CS6 camera system (RGB); IMU/GNSS,27,Autonomous Driving; Object Detection; Semantic Segmentation; Trajectory Prediction; Image Inpainting; Autonomous Driving; Motion Segmentation,71,5,,c
ARCH2S,ARCH2S: Dataset; Benchmark and Challenges for Learning Exterior Architectural Structures from Point Clouds,2024,arXiv,https://arxiv.org/pdf/2406.01337v1,,photo-realistic 3D architectural models dataset and benchmark for semantic segmentation,4 different building purposes of real-world buildings as well as an open architectural landscape in Hong Kong; Each point cloud is annotated into one of 14 semantic classes; detailed annotations of real-world reconstructed exterior building objects into 13 commonly seen built elements grouped into three main categories (miscellaneous; structural; and decorative),outdoor,synthetic,3D models are derived from the 3DBIT00 dataset,14,3D Semantic Segmentation,3,1,,c
Argoverse ,Argoverse: 3D Tracking and Forecasting with Rich Maps,2019,CVPR,https://arxiv.org/pdf/1911.02620v1,https//www.argoverse.org/data.html,tracking benchmark with over 30K scenarios collected in Pittsburgh and Miami,training-validation-test sets: 205942 - 39472 - 78143 sequences (no geographical overlap),outdoor,real,"2 roof-mounted,
rotating 32 beam LiDAR sensor; 2 front-facing stereo cameras  (RGB); 6-DOF localization for each timestamp comes from a combination of GPSbased and sensor-based localization",15,3D Object Detection; Trajectory Prediction; Motion Forecasting; Monocular Cross-View Road Scene Parsing (Road / Vehicle); 3D Object Tracking,370,6,,c
Argoverse 2,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,2021,NeurIPS,https://arxiv.org/pdf/2301.00493v1,https//www.argoverse.org/av2.html,collection of open-source autonomous driving data and high-definition (HD) maps from six U.S. cities: Austin; Detroit; Miami; Pittsburgh; Palo Alto; and Washington; D.C,4 open-source datasets: A) Sensor Dataset: 1000 3D annotated scenarios with lidar; stereo imagery; and ring camera imagery; B) Motion Forecasting Dataset: 250000 scenarios with trajectory data for many object types; C) Lidar Dataset: 20000 unannotated lidar sequences; D) Map Change Dataset: 1000 scenarios; 200 of which depict real-world HD map changes,outdoor,real,LiDAR; Stereo Images (RGB),30,3D Object Detection; (Self-Supervised) Scene Flow Estimation; Dynamic Point Removal,170,3,,c
ARID (Autonomous Robot Indoor Dataset),Recognizing Objects In-the-wild: Where Do We Stand?,2017,ICRA,https://arxiv.org/pdf/1709.05862v2,https//www.acin.tuwien.ac.at/en/vision-for-robotics/software-tools/autonomous-robot-indoor-dataset/,large-scale; multi-view object dataset collected with an RGB-D camera mounted on a mobile robot,RGB and depth images of daily life objects belonging to 51 categories; Each object category contains three instances; for a total of 153 physical objects; and it coincides with one of the 51 WordNet leaf nodes used to determine the categories of a very well-known dataset; the RGB-D Object Dataset (= complete overlap between the categories represented in the two datasets; ARID and ROD),indoor,real,RGB-D camera,51,Object Recognition; Question Generation,5,0,,c
ArraMon,ArraMon: A Joint Navigation-Assembly Instruction Interpretation Task in Dynamic Environments,2020,EMNLP,https://arxiv.org/pdf/2011.07660v1,https//arramonunc.hub.io,A dataset (in English and also extended to Hindi) with human-written navigation and assembling instructions; and the corresponding ground truth trajectories,7.7K task instances (30.8K instructions and paths); challenging dataset (in English and now also Hindi) via a 3D synthetic simulator with diverse object referring expressions; environments and visuospatial relationships,outdoor,synthetic,simulated environment,0,Vision and Language Navigation,3,0,,c
BAAI-VANJEE,BAAI-VANJEE Roadside Dataset: Towards the Connected Automated Vehicle Highway technologies in Challenging Environments of China,2021,arXiv,https://arxiv.org/pdf/2105.14370v1,https//data.baai.ac.cn/data-set,roadside dataset for benchmarking and training various computer vision tasks such as 2D/3D object detection and multi-sensor fusion,LiDAR data and RGB images collected by VANJEE smart base station placed on the roadside about 4.5m high; 2500 frames of LiDAR data; 5000 frames of RGB images; including 20% collected at the same time; 12 classes of objects; 74K 3D object annotations and 105K 2D object annotations,outdoor,real,LiDAR data; RGB images,12,Object Detection; Radar Object Detection,6,0,,c
BenchLMM,BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models,2023,arXiv,https://arxiv.org/abs/2312.02896,https//aifeg.hub.io/BenchLMM/,benchmark to assess the robustness of LMMs (Large Multimodal Models; e.g. GPT-4V) against three different styles: artistic image style; imaging sensor style; and application style; where each style has five sub-styles,a) Artistic style data: 500 images from the COCO-O dataset -> 100 images for each stylistic category (Cartoon; Painting; Sketch; Handmade; and Tattoo styles); b) Sensor-style data: 200 Infrared images; 200 X-ray images; 200 MRI images; 200 CT images; c) Application-style data: 200 images from the DOTA dataset (remote sensing); 250 images from the BDD100K dataset (autonomous driving); 100 images from the Domestic Robot dataset + 117 from the Open Game dataset (agent action prediction); 100 images from the Defect Detection dataset (defect detection) ,other,real,RGB cameras; COCO-O images; Infrared sensors; X-ray sensors; MRI sensors; CT sensors,0,Visual Question Answering (VQA),12,1,,c
BLVD,BLVD: Building A Large-scale 5D Semantics Benchmark for Autonomous Driving,2019,ICRA,https://arxiv.org/pdf/1903.06405v1,https//hub.com/VCCIV/BLVD/,large scale 5D semantics dataset collected by the Visual Cognitive Computing and Intelligent Vehicles Lab,654 high-resolution video clips owing 120k frames extracted from Changshu; Jiangsu Province; China; frame rate is 10fps/sec for RGB data and 3D point cloud; dataset contains fully annotated frames which yield 249129 3D annotations; 4902 independent individuals for tracking with the length of overall 214;922 points; 6;004 valid fragments for 5D interactive event recognition; and 4900 individuals for 5D intention prediction; 4 kinds of scenarios depending on the object density (low and high) and light conditions (daytime and nighttime); 3 classes (pedestrians;vehicles; riders),outdoor,real,Velodyne HDL-64E LIDAR scanner; GPS/inertial system; 2 multi-view color cameras (RGB),3,Trajectory Prediction; Autonomous Driving; Decision Making,9,0,,c
BnB,Airbert: In-domain Pretraining for Vision-and-Language Navigation,2021,ICCV,https://arxiv.org/pdf/2108.09105v1,https//hub.com/airbert-vln/bnb-dataset,large-scale and diverse in-domain VLN (Vision and Language Navigation) dataset -> based on Airbnb photos + descriptions,5.6M listings from over 100K cities all around the world,indoor,real,downloaded listings + metadata; RGB images,0,Vision-Language Navigation,3,0,,c
sh,Breaking Bad: A Dataset for Geometric Fracture and Reassembly,2022,NeurIPS,https://arxiv.org/pdf/2210.11463v1,https//breaking-bad-dataset.hub.io,large-scale dataset of fractured objects,around 10k meshes from PartNet and Thingi10k; For each mesh: 20 fracture modes are pre-computed and then simulate 80 fractures from them; resulting in a total of 1M breakdown patterns,other,synthetic,meshes from PartNet and Thingi10K,0,3D Assembly,11,0,,c
Building3D,Building3D: An Urban-Scale Dataset and Benchmarks for Learning Roof Structures from Point Clouds,2023,ICCV,https://arxiv.org/pdf/2307.11914v1,https//building3d.ucalgary.ca,urban-scale dataset consisting of more than 160 thousands buildings along with corresponding point clouds; mesh and wireframe models,more than 160 thousands buildings; covering 16 cities in Estonia about 998 Km2; raw LiDAR point clouds are collected by a high-precision RIEGL VQ-1560i scanner at altitude 2600 meters and stored in LAZ-format (including XYZ coordinates; RGB color; near infrared information; intensity; reflectance),outdoor,real,"RIEGL VQ-1560i LiDAR scanner (XYZ coordinates + RGB color, NIR, intensity, reflectance)",0,urban modeling; aerial path planning; mesh simplification; semantic/part seg+B30+E41,6,0,,c
BuildingNet,BuildingNet: Learning to Label 3D Buildings,2021,ICCV,https://arxiv.org/pdf/2110.04955v1,https//buildingnet.org,large-scale dataset of 3D building models whose exteriors are consistently labeled,513K annotated mesh primitives; grouped into 292K semantic part components across 2K building models; dataset covers several building categories; such as houses; churches; skyscrapers; town halls; libraries; and castle,outdoor,synthetic,"models from
models were mined from the 3D Warehouse repository; crowdsourced annotations",31,3D Semantic Segmentation; 3D Builiding Mesh Labeling,11,1,,c
BundleFusion (SUN-RGB-D data),BundleFusion: Real-time Globally Consistent 3D Reconstruction using Online Surface Re-integration,2016,TOG,https://arxiv.org/pdf/1604.01093v3,https//graphics.stanford.edu/projects/bundlefusion/,They provide a dataset containing RGB-D data of 7 large scenes (60m average trajectory length; 5833 average number of frames). The RGB-D data was captured using a Structure.io depth sensor coupled with an iPad color camera. ,7 scenes; average number of frames: 5833; each sequence contains: Color frames (RGB; 24-bit; JPG); Depth frames (mm; 16-bit; PNG; invalid depth is set to 0); Camera poses (camera-to-world; invalid transforms -INF; TXT); Camera calibration (color and depth camera intrinsics and extrinsics; TXT),indoor,real,Structure.io depth sensor; iPad RGB camera,0,Mixed Reality; Robotic application; Relocalization,-1,-1,,c
CADC (Canadian Adverse Driving Conditions),Canadian Adverse Driving Conditions Dataset,2020,IJRR,https://arxiv.org/pdf/2001.10117v3,http//cadcd.uwaterloo.ca,autonomous vehicle dataset that focuses on adverse driving conditions,7000 frames collected through a variety of winter weather conditions of annotated data from 8 cameras; collected with the Autonomoose autonomous vehicle platform; based on a modified Lincoln MKZ; collected during winter within in region of Waterloo; Canada,outdoor,real,1x Velodyne VLP-32C Lidar; 8x Ximea MQ013CG-E2 Camera (RGB); 8x Edmund Optics C Series Fixed Focal Length Lens; 1x Dataspeed Advanced Driver Assistance Systems (ADAS) Kit; 1x Novatel OEM638 Triple-Frequency GNSS Receiver; 1x Sensonor STIM300 MEMS IMU; 2x Xsens 1x MTi-300-AHRS 1x MTi-30-AHRS IMUs,10,(3D) Object Detection,23,0,,c
CamVid (Cambridge-driving Labeled Video Database),Semantic object classes in video: A high-definition ground truth database,2009,Pattern Recognit. Lett.,https://www.sciencedirect.com/science/article/pii/S0167865508001220?via%3Dihub,https//mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid/,road/driving scene understanding database which was originally captured as five video sequences with a 960×720 resolution camera mounted on the dashboard of a car,5 video sequences with a 960×720 resolution; sequences were sampled (4 of them at 1 fps and 1 at 15 fps) adding up to 701 frames: manually annotated with 32 classes (void; building; wall; tree; vegetation; fence; sidewalk; parking block; column/pole; traffic cone; bridge; sign; miscellaneous text; traffic light; sky; tunnel; archway; road; road shoulder; lane markings (driving); lane markings (non-driving); animal; pedestrian; child; cart luggage; bicyclist; motorcycle; car; SUV/pickup/truck; truck/bus; train; and other moving object),outdoor,real,3CCD Panasonic HVX200 digital camera (RGB),32,(2D / Real-Time / Video) Semantic Segmentation,224,4,,c
CarFusion,CarFusion: Combining Point Tracking and Part Detection for Dynamic 3D Reconstruction of Vehicles,2018,CVPR,https://openaccess.thecvf.com/content_cvpr_2018/papers/Reddy_CarFusion_Combining_Point_CVPR_2018_paper.pdf,https//www.cs.cmu.edu/~ILIM/projects/IM/CarFusion/cvpr2018/index.html,reconstruction of multiple dynamic rigid objects (eg. vehicles) observed from wide-baseline uncalibrated and unsynchronized cameras,We provide manual annotations of 14 semantic keypoints for 100000 car instances from 53000 images captured from 18 moving cameras at Multiple intersections in Pittsburgh,outdoor,real,RGB camera,0,Vehicle Pose Estimation; 3D Pose Estimation; Pose Estimation; 3D Car Instance Understanding; Multi-View Learning,8,2,,c
CASR (Cyclist Arm Signal Recognition),Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation,2019,T-ITS,https://arxiv.org/pdf/1910.03858v1,https//hub.com/VRU-intention/casr,dataset for cyclist arm signal recognition in videos,contains 219 annotated arm signal actions on videos of approximately 10 seconds each; containing one or two actions per video,outdoor,real,RGB images from Daimler video sequences,0,Intention Recognition,1,0,,c
Chairs (Aubry et al.),Seeing 3D Chairs: Exemplar Part-based 2D-3D Alignment using a Large Dataset of CAD Models,2014,CVPR,https://openaccess.thecvf.com/content_cvpr_2014/papers/Aubry_Seeing_3D_Chairs_2014_CVPR_paper.pdf,https//www.di.ens.fr/willow/research/seeing3Dchairs/,rendered images of around 1000 different three-dimensional chair models,1393 high-quality 3D chair models; variety of chair styles; Given an input image the algorithm searches a database of 1393 3D CAD chair models to detect any depicted chairs in the image,indoor,synthetic,3D CAD models,0,Sketch-Based Image Retrieval,109,1,,c
Chairs (Jiang et al.),Full-Body Articulated Human-Object Interaction,2023,ICCV,https://arxiv.org/pdf/2212.10621v3,https//hub.com/jnnan/chairs,large-scale motion-captured of Full-Body Articulated Human-Object Interact (f-AHOI) dataset,17.3 hours of versatile interactions between 46 participants and 81 articulated and rigid sittable objects; 3D meshes of both humans and articulated objects during the entire interactive process; as well as realistic and physically plausible full-body interactions; 3D meshes of these objects were captured using the Scaniverse app on an iPad Pro; motion capturing (MoCap) interactions,other,real,multi-view RGB-D sequences; 5 hybrid trackers on objects; 17 IMUs on participants,0,Pose Estimation; Action Recognition; Motion Synthesis; Human-Object Interaction Detection,3,0,,c
ChangeSim,ChangeSim: Towards End-to-End Online Scene Change Detection in Industrial Indoor Environments,2021,IROS,https://arxiv.org/pdf/2103.05368v2,https//hub.com/SAMMiCA/ChangeSim,dataset aimed at online scene change detection (SCD) and more,6 maps (12 sequences; 2 sequences for each map) as the training set and the remaining 4 maps (8 sequences; 2 sequences for each map) as the test set,indoor,synthetic,Unreal Engine 4 (UE4) to implement the virtual environment,24,(Scene) Change Detection,7,2,,c
CITR Dataset (& DUT dataset),Top-view Trajectories: A Pedestrian Dataset of Vehicle-Crowd Interaction from Controlled Experiments and Crowded Campus,2019,IV,https://arxiv.org/pdf/1902.00487v2,https//hub.com/dongfang-steven-yang/vci-dataset-citr,two pedestrian trajectory datasets; CITR dataset and DUT dataset -> CITR Dataset consists of experimentally designed fundamental VCI (Vehicle-Crowd-Interaction) scenarios (front; back; and lateral VCIs) and provides unique ID for each pedestrian; DUT dataset gives two ordinary and natural VCI scenarios in crowded university campus,CITR: 6 scenarios; pedestrian-only and VCI scenarios; 38 video clips in total; which include approximate 340 pedestrian trajectories; DUT:  17 clips of crosswalk scenarios and 11 clips of shared space scenarios; including 1793 trajectories; Some of the clips contains multiple VCIs; i.e.; more than 2 vehicles interacting with pedestrians simultaneousl,outdoor,real,"DJI
Phamton 3 SE Drone with a down-facing camera (RGB)",0,Autonomous Vehicles; Autonomous Driving; Motion Prediction,1,0,,c
Cityscapes,The Cityscapes Dataset for Semantic Urban Scene Understanding,2016,CVPR,https://arxiv.org/pdf/1604.01685v2,https//www.cityscapes-dataset.com/dataset-overview/,large-scale database which focuses on semantic understanding of urban street scenes,semantic; instance-wise; and dense pixel annotations for 30 classes grouped into 8 categories (flat surfaces; humans; vehicles; constructions; objects; nature; sky; and void); around 5000 fine annotated images and 20000 coarse annotated ones; Data was captured in 50 cities during several months; daytimes; and good weather conditions,outdoor,real,CMOS 2 MP sensors (OnSemi AR0331) (RGB),30,(2D) Semantic Segmentation; Image Generation; Instance Segmentation; Scene Parsing; Image-to-Image Translation; Depth Estimation; Panoptic Segmentation; Video Prediction…,3603,54,,c
Cityscapes 3D,Cityscapes 3D: Dataset and Benchmark for 9 DoF Vehicle Detection,2020,arXiv,https://arxiv.org/pdf/2006.07864v1,https//hub.com/mcordts/cityscapesScripts,extends the original Cityscapes dataset with 3D bounding box annotations for all types of vehicles,Cityscapes + 3D bounding boxes,outdoor,real,CMOS 2 MP sensors (OnSemi AR0331) (RGB),8,Semantic Segmentation; 3D Object Detection; Monocular Depth Estimation,11,3,,c
Cityscapes Panoptic Parts,Part-aware Panoptic Segmentation,2021,CVPR,https://arxiv.org/pdf/2106.06351v1,https//hub.com/pmeletis/panoptic_parts,part-aware panoptic segmentation annotations for the Cityscapes dataset,Cityscapes + part-aware panoptic segmentation annotiations; 23 part-level semantic classes,outdoor,real,CMOS 2 MP sensors (OnSemi AR0331) (RGB),23,Scene Understanding; (Part-Aware) Panoptic Segmentation; Human Part Segmentation,8,1,,c
Clear Weather (DENSE),Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather,2020,CVPR,https://arxiv.org/pdf/1902.08913v3,https//www.uni-ulm.de/en/in/driveu/projects/dense-datasets#c811669,object detection dataset in challenging adverse weather conditions,12000 samples in real-world driving scenes; 1500 samples in controlled weather conditions within a fog chamber; different weather conditions (like fog; snow; rain); acquired by over 10;000 km of driving in northern Europe; 100k objects were labeled with 2D / 3D bounding boxes,outdoor,real,RGB cameras (2 OnSemi AR0230 imagers); NIR camera ( BrightwayVision BrightEye); (FMCW) radar; LiDAR (Velodyne HDL64 S3D and VLP32C); Q1922 FIR camera; environmental sensors (Airmar WX150 weather station),0,(2D / 3D) Object Detection ,32,2,,c
COCO-O(ut-of-distribution),COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts,2023,ICCV,https://arxiv.org/abs/2307.12730,https//cocodataset.org/#home,Benchmark for Object Detectors under Natural Distribution Shifts ,6 domains (sketch; cartoon; painting; weather; handmake; tattoo) of COCO objects which are hard to be detected by most existing detectors; 6782 images and 26624 labelled bounding boxes,other,real,RGB images from the Internet,0,(2D) Object Detection,43,1,,c
COCO-Stuff (Common Objects in COntext-stuff),COCO-Stuff: Thing and Stuff Classes in Context,2018,CVPR,https://arxiv.org/pdf/1612.03716v4,https//hub.com/nightrome/cocostuff,dataset for scene understanding tasks like semantic segmentation; object detection and image captioning; constructed by annotating the original COCO dataset; which originally annotated things while neglecting stuff annotations,164k images that span over 172 categories including 80 things; 91 stuff; and 1 unlabeled class,mixed,real,COCO RGB images,172,Semantic Segmentation; Image-to-Image Translation…,313,20,,c
Completion3D,TopNet: Structural Point Cloud Decoder,2019,CVPR,https://openaccess.thecvf.com/content_CVPR_2019/papers/Tchapmi_TopNet_Structural_Point_Cloud_Decoder_CVPR_2019_paper.pdf,https//completion3d.stanford.edu,The Completion3D benchmark is a dataset for evaluating state-of-the-art 3D Object Point Cloud Completion methods; Given a partial 3D object point cloud the goal is to infer a complete 3D point cloud for the object,subset of the Shapenet dataset; derived from the dataset in Yuan et al.; The ground-truth was generated by uniformly sampling 16384 points on the mesh surfaces and the partial point clouds were generated by back-projecting 2.5D depth images into 3D,other,synthetic,subset of the Shapenet dataset -> raw 3D model data  from public online repositories and existing research datasets,8,Point Cloud Completion,33,1,,c
ContactDB,ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging,2019,CVPR,https://arxiv.org/pdf/1904.06830v1,https//hub.com/samarth-robo/contactdb_utils,dataset of contact maps for household objects that captures the rich hand-object contact that occurs during grasping; enabled by use of a thermal camera,Participants in our study grasped 3D printed objects with a post-grasp functional intent. ContactDB includes 3750 3D meshes of 50 household objects textured with contact maps and 375K frames of synchronized RGB-D+thermal images,other,real,FLIR Boson 640 thermal camera; Kinect v2 RGB-D sensor,0,Grasp Contact Prediction,21,1,,c
CP2A dataset (CARLA Pedestrian Action Anticipation dataset),Is attention to bounding boxes all you need for pedestrian action prediction?,2021,IV,https://arxiv.org/pdf/2107.08031v3,https//hub.com/linaashaji/CP2A,simulated dataset for pedestrian action anticipation collected using the CARLA simulator,they generated the data in two urban environments available in the CARLA simulator: Town02 and Town03; total number of simulated pedestrians is nearly 55k; equivalent to 14M bounding boxes samples; crossing behavior represents 25% of the total pedestrians; nearly 50k pedestrian sequences;  nearly 220k observation sequences,outdoor,synthetic,Simulated with CARLA,0,Action Anticipation,1,0,,c
CrossLoc,CrossLoc: Scalable Aerial Localization Assisted by Multimodal Synthetic Data,2022,CVPR,https://arxiv.org/pdf/2112.09081v5,https//datadryad.org/stash/dataset/doi10.5061/dryad.mgqnk991c,CrossLoc benchmark datasets—a multimodal aerial sim-to-real data available for flights above nature and urban terrains,multimodal aerial sim-to-real data; workflow produces synthetic RGB images and the geo-referenced scene coordinates as raw output; The semantic labels have seven classes (sky; ground; vegetation; building; water; bridge; others),outdoor,synthetic,synthetic RGB images,7,Camera Localization; Visual Localization,1,0,,c
DADA-2000,DADA: Driver Attention Prediction in Driving Accident Scenarios,2019,T-ITS,https://arxiv.org/pdf/1912.12148v2,https//hub.com/JWFan/LOTVS-DADA,large-scale benchmark for driver attention in driving accident scenarios (DADA),2000 video sequences; laborious annotation for driver attention (fixation; saccade; focusing time); accident objects/intervals; and accident categories (54 categories); Senso Motoric Instruments (SMI) RED250 desktop-mounted infrared eye tracker was used to track eye movements,outdoor,real,"RGB video frames; Senso Motoric Instruments (SMI) RED250 desktop-mounted
infrared eye tracker",54,Semantic Segmentation; Autonomous Vehicles; Scene Understanding,14,0,,c
DAIR-V2X,DAIR-V2X: A Large-Scale Dataset for Vehicle-Infrastructure Cooperative 3D Object Detection,2022,CVPR,https://arxiv.org/pdf/2204.05575v1,https//hub.com/AIR-THU/DAIR-V2X,large-scale; multi-modality; multi-view dataset from real scenarios for VICAD (Vehicle-Infrastructure Cooperative Autonomous Driving),71254 LiDAR frames and 71254 Camera frames; all frames are captured from real scenes with 3D annotations,outdoor,real,Infrastructure Sensors (LiDAR; RGB camera); Vehicle Sensors (LiDAR; RGB Camera; GPS & IMU),10,3D Object Detection,38,2,,c
DALES (Dayton Annotated LiDAR Earth Scan),DALES: A Large-scale Aerial LiDAR Data Set for Semantic Segmentation,2020,CVPR,https://arxiv.org/pdf/2004.11985v1,https//udayton.edu/engineering/research/centers/vision_lab/research/was_data_analysis_and_processing/dale.php,Large-scale Aerial LiDAR Data Set for Semantic Segmentation,over a half-billion hand-labeled points spanning 10 square kilometers of area and eight object categories; Point cloud data collected from an Aerial Laser Scanner (ALS),outdoor,real,aerial LiDAR data,8,Panoptic Segmentation; 3D Semantic Segmentation,26,2,,c
DeformingThings4D,4DComplete: Non-Rigid Motion Estimation Beyond the Observable Surface,2021,ICCV,https://openaccess.thecvf.com//content/ICCV2021/papers/Li_4DComplete_Non-Rigid_Motion_Estimation_Beyond_the_Observable_Surface_ICCV_2021_paper.pdf,hubhttps//hub.com/rabbityl/DeformingThings4D,synthetic dataset containing animation sequences containing humanoids and animals,1972 animation sequences spanning 31 categories of humanoids and animals; 200 animations for humanoids; 1772 animations for animals,other,synthetic,3D characters obtained from Adobe Mixamo,31,Scene Flow Estimation; Non-rigid tracking / registration; shape and motion completion; learning riggings from observation; generic non-rigid reconstruction ,38,1,,c
Dense Fog (DENSE),Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather,2020,CVPR,https://arxiv.org/pdf/1902.08913v3,https//www.uni-ulm.de/en/in/driveu/projects/dense-datasets#c811669,object detection dataset in challenging adverse weather conditions,12000 samples in real-world driving scenes; 1500 samples in controlled weather conditions within a fog chamber; different weather conditions (like fog; snow; rain); acquired by over 10;000 km of driving in northern Europe; 100k objects were labeled with 2D / 3D bounding boxes,outdoor,real,RGB cameras (2 OnSemi AR0230 imagers); NIR camera ( BrightwayVision BrightEye); (FMCW) radar; LiDAR (Velodyne HDL64 S3D and VLP32C); Q1922 FIR camera; environmental sensors (Airmar WX150 weather station),0,(2D / 3D) Object Detection ,17,2,,c
Description Detection Dataset,Described Object Detection: Liberating Object Detection with Flexible Expressions,2023,NeurIPS,https://arxiv.org/abs/2307.12813,https//hub.com/shikras/d-cube,next-generation object detection dataset; The dataset is meant for the Described Object Detection (DOD) task. OVD detects object based on category name; and each category can have zero to multiple instances,complex and descriptive class names; such as a dog not being held by a leash; For each image in the dataset; any object that matches the description is annotated; The dataset provides annotations such as bounding boxes and finely crafted instance masks.It comprises of 422 well-designed descriptions and 24282 positive object-description pairs.,other,real,RGB images,0,Object Detection; Referring Expression Comprehension; Open Vocabulary Object Detection; Described Object Detection,8,1,,c
Dex-Net 2.0 (Dexterity Network 2.0),Dex-Net 2.0: Deep Learning to Plan Robust Grasps with Synthetic Point Clouds and Analytic Grasp Metrics,2017,arXiv,https://arxiv.org/pdf/1703.09312v3,https//berkeleyautomation.hub.io/dex-net/,Dex-Net 2.0 is a dataset associating 6.7 million point clouds and analytic grasp quality metrics with parallel-jaw grasps planned using robust quasi-static GWS analysis on a dataset of 1500 3D object models,Dex-Net 2.0 contains 6.7 million datapoints; The dataset contains a subset of 1500 3D object mesh models from Dex-Net 1.0: 1371 synthetic models from 3DNet and 129 laser scans from the KIT object database,other,synthetic,synthetic models from 3DNet; laser scans from the KIT object database,0,Robotic Grasping,30,0,,c
DIODE (Dense Indoor and Outdoor DEpth),DIODE: A Dense Indoor and Outdoor DEpth Dataset,2019,arXiv,https://arxiv.org/pdf/1908.00463v2,https//diode-dataset.org,dataset that contains thousands of diverse high resolution color images with accurate; dense; long-range depth measurements,The training set consists of 8574 indoor and 16884 outdoor samples from 20 scans each; The validation set contains 325 indoor and 446 outdoor samples with each set from 10 different scans,mixed,real,FARO Focus S350 scanner (RGB camera + depth laser),0,Depth Estimation; (Indoor) Monocular Depth Estimation,73,2,,c
DOLPHINS,DOLPHINS: Dataset for Collaborative Perception enabled Harmonious and Interconnected Self-driving,2022,ACCV,https://arxiv.org/pdf/2207.07609v1,https//dolphins-dataset.net,DOLPHINS = Dataset for cOllaborative Perception enabled Harmonious and INterconnected Self-driving; new simulated large-scale various-scenario multi-view multi-modality autonomous driving dataset; which provides a ground-breaking benchmark platform for interconnected autonomous driving,6 typical scenarios with dynamic weather conditions; 42376 frames and 292549 objects; as well as the corresponding 3D annotations; geo-positions; and calibrations; Full-HD images and 64-line LiDARs construct high-resolution data with sufficient details; well-organized APIs and open-source codes,outdoor,synthetic,CARLA simulator; LiDAR and an RGB camera,0,2D / 3D Object Detection; Object Tracking; Multiview Detection,7,0,,c
DOORS (Dataset fOr bOuldeRs Segmentation),DOORS: Dataset fOr bOuldeRs Segmentation. Statistical properties and Blender setup,2022,arXiv,https://arxiv.org/pdf/2210.16253v1,https//zenodo.org/records/7107409#.Y20Ja-z7RhE,dataset designed for boulders recognition; centroid regression; segmentation; and navigation applications,2 sets: A) Regression -> Contains images; masks; and labels for 4 splits of single boulders positioned on the surface of a spherical mesh; B) Segmentation: Contain images; masks; and labels of 2 datasets: DS1 and DS2. DS1 is made of the same images of the Regression dataset but is specifically designed for segmentation. DS2 is made of images with multiple instances of boulders appearing on the surface of the Didymos asteroid model,other,synthetic,Blender models,0,Semantic Segmentation; 3D Object Recognition,3,0,,c
DrivAerNet,DrivAerNet: A Parametric Car Dataset for Data-Driven Aerodynamic Design and Graph-Based Drag Prediction,2024,arXiv,https://arxiv.org/pdf/2403.08055v1,https//hub.com/Mohamedelrefaie/DrivAerNet,large-scale; high-fidelity CFD dataset of 3D industry-standard car shapes designed for data-driven aerodynamic design,4000 high-quality 3D car meshes and their corresponding aerodynamic performance coefficients; alongside full 3D flow field information,other,synthetic,Modeling,0,Graph Regression; Physical Simulations; 3D Shape Modeling; 3D Geometric Prediction; Parameter Prediction; Physics-infomred machine learning; PDE Surrogate Modeling,3,1,,c
DublinCity,DublinCity: Annotated LiDAR Point Cloud and its Applications,2019,arXiv,https://arxiv.org/pdf/1909.03613v1,https//v-sense.scss.tcd.ie/dublincity/,benchmark dataset that includes a manually annotated point cloud for over 260 million laser scanning points into 100'000 (approx.) assets from Dublin LiDAR point cloud in 2015,manually annotated point cloud for over 260 million laser scanning points into 100000 (approx.) assets from Dublin LiDAR point cloud; Objects are labelled into 13 classes using hierarchical levels of detail from large (i.e.; building; vegetation and ground) to refined (i.e.; window; door and tree) elements,outdoor,real,LiDAR point cloud + RGB images,13,Scene Understanding; 3D Reconstruction; Visual Localization,15,0,,c
DurLAR,DurLAR: A High-fidelity 128-channel LiDAR Dataset with Panoramic Ambient and Reflectivity Imagery for Multi-modal Autonomous Driving Applications,2021,3DV,https://arxiv.org/pdf/2406.10068,https//hub.com/l1997i/DurLAR,high-fidelity 128-channel 3D LiDAR dataset with panoramic ambient (near infrared) and reflectivity imagery for multi-modal autonomous driving applications,LiDAR with 128 channels; 5 folders with a total of 145911 frames,outdoor,real,Ouster OS1-128 LiDAR sensor; Carnegie Robotics MultiSense S21 stereo camera (RGB); GNSS/INS (OxTS RT3000v3); Yocto Light V3 Lux Meter,0,Depth Estimation; Autonomous Driving; 3D Depth Estimation,5,0,,c
ECLAIR (Extended Classification of Lidar for AI Recognition),ECLAIR: A High-Fidelity Aerial LiDAR Dataset for Semantic Segmentation,2024,CVPR,https://arxiv.org/pdf/2404.10699v1,https//hub.com/SharperShape/eclair-dataset,outdoor large-scale aerial LiDAR dataset designed specifically for advancing research in point cloud semantic segmentation,total area of 10km2 with close to 600 million points; eleven distinct object categories (Ground; Vegetation; Buildings; Noise; Transmission Wired; Distribution Wires; Poles; Transmission Towers; Fence; Vehicle; Unassinged),outdoor,real,aerial LiDAR,11,3D Semantic Segmentation,1,1,,c
EgoBody,EgoBody: Human Body Shape and Motion of Interacting People from Head-Mounted Devices,2022,ECCV,https://arxiv.org/pdf/2112.07642v3,https//sanweiliti.hub.io/egobody/egobody.html,large-scale dataset for egocentric 3D human pose; shape and motions under interactions in complex 3D scenes,"125 sequences from 36 subjects (18 male and 18 female) performing diverse social interactions in 15 indoor scenes;  219731 synchronized frames captured from Azure Kinects; Microsoft  HoloLens2 headset for capturing multi-modal egocentric data (RGB, depth, head, hand and eye gaze tracking streams)",indoor,real,HoloLens2 headset (RGB; depth; tracking streams),0,3D Human Pose and Shape Estimation,28,1,,c
EHE (Elderly Home Exercise),Skeleton-based human action evaluation using graph convolutional network for monitoring Alzheimer’s progression,2021,Pattern Recognit.,https://www.sciencedirect.com/science/article/abs/pii/S003132032100282X,https//hub.com/bruceyo/egcnplusplus/tree/main/EHE_dataset,Human Action Evaluation (HAE) has rarely been applied to real-world disease monitoring; EHE consists of several actions from morning exercises that patients complete daily in the elderly home,869 action repetitions performed by 25 older people; Six exercises were collected for the EHE dataset via Kinect v2,indoor,real,Kinect v2 ,0,Action Assessment,2,1,,c
EQA (Embodied Question Answering),Embodied Question Answering,2018,CVPR,https://arxiv.org/pdf/1711.11543v2,https//embodiedqa.org/data,dataset of visual questions and answers grounded in House3D based on 3D indoor scenes from the SUNCG dataset,"EQA v1 dataset consists of over 5000 question across over 750 environments; referring to a total of 45 unique objects in 7 unique room types; 3 decoding heads that predict 1) original RGB values, 2) semantic class (segmentation is done over 191 classes), and 3) depth for each pixel (range [0,1]",indoor,synthetic,EmbodiedQA is instantiated in House3D -> sourced from SUNCG -> scenes are manually created using Planner5D,191,Embodied Question Answering,74,0,,c
ETH,Depth and Appearance for Mobile Scene Analysis,2007,ICCV,https://ieeexplore.ieee.org/document/4409092,https//data.vision.ee.ethz.ch/cvl/aess/dataset/,dataset for pedestrian detection,1804 images in three video clips; captured from a stereo rig mounted on car; with a resolution of 640 x 480 (bayered); and a framerate of 13--14 FPS,outdoor,real,stereo (RGB) cameras,1,Object Detection; Person Re-Identification; Trajectory Prediction; Pedestrian Detection; Multi-Future Trajectory Prediction,58,5,,c
EuRoC MAV,The EuRoC micro aerial vehicle datasets,2016,IJRR,https://journals.sagepub.com/doi/full/10.1177/0278364915620033?casa_token=o6MzyEPgGq0AAAAA%3AJnQ2Yus_i5_QbBuN7r93Qyw7Hy3q9Ts4PG4830rZFYKoSjaT2ZyXf106ANu_w6aBzlQeLOF1OdRT,https//projects.asl.ethz.ch/datasets/doku.php?id=kmavvisualinertialdatasets,Visual-inertial datasets collected on-board a Micro Aerial Vehicle (MAV),"contains stereo images; synchronized IMU measurements; and accurate motion and structure ground-truth; 2 types of datasets: 1) first batch of datasets was recorded in a large machine hall and aims at testing visual-inertial motion estimation algorithms or SLAM frameworks; for ground truth; a 3D position was provided by a laser tracker; 2) second batch of datasets was recorded in our Vicon room equipped with a motion capture system; For these datasets; additional reference point clouds are available that enable the assessment of multi-view reconstruction approaches; AscTec Firefly MAV, equipped with a visual-inertial sensor unit for data collection",indoor,real,MT9V034 (RGB) Cameras; ADUS16448 IMU; Leica MS50 laser tracker; Vicon motion capture system,0,Visual Odometry ,13,1,,c
EviLOG (Evidential Lidar Occupancy Grid Mapping),A Simulation-based End-to-End Learning Framework for Evidential Occupancy Grid Mapping*,2021,IV,https://arxiv.org/pdf/2102.12718v3,https//hub.com/ika-rwth-aachen/EviLOG#data,The dataset contains synthetic training-validation-test data for occupancy grid mapping from lidar point clouds,10 scenarios; each containing random variations of the dynamic environment; With each scenario; 1000 training samples were created = 10000 training samples in total; 1000 samples for validation; classification into free and occupied (and unknown),outdoor,synthetic,deep learning-based inverse lidar model is created using a simula- tion environment,0,Prediction of Occupancy Grid Maps,1,0,,c
(MPI) FAUST,FAUST: Dataset and Evaluation for 3D Mesh Registration,2014,CVPR,https://files.is.tue.mpg.de/black/papers/FAUST2014.pdf,https//faust-leaderboard.is.tuebingen.mpg.de,novel mesh registration technique that combines 3D shape and appearance information to produce high-quality alignments,"Contains 300 scans of 10 people in a wide range of poses together with an evaluation methodology;  acquisition system is a full-body 3D stereo capture
system (3dMD; Atlanta; GA); composed by 22 scanning units; each unit contains a pair of stereo cameras for 3D shape computation; one or two speckle projectors; a single 5MP RGB camera.",other,real,22 scanning units -> 1 pair of stereo cameras; 1-2 speckle projectors; 1 5MP RGB camera,0,Gaussian Processes; Superpixel Image Classification,13,0,,c
FAUST-partial / FPv1,Addressing the generalization of 3D registration methods with a featureless baseline and an unbiased benchmark,2024,Mach. Vis. Appl.,https://link.springer.com/article/10.1007/s00138-024-01510-w,SourceCodehub.com/DavidBoja/exhaustive-grid-search,3D registration benchmark dataset created to provide a more informative evaluation of 3D registration methods,based on the FAUST dataset; with several difficulty levels; benchmark is created using the FAUST training dataset comprised of 100 3D scans of human bodies; For a given scan from the FAUST dataset: translate its minimal bounding box point to the origin; surround the scan with a regular icosahedron; use icosahedron as a viewpoint used to creade a partial scan using the hidden point removal algorithm,other,real,based on the FAUST dataset -> stereo cameras; speckle projectors; RGB camera,0,Point Cloud Registration,5,9,,c
FEE Corridor,Self-Supervised Depth Correction of Lidar Measurements from Map Consistency Loss,2023,RA-L,https://arxiv.org/pdf/2303.01123v4,http//ptak.felk.cvut.cz/vras/data/fee_corridor/,The data set contains point cloud data captured in an indoor environment with precise localization and ground truth mapping information,2 ”stop-and-go” data sequences are provided of a robot with mounted Ouster OS0-128 lidar; lidar poses are recorded with the theodolite Leica TS15 tracker; point clouds recorded with the Leica BLK360 scanner are provided as mapping ground-truth data;  8 sub-sequences  each containing 50 consecutive scans (from 102 to 151) and corresponding lidar poses. From ,indoor,real,Ouster OS1-128 lidar;  Leica BLK360 scanner ground truth map,0,3D Geometry Perception,1,0,,c
Fishyscapes,The Fishyscapes Benchmark: Measuring Blind Spots in Semantic Segmentation,2019,IJCV,https://arxiv.org/pdf/1904.03215v4,https//fishyscapes.com,public benchmark for uncertainty estimation in a real-world task of semantic segmentation for urban driving,(i) FS Static is based on the validation set of Cityscapes; (ii) FS Web -> images from Cityscapes are overlayed with objects that are regularly crawled from the web in an open-world setup; (iii) Fishyscapes Lost & Found -> builds up on a road hazard dataset collected with the same setup as Cityscapes + supplemented with labels; blending-based reference dataset FS Static (based on the validation set of Cityscapes);  dynamically changing dataset FS Web; Anomalous objects are extracted from the generic Pascal VOC; overlay objects from classes that cannot be found in Cityscapes (aeroplane; bird; boat; bottle; cat; chair; cow; dog; horse; sheep; sofa; tvmonitor);  pixel-wise annotations that distinguish between objects (the 12 anomalies) - background (30 classes contained in Cityscapes) - void (anything not contained in Cityscapes classes that still appears in the training images); public validation set of 100 images and a testset of 275 images,outdoor,synthetic,based on data from Cityscapes (RGB); objects from Pascal VOC (RGB),43,Anomaly Detection,47,2,,c
Five-Billion-Pixels,Enabling Country-Scale Land Cover Mapping with Meter-Resolution Satellite Imagery,2022,ISPRS,https://arxiv.org/pdf/2209.00727v2,https//x-ytong.hub.io/project/Five-Billion-Pixels.html,large-scale land cover dataset,more than 5 billion labeled pixels of 150 high-resolution Gaofen-2 (4 m) satellite images; annotated in a 24-category system (covering artificial-constructed agricultural; natural classes); GF-2 satellite equipped with two panchromatic and multispectral (PMS) sensors; combined swath of 45 km; effective spatial resolution of the sensors is 1 m panchromatic (pan)/4 m multispectral (MS),outdoor,real,Gaofen-2 satellite -> 2 panchromatic + multispectral (PMS) sensors,24,Semantic Segmentation; (Unsupervised) Domain Adaption; (Semantic) Segmentation of Remote Sensing Imagery,3,0,,c
Flickr30K,From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,2014,TACL,https://aclanthology.org/Q14-1006.pdf,https//shannon.cs.illinois.edu/DenotationGraph/,using the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics,"31783 photographs of everyday activities-events-scenes
(all harvested from Flickr); 158915 captions obtained via crowdsourcing",mixed,real,(RGB) images collected from Flickr,0,Node Classification; Image Retrieval; Image Captioning; Cross-Modal Retrieval; Image-to-Text Retrieval…,839,10,,c
Flickr30K Entities,Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models,2015,ICCV,https://arxiv.org/pdf/1505.04870v4,https//bryanplummer.com/Flickr30kEntities/,extension to the Flickr30K dataset; used to define a new benchmark for localization of textual entity mentions in an image,"extension to the Flickr30K dataset; it augments the original 158k captions with 244k coreference chains; linking mentions of the same entities across different captions for the same image; and associating them with 276k manually annotated bounding boxes; we define Object Categories as the set of unique phrases after
filtering out non-nouns in our annotated phrases ",mixed,real,(RGB) images collected from Flickr,"44,518",Phrase Grounding,141,2,,c
Foggy Cityscapes,Semantic Foggy Scene Understanding with Synthetic Data,2018,IJCV,https://arxiv.org/pdf/1708.07819v3,https//people.ee.ethz.ch/~csakarid/SFSU_synthetic/,synthetic foggy dataset which simulates fog on real scenes,"(i) Foggy Cityscapes: derived from Cityscapes dataset ->  refined set of 550 images; 498 from the training set and 52 from the validation set; annotations for 8 classes (person; rider; car; truck; bus; train; motorcycle and bicycle); (ii) Foggy Driving: 101 color images depicting real-world foggy driving scenes -> they captured 51 of these images with a cell phone camera in foggy conditions at various areas of Zurich; the rest 50 images were carefully collected from the web; dense, pixel-level semantic annotations for all images of Foggy Driving -> the 19 semantic classes of Cityscapes (road; sidewalk;
building; wall; fence; pole; traffic light; traffic sign; vegetation; terrain; sky; person; rider; car; train;
motorcycle; bicycle) + labels for individual
instances (person; rider; car; truck; bus; train; motorcycle and bicycle)",outdoor,synthetic,(RGB) images from Cityscapes; (RGB) phone images,19,Object Detection; Domain Adaption; Image-to-Image Translation; Foggy Scene Segmentation,239,6,,c
Foggy KITTI,MonoWAD: Weather-Adaptive Diffusion Model for Robust Monocular 3D Object Detection,2024,ECCV,https://arxiv.org/pdf/2407.16448v1,https//hub.com/VisualAIKHU/MonoWAD,"The Foggy KITTI dataset extends the KITTI dataset to include challenging weather conditions, aiming to support research in real-world applications such as autonomous driving.",they generate foggy images from all images in the KITTI dataset (called foggy KITTI); that emulate the foggy scene; 3712 training images and 3769 validation images; detection of the car category,outdoor,synthetic,based on KITTI dataset (RGB images),1,Object Detection; Autonomous Driving,1,0,,c
FOR-instance,FOR-instance: a UAV laser scanning benchmark dataset for semantic and instance segmentation of individual trees,2023,arXiv,https://arxiv.org/pdf/2309.01279v1,https//zenodo.org/records/8287792,accurately segmenting individual trees from laser scanning data,forest laser scanning point clouds from unamnned aerial vehicle (using Riegl sensors) that are manually segmented according to the individual trees (1130 trees) and semantic classes; The point clouds are subdivided into five data collections representing different forests (in Norway; the Czech Republic; Austria; New Zealand; Australia); UAV-LS data were acquired with two sensors: the Riegl VUX-1UAV and the Riegl MiniVUX-1 UAV; data were fully annotated into single trees (i.e. instance annotation) and different parts of the trees and the forest (i.e. semantic annotation -> 0 Unclassified; 1 Low-vegetation; 2 Terrain; 3 Out-points; 4 Stem; 5 Live branches; 6 Woody branches) using CloudCompare,outdoor,real,2 sensors (LiDAR): Riegl VUX-1UAV; Riegl MiniVUX-1 UAV ,6,3D Semantic Segmentation; 3D Instance Segmentation,7,0,,c
Ford Campus Vision and Lidar Data Set,Ford Campus vision and lidar data set,2011,IJRR,https://journals.sagepub.com/doi/10.1177/0278364911400640,https//robots.engin.umich.edu/Main/Login,dataset collected by an autonomous ground vehicle testbed,autonomous ground vehicle testbed based upon a modified Ford F-250 pickup truck (outfitted with a professional (Applanix POS-LV) and consumer (Xsens MTi-G) inertial measurement unit; a Velodyne three-dimensional lidar scanner; two push-broom forward-looking Riegl lidars; a Point Grey Ladybug3 omnidirectional camera system); time-registered data from these sensors mounted on the vehicle; number of points per scan is typically 80000–100000,outdoor,real,Applanix POS-LV and Xsens MTi-G inertial measurement unit; 1 Velodyne 3D lidar scanner; 2 push-broom forward-looking Riegl lidars; 1 Point Grey Ladybug3 omnidirectional camera system (RGB),0,Computer Vision; Simultaneous Localization and Mapping,3,0,,c
FPL (First-Person Locomotion),Future Person Localization in First-Person Videos,2018,CVPR,https://openaccess.thecvf.com/content_cvpr_2018/papers/Yagi_Future_Person_Localization_CVPR_2018_paper.pdf,https//hub.com/takumayagi/fpl,Supports new task that predicts future locations of people observed in first-person videos,about 4.5 hours of first-person videos recorded by people wearing a chest-mounted camera and walking around in diverse environments; number of observed people is more than 5000 in total; categorized according the walking direction (Toward; Away; Across),outdoor,real,wearable camera (RGB),0,Trajectory Prediction; Motion Forecasting; Trajectory Forecasting,7,0,,c
Freiburg Campus 3D Scan,Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing,2020,IJRR,http://ais.informatik.uni-freiburg.de/publications/papers/wurm10octomap.pdf,http//ais.informatik.uni-freiburg.de/projects/datasets/octomap/,The Freiburg Campus 3D Scan dataset consists of 3D area maps from the Freiburg campus, scanned with 3D lasers; Areas: corridors; outdoor campus; some of the colleges and buildings; A) small-scale indoor -> 3.5 x 5.2 x 1.7 m^3; B) corridor 079 at Freiburg campus (FR-079) -> 66 scans; 43.8 x 18.2 x 3.3 m^3; C)Freiburg outdoor -> 81 dense 3D scans covering an area of 292 m × 167 m x 28m,outdoor,real,SICK LMS laser range scanner,0,Robot mapping; navigation; and mobile manipulation,0,0,,c
Freiburg Spatial Relations,Metric Learning for Generalizing Spatial Relations to New Objects,2017,IROS,https://arxiv.org/pdf/1703.01946v3,http//spatialrelations.cs.uni-freiburg.de,Their approach enables a robot to reason about the similarity between pairwise spatial relations; thereby enabling it to use its previous knowledge when presented with a new relation to imitate,546 scenes each containing two out of 26 household objects; we recorded 3D models of 26 household objects and used SimTrack to detect them and compute their poses in a scene using a Kinect camera,indoor,real,Kinect camera (RGB-D),26,Metric Learning,2,0,,c
Freiburg Street Crossing,Multimodal Interaction-aware Motion Prediction for Autonomous Street Crossing,2018,IJRR,https://arxiv.org/pdf/1808.06887v5,http//aisdatasets.informatik.uni-freiburg.de/streetcrossing/,dataset consists of data collected from three different street crossings in Freiburg; Germany; ; two of which were traffic light regulated intersections and one a zebra crossing without traffic lights; The data can be used to train agents to cross roads autonomously.,data was captured over the course of two weeks and it is divided into 10 different sequences containing approximately over 2000 tracked objects; Each object is identified by a unique track ID; spatial coordinates; velocity and orientation angle; tracking traffic light states (Red; Green; Off),outdoor,real,"(RGB) images, LIDAR as well as RADAR data",,Motion Prediction,1,0,,c
Fusion 360 Gallery,Fusion 360 Gallery: A Dataset and Environment for Programmatic CAD Construction from Human Design Sequences,2021,TOG,https://arxiv.org/pdf/2010.02392v2,https//hub.com/AutodeskAILab/Fusion360GalleryDataset,Dataset contains rich 2D and 3D geometry data derived from parametric CAD models,8625 designs produced by users of the CAD software Autodesk Fusion 360 and submitted to the publicly available Autodesk Online Gallery,other,synthetic,models produced with the CAD software Autodesk,0,B-Rep face segmentation,19,2,,c
FutureHouse,PhyIR: Physics-Based Inverse Rendering for Panoramic Indoor Images,2022,CVPR,https://openaccess.thecvf.com//content/CVPR2022/papers/Li_PhyIR_Physics-Based_Inverse_Rendering_for_Panoramic_Indoor_Images_CVPR_2022_paper.pdf,https//yodlee.top/PhyIR/,large-scale synthetic photorealistic panoramic dataset,over 70000 high-quality models with high-resolution meshes and physical material (measured in real world standards); Selected scene layouts are carefully designed by over 100 excellent artists; 28579 panoramic views from 1752 house-scale scenes; geometric + material + lighting + light source  annotation,indoor,synthetic,manually designed CAD models and scenes; rendered in Unreal Engine 4,0,Depth Estimation; Surface Normal Estimation; Instrinsic Image Decomposition; Neural Rendering,3,0,,c
G-VUE (General-purpose Visual Understanding Evaluation),Perceive; Ground; Reason; and Act: A Benchmark for General-purpose Visual Representation,2022,arXiv,https://arxiv.org/pdf/2211.15402v1,https//hub.com/wllmzhu/G-VUE,comprehensive benchmark covering the full spectrum of visual cognitive abilities with four functional domains -- Perceive; Ground; Reason; and Act,The four domains are embodied in 11 carefully curated tasks; from 3D reconstruction to visual reasoning and manipulation; visual tasks and datasets are selected for each domain: Perceive (NYUv2; CL & 7-scenes; ShapeNetCore); Ground (Flickr30K; RefCOCO; ADE20k); Reason (GQA; VCR; Bongard-HOI); Act (R2R; Ravens); Semantic segmentation task: ADE20K + corresponding train/test split from the MIT Scene Segmentation Benchmark -> 150 categories of stuff (e.g. sky) and discrete objects (e.g. car) with their corresponding semantic masks on each image,mixed,mixed,RGB-D images; 3D models from databases,150,Semantic Segmentation; Question Answering; Depth Estimation; Common Sense Reasoning; 3D Reconstruction; Visual Reasoning; Phrase Grounding; Navigate,2,0,,c
Gibson Database / Gibson Environment ,Gibson Env: Real-World Perception for Embodied Agents,2018,CVPR,https://openaccess.thecvf.com/content_cvpr_2018/papers/Xia_Gibson_Env_Real-World_CVPR_2018_paper.pdf,http//gibsonenv.stanford.edu,Gibson is an opensource perceptual and physics simulator to explore active and real-world perception,virtual reconstruction of real-world data; 572 full buildings composed of 1447 floors covering a total area of 211 km^2; (for each space: 3D reconstruction; RGB images; depth; surface normal); for a fraction of the spaces: semantic object annotations; The spaces in Gibson database are collected using various scanning devices (incl.NavVis; Matterport; DotProduct) covering a diverse set of spaces (e.g. offices; garages; stadiums; grocery stores; gyms; hospitals; houses); All spaces are fully reconstructed in 3D and post processed to fill the holes and enhance the mesh,indoor,other,RGB-D cameras,0,Self-Supervised Learning; Robot Navigation; Hierarchical Reinforcement Learning,22,0,,c
GQA,GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering,2019,CVPR,https://arxiv.org/pdf/1902.09506v3,https//cs.stanford.edu/people/dorarad/gqa/,large-scale visual question answering dataset with real images from the Visual Genome dataset and balanced question-answer pairs,"113K images and 22M multi-step questions that require a diverse set of reasoning skills; with both binary and open questions (they leveraged Visual Genome scene graph structures to create questions); clean-consolidated-unambiguous ontology over the graph with 2690 classes including various objects, attributes and relations",mixed,real,based on Visual Genome -> RGB images + questions,2690,Visual Question Answering (VQA); Graph Question Answering,643,6,,c
GQA-REX,REX: Reasoning-aware and Grounded Explanation,2022,CVPR,https://arxiv.org/pdf/2203.06107v1,https//hub.com/szzexpoi/rex,A GQA-based dataset with 1040830 multi-modal explanations of visual reasoning processes,GQA + 1040830 multi-modal explanations of visual reasoning processes (#XAI); 1660 unique types of object categories,mixed,real,based on GQA -> RGB images + questions + explanations,1660,Explanatory Visual Question Answering,8,1,,c
GSO (Google Scanned Objects),Google Scanned Objects: A High-Quality Dataset of 3D Scanned Household Items,2022,ICRA,https://arxiv.org/pdf/2204.11918v1,https//app.gazebosim.org/GoogleResearch/fuel/collections/Scanned%20Objects%20by%20Google%20Research,dataset of common household objects that have been 3D scanned for use in robotic simulation and synthetic perception research,1030 scanned objects from 17 categories + associated metadata; totalling 13Gb; licensed under the CCBY 4.0 License; Visual meshes are in Wavefront OBJ format averaging 1.4MB (min: 0.1MB; max: 11.1MB) per model; The accompanying diffuse texture maps are in PNG format; and average 11.2MB (min: 6.5MB ; max: 23.5MB) per texture; The lighting-controlled physical enclosure captures 3D geometry using structured light scanning with two machine vision cameras and a projector and captures textures with product-friendly lighting with a separate DSLR high-resolution camera,other,real,structured light scanning; 2 machine vision cameras; 1 projector; DSLR high-resolution camera (textures),17,Simulation; Single-View 3D Reconstruction,91,1,,c
gRefCOCO,GRES: Generalized Referring Expression Segmentation,2023,CVPR,https://openaccess.thecvf.com/content/CVPR2023/papers/Liu_GRES_Generalized_Referring_Expression_Segmentation_CVPR_2023_paper.pdf,https//henghuiding.hub.io/GRES/,large-scale Generalized Referring Expression Segmentation dataset that contains multi-target; no-target; and single-target expressions,"278232 expressions; including 80022 multi-target and 32202 no-target expressions; referring to 60287 distinct instances in 19994 images; Masks and
bounding boxes for all target instances are given; The basic annotation procedure follows ReferIt to ensure the annotation quality",mixed,real,RGB images + natural language expressions,0,Generalized Referring Expression Segmentation; Generalized Referring Expression Comprehension,30,2,,c
GTA5 (Grand Theft Auto 5),Playing for Data: Ground Truth from Computer Games,2016,ECCV,https://arxiv.org/pdf/1608.02192v1,,use of commercial video games for creating large-scale pixel-accurate ground truth data for training semantic segmentation systems,24966 synthetic images with pixel level semantic annotation; The images have been rendered using the open-world video game Grand Theft Auto 5 and are all from the car perspective in the streets of American-style virtual cities; There are 19 semantic classes which are compatible with the ones of Cityscapes dataset,outdoor,synthetic,(RGB) images rendered using GTA 5,19,Semantic Segmentation; Domain Adaption; Image-to-Image Translation; Domain Adaption; Synthetic-to-real Translation…,405,8,,c
H3D (Honda Research Institute 3D),The H3D Dataset for Full-Surround 3D Multi-Object Detection and Tracking in Crowded Urban Scenes,2019,ICRA,https://arxiv.org/pdf/1903.01568v1,https//usa.honda-ri.com/H3D,large scale full-surround 3D multi-object detection and tracking dataset; gathered from HDD dataset; a large scale naturalistic driving dataset collected in San Francisco Bay Area,Full 360 degree LiDAR dataset (dense pointcloud from Velodyne-64); 160 crowded and highly interactive traffic scenes; 1;071;302 3D bounding box labels; 8 common classes of traffic participants (Manually annotated every 2Hz and linearly propagated for 10 Hz data),outdoor,real,,,(3D) Object Detection; Autonomous Driving,37,1,,
HANDAL,HANDAL: A Dataset of Real-World Manipulable Object Categories with Pose Annotations; Affordances; and Reconstructions,2023,IROS,https://arxiv.org/pdf/2308.01477v1,https//nvlabs.hub.io/HANDAL/,dataset for category-level object pose estimation and affordance prediction,308k annotated image frames from 2.2k videos of 212 real-world objects in 17 categories;  focus on hardware and kitchen tool objects,indoor,real,,,Object Pose Estimation,4,0,,
Headcam,Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic Video,2019,AIVR,https://arxiv.org/pdf/1901.00979v2,https//hub.com/jonathanventura/cylindricalsfmlearner,panoramic video captured from a helmet-mounted camera while riding a bike through suburban Northern Virginia,footage was first stitched into equirectangular panoramic video using the Samsung Gear 360 software; It was then broken into frames at 5fps; warped into a cylindrical projection model; resized to 512 × 128; and formatted into three-frame sequences; The final formatted dataset contains 27;538 frames; Training was conducted on 90% of the data; qualitative testing was done on the remaining 10%,outdoor,real,,,Depth Estimation; Motion Estimation,2,0,,
Heavy Snowfall (DENSE),Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather,2020,CVPR,https://arxiv.org/pdf/1902.08913v3,https//www.uni-ulm.de/en/in/driveu/projects/dense-datasets#c811669,object detection dataset in challenging adverse weather conditions,12000 samples in real-world driving scenes; 1500 samples in controlled weather conditions within a fog chamber; different weather conditions (like fog; snow; rain); acquired by over 10;000 km of driving in northern Europe; 100k objects were labeled with 2D / 3D bounding boxes,outdoor,real,RGB cameras (2 OnSemi AR0230 imagers); NIR camera ( BrightwayVision BrightEye); (FMCW) radar; LiDAR (Velodyne HDL64 S3D and VLP32C); Q1922 FIR camera; environmental sensors (Airmar WX150 weather station),0,(3D) Object Detection ,4,1,,c
HePIC,Fully Automated Scan-to-BIM via Point Cloud Instance Segmentation,2023,ICIP,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10222064,https//hub.com/LTTM/Scan-to-BIM,Heritage Pointcloud Instance Collection dataset; acquired from two large buildings and annotated at a point-wise semantic level based on existent BIM models,two large buildings: a Castle and a Church;  91 (68 + 23) rooms; each composed of 100k points with both semantic and instance labels; automatically assigned by comparing the point cloud with the BIM model obtained from it; Eight classes were considered: walls; floors; roofs; doors; windows; beams; columns; stairs; Any point not belonging to one of these classes was marked as unassigned,outdoor,real,,,(3D) Semantic Segmentation; (3D) Instance Segmentation,1,0,,
HEV-I (Honda Egocentric View-Intersection Dataset),Egocentric Vision-based Future Vehicle Localization for Intelligent Driving Assistance Systems,2019,ICRA,https://arxiv.org/pdf/1809.07408v2,https//usa.honda-ri.com/hevi,introduced to enable research on traffic participants interaction modelling; future object localization; as well as learning driver action in challenging driving scenarios,230 video clips of real human driving in different intersections from the San Francisco Bay Area; collected using an instrumented vehicle equipped with different sensors including cameras; GPS/IMU; and vehicle states signals,outdoor,real,,,Trajectory Prediction,5,1,,
HM3D (Habitat-Matterport 3D),Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI,2021,arXiv,https://arxiv.org/pdf/2109.08238v1,https//hub.com/facebookresearch/habitat-lab#HM3D,large-scale dataset of 1;000 building-scale 3D reconstructions from a diverse set of real-world locations. Each scene in the dataset consists of a textured 3D mesh reconstruction of interiors such as multi-floor residences; stores; and other private indoor spaces,1;000 building-scale 3D reconstructions from a diverse set of real-world locations; 112.5k m^2 of navigable space,indoor,real,,,Embodied AI,69,0,,
HM3DSem,Habitat-Matterport 3D Semantics Dataset,2023,CVPR,https://arxiv.org/pdf/2210.05633,https//aihabitat.org/datasets/hm3d-semantics/,dataset of semantically-annotated 3D indoor spaces,dense semantic annotations for 216 high-resolution; 3D; scanned scenes from the Habitat-Matterport 3D Dataset (HM3D). The HM3D scenes are annotated with 142;646 raw object names additionally mapped to the 40 Matterport 3D categories,indoor,real,,,Embodied AI (Home Robots; AI Assistants); Semantic Navigation,1,0,,
HOI(Hand-Object Interaction)-Synth,Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection,2024,ECCV,https://arxiv.org/pdf/2312.02672v3,https//fpv-iplab.hub.io/HOI-Synth/,The HOI-Synth benchmark extends three egocentric datasets designed to study hand-object interaction detection; EPIC-KITCHENS VISOR; EgoHOS; and ENIGMA-51; with automatically labeled synthetic data obtained through a novel HOI generation pipeline,A) EPIC-KITCHENS VISOR: 36 hours of egocentric videos from EPIC-KITCHENS-100; including 32;857 training images and pixel-wise annotations for 42;787 hand-object relations -> complemented with 30;259 synthetic images including 45;353 HOIs; B) EgoHOS: 8;107 egocentric training images of HOIs sparsely sampled from videos belonging to EGO4D; THU-READ; EPIC-KITCHENS; and other egocentric videos of people playing escape rooms -> labeled with pixel-wise annotations of 13;659 hand-object relations; complemented with 8;107 synthetic images including 12;129 HOIs; C) ENIGMA-51: egocentric dataset of subjects following instructions to repair electrical boards in an industrial laboratory. The dataset contains 3;479 training images with pixel-wise annotations of 13;659 hand-object interactions -> complemented with 2 sets of synthetic images: an in-domain set and an out-domain set,indoor,synthetic,,,Hand-Object Interaction Detection,2,0,,
HoME (Household Multimodal Envisonment),HoME: a Household Multimodal Environment,2017,arXiv,https://arxiv.org/pdf/1711.11017v1,https//home-platform.hub.io,multimodal environment for artificial agents to learn from vision; audio; semantics; physics; and interaction with objects and other agents; all within a realistic context,over 45;000 diverse hand-designed 3D house layouts based on the SUNCG dataset,indoor,synthetic,,,reinforcement learning; language grounding; sound-based navigation; robotics; multi-agent learning,22,0,,
HomebrewedDB,HomebrewedDB: RGB-D Dataset for 6D Pose Estimation of 3D Objects,2019,ICCV,https://arxiv.org/pdf/1904.03167v2,https//campar.in.tum.de/personal/ilic/homebreweddb/index.html,dataset for 6D pose estimation mainly targeting training from 3D models (both textured and textureless); scalability; occlusions; and changes in light conditions and object appearance,33 objects (17 toy; 8 household and 8 industry-relevant objects) over 13 scenes of various difficulty; + set of benchmarks to test various desired detector properties; particularly focusing on scalability with respect to the number of objects and resistance to changing light conditions; occlusions and clutter,indoor,,,,6D Pose Estimation,26,0,,
HOMER (Household Object Movements from Everyday Routines),Proactive Robot Assistance via Spatio-Temporal Object Modeling,2022,arXiv,https://arxiv.org/pdf/2211.15501v1,https//hub.com/Maithili/SpatioTemporalObjectTracking,They formulate proactive assistance as the problem of the robot anticipating temporal patterns of object movements associated with everyday user routines; and proactively assisting the user by placing objects to adapt the environment to their needs,routine behaviors for five households; spanning 50 days for the train split and 10 days for test split; The households are based on an identical apartment setting with four rooms and 108 objects and 33 atomic actions such as find; grab; etc.,indoor,,,,Object Tracking,3,0,,
House3D Environment,Building Generalizable Agents with a Realistic and Rich 3D Environment,2018,ICLR,https://arxiv.org/pdf/1801.02209v2,https//hub.com/facebookresearch/House3D,virtual 3D environment which consists of thousands of indoor scenes equipped with a diverse set of scene types,layouts and objects sourced sourced from the SUNCG dataset -> 45622 human-designed 3D scenes ranging from single-room studios to multi-floor houses; 8.9 rooms and 1.3 floors per scene on average; over 20 different room types (e.g. bedroom; living room; kitchen; bathroom etc.) with over 80 different object categories; 404508 different rooms and 5697217 object instances drawn from 2644 unique object meshes,indoor,synthetic,sourced from the SUNCG dataset -> scenes are manually created using Planner5D,84,Question Answering; Visual Navigation; Efficient Exploration,11,0,,c
HouseExpo,HouseExpo: A Large-scale 2D Indoor Layout Dataset for Learning-based Algorithms on Mobile Robots,2020,IROS,https://arxiv.org/pdf/1903.09845v1,https//hub.com/teaganli/houseexpo/,large-scale indoor layout dataset,35;357 2D floor plans including 252;550 rooms,indoor,,,,Motion Planning,10,0,,
Housekeep,Housekeep: Tidying Virtual Households using Commonsense Reasoning,2022,ECCV,https://arxiv.org/pdf/2205.10712v1,https//yashkant.hub.io/housekeep/,benchmark to evaluate common sense reasoning in the home for embodied AI,dataset contains where humans typically place objects in tidy and untidy houses constituting 1799 objects; 268 object categories; 585 placements; and 105 rooms,indoor,,,,Language Modelling; Common Sense Reasoning,9,0,,
Houses3K,Next-Best View Policy for 3D Reconstruction,2020,ECCV,https://arxiv.org/pdf/2008.12664v2,https//hub.com/darylperalta/Houses3K,dataset of 3D house models from the 2020 ECCV Workshop paper Next-Best View Policy for 3D Reconstruction,3000 textured 3D house models; divided into 12 batches; each containing 50 unique house geometries; For each batch; 5 different textures were applied forming the sets (A; B; C; D; E),indoor,,,,3D Reconstruction,4,0,,
Hypersim,Hypersim: A Photorealistic Synthetic Dataset for Holistic Indoor Scene Understanding,2021,ICCV,https://arxiv.org/pdf/2011.02523v5,https//hub.com/apple/ml-hypersim,photorealistic synthetic dataset for holistic indoor scene understanding,77;400 images of 461 indoor scenes with detailed per-pixel labels and corresponding ground truth geometry,indoor,synthetic,,,2D / 3D Object Detection; (3D) Semantic Segmentation; Instance Segmentation; Depth Estimation; (3D) Panoptic Segmentation; 3D Reconstruction; Monocular Depth Estimation; Multi-Task Leraning; Single-View 3D Reconstruction; 3D Pose Estimation; 3D Shape Reconstruction; Inverse Rendering; Intrinsic Image Decomposition; 2D Shape Recognition,72,1,,
IFCNet,IFCNet: A Benchmark Dataset for IFC Entity Classification,2021,EG-ICE,https://arxiv.org/pdf/2106.09712v1,https//ifcnet.e3d.rwth-aachen.de,A Benchmark Dataset for IFC (Industry Foundation Classes) Entity Classification,full IFCNet dataset: 19;613 confirmed objects distributed over 65 classes; most of which are highly imbalanced with respect to the number of objects they contain; IFCNetCore: subset of 7;930 objects,indoor,,,,IFC Entity Classification,2,1,,
iGibson 2.0,iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday Household Tasks,2021,arXiv,https://arxiv.org/pdf/2108.03272v4,https//svl.stanford.edu/igibson/,open-source simulation environment that supports the simulation of a more diverse set of household tasks through three key innovations: 1) iGibson 2.0 supports object states; including temperature; wetness level; cleanliness level; and toggled and sliced states; 2) iGibson 2.0 implements a set of predicate logic functions that map the simulator states to logic states like Cooked or Soaked; 3) iGibson 2.0 includes a virtual reality (VR) interface to immerse humans in its scenes to collect demonstrations,15 fully interactive high quality scenes; hundreds of large 3D scenes reconstructed from real homes and offices; and compatibility with datasets like CubiCasa5K and 3D-Front; providing 12000+ additional interactive scenes,indoor,,,,Robot Learning,10,0,,
IKEA Object State Dataset,IKEA Object State Dataset: A 6DoF object pose estimation dataset and benchmark for multi-state assembly objects,2021,arXiv,https://arxiv.org/pdf/2111.08614v1,https//hub.com/mxllmx/IKEAObjectStateDataset,dataset that contains IKEA furniture 3D models; RGBD video of the assembly process; the 6DoF pose of furniture parts and their bounding box.,IKEA 3D models corresponding to 5 IKEA products in final assembly state; where all parts a customer interact with during assembly is included,indoor,,,11,IKEA Furniture Assembly,2,0,,
ImageNet,ImageNet: A Large-Scale Hierarchical Image Database,2009,CVPR,https://ieeexplore.ieee.org/document/5206848,https//www.image-net.org,The ImageNet dataset contains 14;197;122 annotated images according to the WordNet hierarchy. Since 2010 the dataset is used in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC); a benchmark in image classification and object detection. ,Total number of non-empty WordNet synsets: 21841; Total number of images: 14197122; Number of images with bounding box annotations: 1;034;908; Number of synsets with SIFT features: 1000; Number of images with SIFT features: 1.2 million,mixed,,,,Image Classification; Image Generation; Zero-Shot Learning; Visual Question Answering (VQA); Image Super-Resolution; Few-Shot Learning; Image Clustering; Neural Architecture Search; Weakly Supervised Object Detection; Image Inpainting;  Binarization; Prompt Engineering; Object Recognition; Quantization; Medical Image Classification; Object Localization; Image Deblurring; Composed Image Retrieval; Network Pruning; Data Augmentation; Image Compressed Sensing; Model Compression; Sparse Learning; Image Colorization; JPEG Decompression; Feature Upsampling...,14103,43,,
InLUT3D (Indoor Lodz University of Technology Point Cloud Dataset),InLUT3D: Challenging real indoor dataset for point cloud analysis,2024,arXiv,https://arxiv.org/pdf/2408.03338,https//zenodo.org/records/8131487,point cloud set tailored for real object classification and both semantic and instance segmentation tasks,321 scans; some areas in the dataset are covered by multiple scans; 18 distinct categories (ceiling; floor; wall; stairs; column; chair; sofa; table; storage; door; window; plant; dish; wallmounted; device; radiator; lighting; other),indoor,,,,3D Semantic Segmentation; 3D Instance Segmentation; 3D Classification,0,0,,
InteriorNet,InteriorNet: Mega-scale Multi-sensor Photo-realistic Indoor Scenes Dataset,2018,arXiv,https://arxiv.org/pdf/1809.00716v1,https//interiornet.org,synthetic RGB-D dataset for large scale interior scene understanding and mapping,20M photo-realistic images created by pipeline: A) the authors collected around 1 million CAD models provided by world-leading furniture manufacturers; B) around 1;100 professional designers created around 22 million interior layouts; C) For each layout; different configurations were generated (representing random lightings; simulating scene change over time in daily life); D) interactive simulator (ViSim) to create ground truth IMU; events; and monocular or stereo camera trajectories including hand-drawn; random walking and neural network based realistic trajectory; E) All supported image sequences and ground truth,indoor,synthetic,,,Object Detection; Semantic Segmentation; Simultaneous Localization and Mapping,29,0,,
Interiorverse,Learning-based Inverse Rendering of Complex Indoor Scenes with Differentiable Monte Carlo Raytracing,2022,SIGGRAPH Asia,https://arxiv.org/pdf/2211.03017v2,https//jingsenzhu.hub.io/invrend/#,Interiorverse is a high-quality synthetic indoor scene dataset with rich details; including complex furniture and decorations and it is rendered with GGX BRDF model; which has stronger material modeling capability than any BRDF models,thounsands of well-designed indoor scenes; synthetic ground truths of material; geometry; spatially-varying lighting,indoor,synthetic,,,Complex Object Insertion; Material Editing (with high fidelity),3,0,,
IQUAD (Interactive Question Answering Dataset),IQA: Visual Question Answering in Interactive Environments,2018,CVPR,https://arxiv.org/pdf/1712.03316v3,https//hub.com/danielgordon10/thor-iqa-cvpr-2018,dataset for Visual Question Answering in interactive environments; built upon AI2-THOR; a simulated photo-realistic environment of configurable indoor scenes with interactive object,To evaluate HIMN; we introduce IQUAD V1; a new dataset built upon AI2-THOR; a simulated photo-realistic environment of configurable indoor scenes with interactive objects; IQUAD V1 has 75;000 questions; each paired with a unique scene configuration,indoor,synthetic,,,Question Answering; Visual Question Answering (VQA); Visual Navigation,7,0,,
ISPRS 3D (ISPRS Vaihingen),Contextual classification of lidar data and building object detection in urban areas,2014,ISPRS,https://www.sciencedirect.com/science/article/pii/S0924271613002359,https//www.isprs.org/education/benchmarks/UrbanSemLab/3d-semantic-labeling.aspx,3D semantic labeling contest; airborne laser scanning data from Vaihingen,9 classes (Powerline; Low vegetation; Impervious surfaces; Car; Fence/Hedge; Roof; Facade; Shrub; Tree); each point in the dataset is labeled accordingly; area is subdivided into 2 parts -> both: simple ASCII file with XYZ coordinates; reflectance; return count information; one area: reference information; second area: reference for evaluating participants' results.,outdoor,real,,,Semantic Segmentation,16,1,,
JAAD (Joint Attention in Autonomous Driving),Agreeing to Cross: How Drivers and Pedestrians Communicate,2017,IV,https://arxiv.org/pdf/1702.03555v1,https//ieeexplore.ieee.org/document/8265243,dataset for studying joint attention in the context of autonomous driving;  focus on pedestrian and driver behaviors at the point of crossing and factors that influence them,richly annotated collection of 346 short video clips (5-10 sec long) extracted from over 240 hours of driving footage; Bounding boxes with occlusion tags are provided for all pedestrians;  For each video: several tags (weather; locations; etc.) and timestamped behavior labels from a fixed list (e.g. stopped; walking; looking; etc.); for each pedestrian: list of demographic attributes for each pedestrian (e.g. age; gender; direction of motion; etc.) for each frame: list of visible traffic scene elements (e.g. stop sign; traffic signal; etc.) ,outdoor,real,,,Trajectory Prediction; Multi-future Trajectory Prediction; Trajectory Forecasting; Pedestrian Trajectory Prediction,21,2,,
K-Lane (KAIST-Lane),K-Lane: Lidar Lane Dataset and Benchmark for Urban Roads and Highways,2022,CVPR,https://arxiv.org/pdf/2110.11048v3,https//hub.com/kaist-avelab/K-Lane,public urban road and highway lane dataset for Lidar,K-Lane has more than 15K frames and contains annotations of up to six lanes under various road and traffic conditions; e.g.; occluded roads of multiple occlusion levels; roads at day and night times; merging (converging and diverging) and curved lanes,outdoor,,,,Lane Detection,3,1,,
KAIST (multi-spectral Day/Night 2018),KAIST Multi-Spectral Day/Night Data Set for Autonomous and Assisted Driving,2018,T-ITS,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8293689&casa_token=Ui8JWZJfCA8AAAAA:0MATD4yp3Yb3cPWZZD3mmnkiXrTWLs3L-3_tZ5Zva9g5qAqXW74o5E9yZaY9ZtugaEU0ECCcw9hh,https//sites.google.com/view/multispectral/home,multi-spectral dataset; which covers a greater range of drivable regions; from urban to residential; for autonomous systems; the dataset provides different perspectives of the world captured in coarse time slots (day and night) in addition to fine time slots (sunrise; morning; afternoon; sunset; night and dawn),we captured various regular traffic scenes at day and night time to consider changes in light conditions. and; consists of 95k color-thermal pairs (640x480; 20Hz) taken from a vehicle. All the pairs are manually annotated (person; people; cyclist) for the total of 103;128 dense annotations and 1;182 unique pedestrians. The annotation includes temporal correspondence between bounding boxes like Caltech Pedestrian Dataset.,outdoor,,,,Pedestrian Detection,0,0,,
KeypointNet,KeypointNet: A Large-scale 3D Keypoint Dataset Aggregated from Numerous Human Annotations,2020,CVPR,https://arxiv.org/pdf/2002.12687v6,https//hub.com/qq456cvb/KeypointNet,large-scale and diverse 3D keypoint dataset,contains 103;450 keypoints and 8;234 3D models from 16 object categories; ; by leveraging numerous human annotations; based on ShapeNet models,indoor,,,,Pose Estimation; Keypoint Detection; 3D Shape Representation,23,0,,
Kitchen Scenes,Multiview RGB-D Dataset for Object Instance Detection,2016,3DV,https://arxiv.org/pdf/1609.07826v1,https//cs.gmu.edu/~robot/gmu-kitchens.html,multi-view RGB-D dataset of nine kitchen scenes; each containing several objects in realistic cluttered environments including a subset of objects from the BigBird dataset,9 videos from realistic kitchen environments; avg. 13 objects per scene; avg. 748 frames per scene; avg. 10.8x10^6 3D points per scene,indoor,,,,Object Detection (in Indoor Scenes); Image Retrieval,4,0,,
KITTI (Karlsruhe Institute of Technology and Toyota Technological Institute),Are we ready for autonomous driving? The KITTI vision benchmark suite,2012,CVPR,https://ieeexplore.ieee.org/document/6248074,https//www.cvlibs.net/datasets/kitti/,one of the most popular datasets for use in mobile robotics and autonomous driving,389 stereo and optical flow image pairs; stereo visual odometry sequences of 39.2 km length; more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image); standard station wagon  equipped with 2 color and 2 grayscale PointGrey Flea2 video cameras (10 Hz; resolution: 1392×512 pixels; opening: 90◦ ×35◦); 1 Velodyne HDL-64E 3D laser scanner (10 Hz; 64 laser beams; range: 100 m); 1 GPS/IMU localization unit with RTK correction signals (open sky localization errors < 5 cm) 1 powerful computer running a real-time database; 8 object classes; 16 orientation classes,outdoor,real, 2 color (RGB) cameras; 2 grayscale PointGrey Flea2 video cameras ; 1 Velodyne HDL-64E 3D laser scanner; 1 GPS/IMU localization unit with RTK correction signals,8,(3D) Object Detection; Semantic Segmentation; Pose Estimation; Object Tracking; Image Super-Resolution; Image-to-Image Translation; Image Clustering; Depth Estimation; Object Localization; Visual Place Recognition; Panoptic Segmentation; Point Cloud Registration...,3529,140,,c
KITTI-360,KITTI-360: A Novel Dataset and Benchmarks for Urban Scene Understanding in 2D and 3D,2023,TPAMI,https://arxiv.org/pdf/2109.13410v2,https//www.cvlibs.net/datasets/kitti-360/,large-scale dataset that contains rich sensory information and full annotations; successor of the popular KITTI dataset; providing more comprehensive semantic/instance labels in 2D and 3D; richer 360 degree sensory information (fisheye images and pushbroom laser scans);  accurate and geo-localized vehicle and camera poses; and a series of new challenging benchmarks,several suburbs of a midsize city corresponding to over 300k images and 80k laser scans; covering a driving distance of 73.7km,outdoor,,,,(2D / 3D) Semantic Segmentation; (3D) Instance Segmentation; Panoptic Segmentation; Novel View Synthesis; 3D Semantic Scene Completion; 3D Object Detection From Monocular Images; 3D Semantic Scene Completion from a single RGB image; Weakly Supervised 3D Detection; Semantic SLAM,187,7,,
KITTI-C,Robo3D: Towards Robust and Reliable 3D Perception against Corruptions,2023,ICCV,https://arxiv.org/pdf/2303.17597v4,https//ldkong.com/Robo3D,KITTI + simulated physically-principled corruptions on the val set; corruptions: 1) Severe weather conditions: fog; rain; snow -> back-scattering; attenuation; reflections; 2) External disturbances: bumpy surfaces; dust; insects... -> motion blur; LiDAR beam missing issues; 3) Internal sensor failure: incomplete echo; miss detection of instances with a dark color (e.g.; black car); crosstalk among multiple sensors -> deteriorates the 3D perception accuracy,KITTI + corruption,outdoor,synthetic,,,(Robust) 3D Object Detection; Unsupervised Monocular Depth Estimation,24,2,,
KITTI-Depth,Sparsity Invariant CNNs,2017,3DV,https://arxiv.org/pdf/1708.06500v2,https//www.cvlibs.net/datasets/kitti/eval_depth.php?benchmark=depth_prediction,The KITTI-Depth dataset includes depth maps from projected LiDAR point clouds that were matched against the depth estimation from the stereo cameras,The depth images are highly sparse with only 5% of the pixels available and the rest is missing; The dataset has 86k training images; 7k validation images; and 1k test set images on the benchmark server with no access to the ground truth,outdoor,,,,Depth Estimation; Autonomous Driving; Depth Completion,14,0,,
L-CAS 3D Point Cloud People,Online Learning for Human Classification in 3D LiDAR-based Tracking,2017,IROS,https://ieeexplore.ieee.org/iel7/8119304/8202121/08202247.pdf?casa_token=vbk-7rG2A00AAAAA:NlRUSK3b1N9qE4z9wmCfHu8RzuR-qQa7mlN4IUdjuLaxLoaPc7Kaln2w82349rOQ4SnWM3wkNv1PQQ,https//lcas.lincoln.ac.uk/wp/research/data-sets-software/l-cas-3d-point-cloud-people-dataset/,3D LiDAR dataset of people moving in a large indoor public space; which is made available to the research community,28;002 Velodyne scan frames acquired in one of the main buildings (Minerva Building) of the University of Lincoln; UK; Total length of the recorded data: about 49 minutes; Data were grouped into two classes according to whether the robot was stationary or moving.,indoor,,,,Human Detection and Tracking,1,0,,
Lani,Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction,2018,EMNLP,https://arxiv.org/pdf/1809.00786v2,https//hub.com/lil-lab/ciff,3D navigation environment and corpus; where an agent navigates between landmarks,27;965 crowd-sourced instructions for navigation in an open environment; Each datapoint includes an instruction; a human-annotated ground-truth demonstration trajectory; and an environment with various landmarks and lakes; The dataset train/dev/test split is 19;758/4;135/4;072; Each environment specification defines placement of 6–13 landmarks within a square grass field of size 50m×50m,outdoor,,,,Continuous Control; Starcraft; Starcraft II,10,0,,
LiDAR-MOS  (LiDAR-based Moving Object Segmentation),Moving Object Segmentation in 3D LiDAR Data: A Learning-based Approach Exploiting Sequential Data,2021,RA-L,https://arxiv.org/pdf/2105.08971v2,https//competitions.codalab.org/competitions/28894,moving object segmentation from 3D LiDAR scans,based on SemanticKITTI (28 semantic classes such as vehicles; pedestrians; buildings; roads; etc.) -> reorganization into 2 types: moving and non-moving/static objects,outdoor,,,,3D Semantic Segmentation; 3D Part Segmentation; Moving Point Cloud Processing,6,0,,
Light Snowfall (DENSE),Seeing Through Fog Without Seeing Fog: Deep Multimodal Sensor Fusion in Unseen Adverse Weather,2020,CVPR,https://arxiv.org/pdf/1902.08913v3,https//www.uni-ulm.de/en/in/driveu/projects/dense-datasets#c811669,object detection dataset in challenging adverse weather conditions,12000 samples in real-world driving scenes; 1500 samples in controlled weather conditions within a fog chamber; different weather conditions (like fog; snow; rain); acquired by over 10;000 km of driving in northern Europe; 100k objects were labeled with 2D / 3D bounding boxes,outdoor,real,RGB cameras (2 OnSemi AR0230 imagers); NIR camera ( BrightwayVision BrightEye); (FMCW) radar; LiDAR (Velodyne HDL64 S3D and VLP32C); Q1922 FIR camera; environmental sensors (Airmar WX150 weather station),0,(3D) Object Detection ,2,1,,c
LiPC (LiDAR Point Cloud Clustering Benchmark Suite),3D ToF LiDAR in Mobile Robotics: A Review,2022,arXiv,https://arxiv.org/pdf/2202.11025v1,https//hub.com/cavayangtao/lidar_clustering_bench,benchmark suite for point cloud clustering algorithms based on open-source software and open datasets,datasets used:  L-CAS dataset; EU Long-term dataset; KITTI dataset,outdoor,,,,object detection; robot localization; long-term autonomy; LiDAR data processing under adverse weather conditions; sensor fusion; service robots; assisted and autonomous driving,1,0,,
LOOK,Do Pedestrians Pay Attention? Eye Contact Detection in the Wild,2021,arXiv,https://arxiv.org/pdf/2112.04212v1,https//looking-vita-epfl.hub.io/dataset/,large-scale dataset for eye contact detection in the wild; which focuses on diverse and unconstrained scenarios for real-world generalization; The dataset focuses on real-world scenarios for autonomous vehicles with no control over the environment or the distance of pedestrian,selection of publicly available images from three existing datasets: KITTI; nuScenes and JRDB;  they labeled 13;048 images from four different cities (Boston; Singapore; Tübingen; Palo Alto) in three continents; they selected images with diversity -> pedestrians areas and crowded images from six cameras around the car; and indoor environments from a robot perspective; they labeled around 8;000 unique pedestrians,outdoor,,,,Eye contact detection,3,0,,
LM (LINEMOD),Model Based Training; Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes,2012,ACCV,http://stefan-hinterstoisser.com/papers/hinterstoisser2012accv.pdf,https//bop.felk.cvut.cz/datasets/,The LM (Linemod) dataset is a valuable resource introduced by Stefan Hinterstoisser and colleagues in their research on model-based training; detection; and pose estimation of texture-less 3D objects in heavily cluttered scenes,15 registered video sequences; each containing over 1100 frames; sequences feature 15 different texture-less household objects; Objects in the dataset exhibit discriminative color; shape; and size characteristics,3D Objects,,,,Domain Adaption; 6D Pose Estimation (using RGB / RGBD),34,5,,
LVIS (Large Vocabulary Instance Segmentation) ,Model Based Training; Detection and Pose Estimation of Texture-Less 3D Objects in Heavily Cluttered Scenes,2019,CVPR,https://arxiv.org/pdf/1908.03195v2,https//www.lvisdataset.org/dataset,dataset for long tail instance segmentation,annotations for over 1000 object categories in 164k images,indoor,,,,Object Detection; Instance Segmentation…,487,14,,
M2DGR,M2DGR: A Multi-sensor and Multi-scenario SLAM Dataset for Ground Robots,2021,RA-L,https://arxiv.org/pdf/2112.13659v1,https//hub.com/SJTU-ViSYS/M2DGR,multi-sensor dataset focusing on ground robots’ localization and mapping tasks;  indoors and outdoors; complete sensor suite; including 6 surround-view fish-eye cameras; a sky-pointing fish-eye camera; a perspective color camera; an event camera; an infrared camera; a 32-beam LIDAR; two GNSS receivers; and two IMUs,36 sequences (about 1TB) captured in diverse scenarios including both indoor and outdoor environments -> 10x Street; 2x Circle; 3x Gate; 1x Walk; 5x Hall; 2x Door; 4x Lift; 3x Room; 6x Roomdark,indoor,,,,SLAM,22,0,,
MatSynth,MatSynth: A Modern PBR Materials Dataset,2024,CVPR,https://arxiv.org/pdf/2401.06056v3,https//gvecchio.com/matsynth/,Physically Based Rendering (PBR) materials dataset designed for modern AI applications,4;069 high-quality; 4K; tileable materials with permissive licences,other,,,,Image Generation; SVBRDF Estimation,3,0,,
Matterport3D,Matterport3D: Learning from RGB-D Data in Indoor Environments,2017,3DV,https://arxiv.org/pdf/1709.06158v1,https//niessner.hub.io/Matterport/,large RGB-D dataset for scene understanding in indoor environments,10;800 panoramic views inside 90 real building-scale scenes; constructed from 194;400 RGB-D images,indoor,real,RGB-D images; 3 RGB cameras; 3 depth cameras,40,Semantic Segmentation; Depth Estimation; Monocular Depth Estimation: NeWCRFs; Depth Completion; Depth Prediction,437,5,,c
MatterportLayout,Manhattan Room Layout Reconstruction from a Single 360 image: A Comparative Study of State-of-the-art Methods,2021,IJCV,https://arxiv.org/pdf/1910.04099v3,https//hub.com/ericsujw/Matterport3DLayoutAnnotation,MatterportLayout extends the Matterport3D dataset with general Manhattan layout annotations,2;295 RGBD panoramic images from Matterport3D; extended with ground truth 3D layouts,indoor,,,,Semantic Segmentation,5,0,,
Milan Cathedral (IT), A hierarchical machine learning approach for multi-level and multi-resolution 3D point cloud classification,2020, Remote Sens.,https://www.mdpi.com/2072-4292/12/16/2598/pdf,,heritage scenario; features a large amount of 3D points; a great diversity of geometries and styles; a richness in decoration and a lack in the regularity of the architectural elements,late Gothic Cathedral; very rich in decorations (3400 statues; 135 gargoyles and 700 figures that decorate the internal spaces; 135 spires and 30 decorative reverse arches on the external facades); 3D point cloud of the Milan Cathedral: more than 3 billion points; Terrestrial Laser Scanning (TLS) for the interior spaces; photogrammetry for the exteriors,outdoor,,,,point cloud classification,?,,,
MINOS,MINOS: Multimodal Indoor Simulator for Navigation in Complex Environments,2017,arXiv,https://arxiv.org/pdf/1712.03931v1,https//minosworld.hub.io,MINOS is a simulator designed to support the development of multisensory models for goal-directed navigation in complex indoor environments. MINOS leverages large datasets of complex 3D environments and supports flexible configuration of multimodal sensor suites.,the simulator provides immediate support for two datasets: a) the SUNCG dataset of synthetic furnished houses -> approximately 45;000 houses with more than 750K rooms of different types; b) the Matterport3D dataset of reconstructed real buildings-> 90 multi-floor residences with approximately 2;000 annotated room regions,indoor,,,,Object Detection; Visual Navigation; Vision and Language Navigation,22,0,,
MM-Vet,MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities,2023,arXiv,https://arxiv.org/pdf/2308.02490v3,https//hub.com/yuweihao/MM-Vet,evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks,200 images; and 218 questions (samples); all paired with their respective ground truths,other,real,,,Visual Question Answering (VQA),160,1,,
MMBench,,2024,ECCV,https://arxiv.org/pdf/2307.06281v5,https//opencompass.org.cn/mmbench,multi-modality benchmark / objective benchmark for a robust and holistic evaluation of vision-language models,over 3;000 multiple-choice questions covering 20 ability dimensions,,,,,Visual Question Answering (VQA),176,1,,
ModelNet / ModelNet40,3D ShapeNets: A Deep Representation for Volumetric Shapes,2015,CVPR,https://arxiv.org/pdf/1406.5670v3,https//modelnet.cs.princeton.edu,dataset contains synthetic object point clouds; large-scale object dataset of 3D computer graphics CAD models; they compiled a list of the most common object categories in the world; using the statistics obtained from the SUN database,151;128 3D CAD models belonging to 660 unique object categories -> ModelNet40: 40 common object categories from ModelNet with 100 unique CAD models per category (used to train the model from the paper),indoor,synthetic,,660,(Few-Shot / Zero-Shot) 3D Point Cloud Classification; 3D Object Classification; 3D Object Recognition; 3D Point Cloud Data Augmentation; 3D Object Retrieval; 3D Point Cloud Linear Classification; Generative 3D Object Classification…,1300,18,,
ModelNet40-C,Benchmarking Robustness of 3D Point Cloud Recognition Against Common Corruptions,2022,ICLR,https://arxiv.org/pdf/2201.12296v1,https//sites.google.com/umich.edu/modelnet40c,comprehensive dataset to benchmark the corruption robustness of 3D point cloud recognition -> ModelNet40 + corruption,based on the ModelNet40 validation set with 15 corruption types and 5 severity levels for each corruption type including density; noise; and transformation corruption patterns; 185;000 distinct point clouds ,indoor,synthetic,3D CAD models from CAD model websites,40,3D Point Cloud Classification; 3D Classification; 3D Point Cloud Data Augmentation;  Few-Shot Point Cloud Classification; Robust Classification; Classify 3D Point Clouds,29,0,,
Mono3DRefer,Mono3DVG: 3D Visual Grounding in Monocular Images,2023,AAAI,https://arxiv.org/pdf/2312.08022v1,https//hub.com/ZhanYang-nwpu/Mono3DVG,3D visual grounding in monocular RGB images using descriptions with appearance and geometry information,2025 frames of images from the original KITTI; 41;140 expressions in total and a vocabulary of 5;271 words,outdoor,,,,(Monocular) 3D Object Detection; (3D) Visual Grounding; Mono3DVG,2,1,,
MS2 / Multi-Spectral Stereo Dataset,Deep Depth Estimation From Thermal Image,2023,CVPR,https://openaccess.thecvf.com/content/CVPR2023/papers/Shin_Deep_Depth_Estimation_From_Thermal_Image_CVPR_2023_paper.pdf,https//sites.google.com/view/multi-spectral-stereo-dataset,multi-spectral stereo (MS2) outdoor dataset; including stereo RGB; stereo NIR; stereo thermal; stereo LiDAR data; and GPS/IMU information,26K data pairs for training; 4K pairs for validation; and 5.8K; 6.8K; and 5.2K pairs for evaluation of daytime; nighttime; and rainy conditions,outdoor,,,,Depth Estimation; Visual Odometry; Depth Completion; Thermal Image Segmentation; Stereo Depth Estimation; Depth Prediction,0,0,,
MS COCO (Microsoft Common Objects in Context),Microsoft COCO: Common Objects in Context,2014,ECCV,https://arxiv.org/pdf/1405.0312v3,https//cocodataset.org/#home,large-scale object detection; segmentation; key-point detection; and captioning dataset. The dataset consists of 328K images,328K images (>200K labeled); 1.5 million object instances; 80 object categories; 91 stuff categories; 5 captions per image; 250;000 people with keypoints,,,,,Object Detection; Semantic Segmentation; Question Answering; Post Estimation; Visual Question Answering (VQA); Instance Segmentation; Image Retrieval; Image Captioning; Multi-Label Classification; Obejct Localization; Panoptic Segmentation…,10837,96,,
Multi30K,Multi30K: Multilingual English-German Image Description s,2016,Proceedings of the 5th Workshop on Vision and Language,https://arxiv.org/pdf/1605.00459v1,https//hub.com/multi30k/dataset,large-scale multilingual multimodal dataset for interdisciplinary machine learning research; extends the Flickr30K dataset with German translations created by professional translators over a subset of the English descriptions,Flickr30K + German Translations,mixed,real,,,Translation deu-eng; Translation eng-deu; Real-Time Instance Segmenation; Multimodal Machine Translation,129,10,,
Multifog KITTI,3D Object Detection with SLS-Fusion Network in Foggy Weather Conditions,2021,Sensors ,https://www.mdpi.com/1424-8220/21/20/6711,https//maiminh1996.hub.io/about/multifogkitti/,Augmented KITTI dataset with fog for both camera and LiDAR sensors with different visibility ranges from 20 to 80 meters to best match realistic fog environment,KITTI + fog,outdoor,synthetic,,,3D Object Detection; Autonomous Driving,0,0,,
MultiScan,MultiScan: Scalable RGBD scanning for 3D environments with articulated objects,2022,NeurIPS,https://openreview.net/pdf?id=YxUdazpgweG,https//3dlg-hcvc.hub.io/multiscan/#/,calable RGBD dataset construction pipeline leveraging commodity mobile devices to scan indoor scenes with articulated objects and web-based semantic annotation interfaces to efficiently annotate object and part semantics and part mobility parameters,273 scans of 117 indoor scenes containing 10957 objects and 5129 parts,indoor,,,,3D Object Detection; 3D Instance Segmentation,3,1,,
MUSES (MUlti-SEnsor Semantic perception dataset),MUSES: The Multi-Sensor Semantic Perception Dataset for Driving under Uncertainty,2024,arXiv,https://arxiv.org/pdf/2401.12761v3,https//muses.vision.ee.ethz.ch,multi-modal scenes; evenly distributed across various combinations of weather conditions (clear; fog; rain; and snow) and types of illumination (daytime; nighttime),2500 annotated multimodal samples; 500 of which are recorded in daytime and clear weather; The remaining 2000 adverse-condition samples are split between 1000 daytime (333/334/333 samples in fog/rain/snow) and 1000 nighttime (250/250/250/250 samples in clear/fog/rain/snow) samples,outdoor,,,,(2D) Object Detection; (2D) Semantic Segmentation; (2D) Panoptic Segmentation,1,0,,
MVP (Multi-View Partial point cloud),Variational Relational Point Completion Network,2021,CVPR,https://arxiv.org/pdf/2104.10154v1,https//paul007pl.hub.io/projects/VRCNet,multi-view partial point cloud dataset (MVP) ,over 100;000 high-quality scans; which renders partial 3D shapes from 26 uniformly distributed camera poses for each 3D CAD model; MVP dataset consists of a large number of high-quality synthetic partial scans for 3D CAD models; which imitates real-scanned incomplete point clouds caused by self-occlusion,mixed,synthetic,,,Point Cloud Completion,28,0,,
MVTecAD,MVTec AD — A Comprehensive Real-World Dataset for Unsupervised Anomaly Detection,2019,CVPR,https://openaccess.thecvf.com/content_CVPR_2019/papers/Bergmann_MVTec_AD_--_A_Comprehensive_Real-World_Dataset_for_Unsupervised_Anomaly_CVPR_2019_paper.pdf,https//www.mvtec.com/company/research/datasets/mvtec-ad/,dataset for benchmarking anomaly detection methods with a focus on industrial inspection,5354 high-resolution color images of different object and texture categories;  anomalies manifest themselves in the form of over 70 different types of defects such as scratches; dents; contaminations; and various structural changes,may be interesting in specific (industrial) applications (but 2D); real-world deformations,real,,,Image Classification; Object Detection; (Unsupervised / Supervised) Anomaly Detection; Outlier Detection,327,3,,
MVTEC 3D-AD (3D Anomaly Detection),The MVTec 3D-AD Dataset for Unsupervised 3D Anomaly Detection and Localization,2021,VISAPP,https://arxiv.org/pdf/2112.09045v1,https//www.mvtec.com/company/research/datasets/mvtec-3d-ad,comprehensive 3D dataset for the task of unsupervised anomaly detection and localization,over 4000 high-resolution scans acquired by an industrial 3D sensor; Each of the 10 different object categories comprises a set of defect-free training and validation samples and a test set of samples with various kinds of defects; Precise ground-truth annotations are provided for each anomalous test sample,indoor,,,,Anomaly Detection; (RGB+) 3D Anomaly Detection and Segmentation; Depth Anomaly Detection and Segmentation,31,4,,
MVTec D2S (MVTec Densely Segmented Supermarket),MVTec D2S: Densely Segmented Supermarket Dataset,2018,ECCV,https://arxiv.org/pdf/1804.08292v2,https//www.mvtec.com/company/research/datasets/mvtec-d2s,benchmark for instance-aware semantic segmentation in an industrial domain,21;000 high-resolution images with pixel-wise labels of all object instances; The objects comprise groceries and everyday products from 60 categories,indoor,,,,Semantic Segmentation; Instance Segmentation; Data Augmentation ,2,0,,
NavigationNet,NavigationNet: A Large-scale Interactive Indoor Navigation Dataset,2018,arXiv,https://arxiv.org/pdf/1808.08374v1,https//mvig.sjtu.edu.cn/research/nav/NavigationNet.html,computer vision dataset and benchmark to allow the utilization of deep reinforcement learning on scene-understanding-based indoor navigation,hierarchical organization (root > scenes > rooms);  15 scenes; each with 1-3 rooms; The origin room for each scene is at least 50 m2 in area,indoor,,,,Scene Understanding,1,0,,
NCLT (North Campus Long-Term),University of Michigan North Campus long-term vision and lidar dataset,2015,IJRR,https://journals.sagepub.com/doi/10.1177/0278364915614638,https//robots.engin.umich.edu/nclt/,large scale; long-term autonomy dataset for robotics research collected on the University of Michigan’s North Campus; dataset consists of omnidirectional imagery; 3D lidar; planar lidar; GPS; and proprioceptive sensors for odometry collected using a Segway robot,dataset is comprised of 27 sessions spaced approximately biweekly over the course of 15 months,outdoor,,,,Pose Estimation; Visual Place Recognition; Visual Localization,105,0,,
NERDS 360 (NeRF for Reconstruction Decomposition and Scene Synthesis of 360° outdoor scenes),NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes,2023,ICCV,https://arxiv.org/pdf/2308.12967v1,https//hub.com/zubair-irshad/NeO-360#-dataset,large-scale dataset for 3D urban scene understanding,75 outdoor urban scenes with diverse backgrounds; encompassing over 15;000 images; These scenes offer 360? hemispherical views; capturing diverse foreground objects illuminated under various lighting conditions,outdoor,,,,Semantic Segmentation; Instance Segmentation; Depth Etimation; Novel View Synthesis; 6D Pose Estimation; Generalizable Novel View Synthesis,4,1,,
New College,The New College Vision and Laser Data Set,2009,IJRR,https://journals.sagepub.com/doi/pdf/10.1177/0278364909103911,https//www.robots.ox.ac.uk/~mobile/MOOS/wiki/pmwiki.php,freely available dataset collected from a robot completing several loops outdoors around the New College campus in Oxford; odometry; laser scan; and visual information,data was gathered over a 2.2 km traverse of the college grounds,outdoor,,,,Visual Odometry; Loop Closure Detection; Simultaneous Localization and Mapping,16,0,,
Newer College,The Newer College Dataset: Handheld LiDAR; Inertial and Vision with Ground Truth,2020,IROS,https://arxiv.org/pdf/2003.05691v2,https//ori-drs.hub.io/newer-college-dataset/,dataset with a variety of mobile mapping sensors collected using a handheld device carried at typical walking speeds for nearly 2.2 km through New College; Oxford,nearly 2.2 km; data from two commercially available devices - a stereoscopic-inertial camera and a multi-beam 3D LiDAR; which also provides inertial measurements + tripod-mounted survey grade LiDAR scanner to capture a detailed millimeter-accurate 3D map of the test location (containing ?290 million points),outdoor,,,,Vision-based Navigation; Visual and LiDAR SLAM; 3D LiDAR Reconstruction; Appearance-based Place Recognition,1,0,,
NL-Drive (Nonlinear Autonomous Driving Dataset),NeuralPCI: Spatio-temporal Neural Field for 3D Point Cloud Multi-frame Non-linear Interpolation,2023,CVPR,https://arxiv.org/pdf/2303.15126v1,https//hub.com/ispc-lab/NeuralPCI,multi-frame interpolation dataset for autonomous driving scenarios; dataset contains point cloud sequences with large nonlinear movements from three public large-scale autonomous driving datasets: KITTI; Argoverse and Nuscenes,Based on KITTI; Argoverse and Nuscenes; more than 20;000 LiDAR point cloud frames; training; validation and test split in the ratio of 14:3:3,outdoor,,,,3D Point Cloud Interpolation,2,1,,
NoCaps,nocaps: novel object captioning at scale,2019,ICCV,https://arxiv.org/pdf/1812.08658v3,https//nocaps.org,nocaps =  novel object captioning at scale; To encourage the development of image captioning models that can learn visual concepts from alternative data sources; such as object detection datasets; we present the first large-scale benchmark for this task,166;100 human-generated captions describing 15;100 images from the Open Images validation and test sets; The associated training data consists of COCO image-caption pairs; plus Open Images imagelevel labels and object bounding boxes; Since Open Images contains many more classes than COCO; nearly 400 object classes seen in test images have no or very few associated training captions (hence; nocaps),indoor,,,,Object Detection; Image Captioning,152,13,,
nuScenes,nuScenes: A multimodal dataset for autonomous driving,2020,CVPR,https://arxiv.org/pdf/1903.11027v5,https//www.nuscenes.org,large-scale autonomous driving dataset,3D bounding boxes for 1000 scenes collected in Boston and Singapore; Each scene is 20 seconds long and annotated at 2Hz; Total of 28130 samples for training; 6019 samples for validation and 6008 samples for testing; Full autonomous vehicle data suite: 32-beam LiDAR; 6 cameras and radars with complete 360° coverage; 3D object detection challenge evaluates the performance on 10 classes: cars; trucks; buses; trailers; construction vehicles; pedestrians; motorcycles; bicycles; traffic cones and barriers,outdoor,real,6 cameras; 1 LIDAR; 5 RADAR; GPS; IMU,23,(3D) Object Detection; Instance Segmentation; (3D) Semantic Segmentation; Trajectory Prediction; 3D Multi-Obejct Tracking; Weather Forecasting; Trajectory Planing; Motion Detection; …,1951,22,,c
nuScenes-C,Robo3D: Towards Robust and Reliable 3D Perception against Corruptions,2023,ICCV,https://arxiv.org/pdf/2303.17597v4,https//ldkong.com/Robo3D,nuScenes + simulated physically-principled corruptions on the val set; corruptions: 1) Severe weather conditions: fog; rain; snow -> back-scattering; attenuation; reflections; 2) External disturbances: bumpy surfaces; dust; insects... -> motion blur; LiDAR beam missing issues; 3) Internal sensor failure: incomplete echo; miss detection of instances with a dark color (e.g.; black car); crosstalk among multiple sensors -> deteriorates the 3D perception accuracy,nuScenes + corruption,outdoor,synthetic,,,(Robust) 3D Object Detection; (Robust) 3D Semantic Segmentation; Robust Camera Only 3D Object Detection,27,3,,
NYUv2 (NYU-Depth V2),Indoor Segmentation and Support Inference from RGBD Images,2012,ECCV,https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/shkf_eccv2012.pdf,http//cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html,The NYU-Depth V2 data set is comprised of video sequences from a variety of indoor scenes as recorded by both the RGB and Depth cameras from the Microsoft Kinect,1449 RGB-D images; 464 indoor scenes; 26 scene classes; 35064 objects; 894 object classes; Each object is labeled with a class and an instance number (cup1; cup2; cup3; etc); dataset has several components: Labeled (subset of the video data + dense multi-class labels); Raw (raw RGB; depth; and accelerometer data as provided by the Kinect); Toolbox: functions for manipulating the data and labels,indoor,real,Kinect (RGB-D),894,Semantic Segmentation; Instance Segmentation; 3D Object Detection; Depth Estimation; Panoptic Segmentation; Scene Segmentation; Multi-Task Learning; Depth Completion; Surface Normals Estimation; 3D Semantic Scne Completion; …,948,20,,c
OASIS (Open Annotations of Single Image Surfaces),OASIS: A Large-Scale Dataset for Single Image 3D in the Wild,2020,CVPR,https://arxiv.org/pdf/2007.13215v1,https//oasis.cs.princeton.edu,dataset for single-image 3D in the wild,annotations of detailed 3D geometry for 140000 images,mixed,,,,Depth Estimation; Medical Image Classification; Medical Image Registration,24,2,,
Objaverse,Objaverse: A Universe of Annotated 3D Objects,2023,CVPR,https://arxiv.org/pdf/2212.08051v1,https//objaverse.allenai.org,large dataset of objects with 800K+ (and growing) 3D models with descriptive captions; tags; and animations,818K high-quality; diverse; 3D models with paired text descriptions; titles; and tags,mixed,synthetic,objects sourced from Sketchfab,18,Zero-Shot 3D Classification; Generative 3D Object Classification; 3D Object Captioning,316,3,,c
Objaverse-XL,Objaverse-XL: A Universe of 10M+ 3D Objects,2023,NeurIPS,https://arxiv.org/pdf/2307.05663v1,https//objaverse.allenai.org,extensive dataset containing over 10 million 3D objects (12x larger than Objaverse),over 10 million 3D objects,mixed,,,,Zero-Shot 3D Classification; Generative 3D Object Classification; 3D Object Captioning,19,0,,
ObjectFolder,OBJECTFOLDER: A Dataset of Objects with Implicit Visual; Auditory; and Tactile Representations,2021,CoRL,https://arxiv.org/pdf/2109.07991v3,https//ai.stanford.edu/~rhgao/objectfolder/,dataset for multisensory object-centric perception; reasoning; and interaction,100 virtualized objects; ObjectFolder encodes the visual; auditory; and tactile sensory data for all objects; enabling a number of multisensory object recognition tasks.,indoor,,,,Instance recognition; cross-sensory retrieval; 3D reconstruction; robotic grasping,3,0,,
ObjectNet3D,ObjectNet3D: A Large Scale Database for 3D Object Recognition,2016,ECCV,https://cvgl.stanford.edu/papers/xiang_eccv16.pdf,https//cvgl.stanford.edu/projects/objectnet3d/,large scale database for 3D object recognition,100 categories; 90127 images; 201;888 objects in these images and 44;147 3D shapes,indoor,,,,Region Proposal Generation; 2D Object Detection; Joint 2D Detection and 3D Object Pose Estimation; Image-Based 3D Shape Retrieval,2,0,,
OccludedPASCAL3D+,Robust Object Detection under Occlusion with Context-Aware CompositionalNets,2020,CVPR,https://arxiv.org/pdf/2005.11643v2,https//hub.com/Angtian/OccludedPASCAL3D,dataset is designed to evaluate the robustness to occlusion for a number of computer vision tasks; such as object detection; keypoint detection and pose estimation,partial occlusion is simulated by superimposing objects cropped from the MS-COCO dataset on top of objects from the PASCAL3D+ dataset; they use the ImageNet subset in PASCAL3D+; which has 10812 testing images,outdoor,synthetic,,,Object Detection; Keypoint Detection; Pose Estimation,7,0,,
OCTScenes,OCTScenes: A Versatile Real-World Dataset of Tabletop Scenes for Object-Centric Learning,2023,arXiv,https://arxiv.org/pdf/2306.09682v3,https//huggingface.co/datasets/Yinxuan/OCTScenes,They propose a versatile real-world dataset of tabletop scenes for object-centric learning; which is meticulously designed to serve as a benchmark for comparing; evaluating; and analyzing object-centric learning methods,5000 tabletop scenes with a total of 15 everyday objects; Each scene is captured in 60 frames covering a 360-degree perspective; a) OCTScenes-A: 0--3099 scenes without segmentation annotation for training; 3100--3199 scenes with annotation for testing; b) OCTScenes-B: 0--4899 scenes without annotation for training; 4900--4999 scenes with annotation for testing,indoor,,,,Object-Centric Learning,1,0,,
OK-VQA (Outside Knowledge Visual Question Answering),OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge,2019,CVPR,https://arxiv.org/pdf/1906.00067v2,https//okvqa.allenai.org,dataset for visual question answering that requires methods which can draw upon outside knowledge to answer questions,14;055 open-ended questions; 5 ground truth answers per question,NLP (expert knowledge),,,,Question Answering; Visual Question Answering (VQA); Retrieval; Question Generation,302,2,,
OMMO,A Large-Scale Outdoor Multi-modal Dataset and Benchmark for Novel View Synthesis and Implicit Scene Reconstruction,2023,ICCV,https://arxiv.org/pdf/2301.06782v1,https//ommo.luchongshan.com,benchmark for several outdoor NeRF-based tasks; such as novel view synthesis; surface reconstruction; and multi-modal NeRF; contains complex objects and scenes with calibrated images; point clouds and prompt annotations,33 real-world scenes with more than 14K posed images and text description,outdoor,,,,Novel View Synthesis; Surface Reconstruction,6,0,,
OmniObject3D,OmniObject3D: Large-Vocabulary 3D Object Dataset for Realistic Perception; Reconstruction and Generation,2023,CVPR,https://arxiv.org/pdf/2301.07525v2,https//omniobject3d.hub.io,large vocabulary 3D object dataset with massive high-quality real-scanned 3D objects,6000 scanned objects in 190 daily categories; sharing common classes with popular 2D datasets (e.g.; ImageNet and LVIS),indoor,,,,Novel View Synthesis; Zero-shot 3D Point Cloud Classification; Surface Reconstruction; Zero-shot 3D Classification; 3D Geometry Perception,45,2,,
ONCE (One millioN sCenEs),One Million Scenes for Autonomous Driving: ONCE Dataset,2021,NeurIPS,https://arxiv.org/pdf/2106.11037,Datahttp//www.once-for-auto-driving.com,dataset for 3D object detection in the autonomous driving scenario,1 Million LiDAR frames; 7 Million camera images; 200 km² driving regions; 144 driving hours (20x longer than other 3D autonomous driving datasets available like nuScenes and Waymo); 15k fully annotated scenes with 5 classes (Car; Bus; Truck; Pedestrian; Cyclist); Diverse environments (day/night; sunny/rainy; urban/suburban areas),outdoor,,,,3D Object Detection,76,1,,
OPD (Articulated Object Dataset),OPD: Single-view 3D Openable Part Detection,2022,ECCV ,https://arxiv.org/pdf/2203.16421v1,https//3dlg-hcvc.hub.io/OPD/,predicting what parts of an object can open and how they move when they do so; A) OPDSynth: they selected objects with openable parts from an existing dataset of articulated 3D models PartNet-Mobility; B) OPDReal: they  reconstructed 3D polygonal meshes for articulated objects in real indoor environments and annotate their parts and articulation information,A) OPDSynth: 683 objects with 1343 parts over 11 categories (from PartNet-Mobility); B) OPDReal: 763 polygonal meshes for 284 different objects across 8 object categories (they took 863 RGB-D video scans of indoor environments with articulated objects using iPad Pro 2021 devices,indoor,,,,Openable Part Detection; Instance Segmentation,1,0,,
OpenTrench3D,OpenTrench3D: A Photogrammetric 3D Point Cloud Dataset for Semantic Segmentation of Underground Utilities,2024,CVPR,https://arxiv.org/abs/2404.07711,https//hub.com/SimonBuusJensen/OpenTrench3D,publicly available point cloud dataset of underground utilities from open trenches,310 point clouds collected across 7 distinct areas (5 water utility areas and 2 district heating utility areas),outdoor,,,,3D Semantic Segmentation,3,1,,
OPV2V,OPV2V: An Open Benchmark Dataset and Fusion Pipeline for Perception with Vehicle-to-Vehicle Communication,2022,ICRA,https://arxiv.org/pdf/2109.07644v5,https//mobility-lab.seas.ucla.edu/opv2v/,large-scale open simulated dataset for Vehicle-to-Vehicle perception,over 70 interesting scenes; 11464 frames; and 232913 annotated 3D vehicle bounding boxes; collected from 8 towns in CARLA and a digital town of Culver City; Los Angeles,outdoor,synthetic,,,(Monocular) 3D Object Detection,64,2,,
OVDEval (Open-Vocabulary Detection),How to Evaluate the Generalization of Detection? A Benchmark for Comprehensive Open-Vocabulary Detection,2023,AAAI,https://arxiv.org/pdf/2308.13177v2,https//hub.com/om-ai-lab/OVDEval,benchmark -> evaluations on commonsense knowledge; attribute understanding; position understanding; object relation comprehension; and more,9 large datasets that cover 6 linguistic aspects: object; proper noun; attribute; position; relationship; and negation,mixed,,,,Open-Vocabulary Detection (Object; Proper Noun; Attribute; Position; Relationship; Negation),1,2,,
P2S (Points2Surf),Points2Surf Learning Implicit Surfaces from Point Clouds,2020,ECCV,https://arxiv.org/pdf/2007.10453v2,https//www.cg.tuwien.ac.at/research/publications/2020/erler-2020-p2s/,method that turns point clouds into meshes; consists of objects from the ABC Dataset; a collection of Famous meshes and objects from Thingi10k,training set consists of 4950 ABC objects with varying number of scans and noise strength; 100 meshes for the validation and test sets,other,,,,Surface Reconstruction,4,0,,
PackIt,PackIt: A Virtual Environment for Geometric Planning,2020,ICML,https://arxiv.org/pdf/2007.11121v1,https//hub.com/princeton-vl/PackIt,virtual environment to evaluate and potentially learn the ability to do geometric planning; where an agent needs to take a sequence of actions to pack a set of objects into a box with limited space,shapes come from ShapeNet -> they created three smaller (mutually-exclusive) subsets -> sets of 4207; 4196 and 4185 shapes for training; testing and validation,,,,, General Reinforcement Learning; Robot Task Planning; Decision Making,3,1,,
PACO (Parts and Attributes of Common Objects),PACO: Parts and Attributes of Common Objects,2023,CVPR,https://arxiv.org/pdf/2301.01795v1,https//hub.com/facebookresearch/paco,detection dataset that goes beyond traditional object boxes and masks and provides richer annotations such as part masks and attributes,75 object categories; 456 object-part categories and 55 attributes across image (LVIS) and video (Ego4D) datasets; 641K part masks annotated across 260K object boxes; with half of them exhaustively annotated with attributes as well,,,,,2D Object Detection,20,0,,
PandaSet,PandaSet: Advanced Sensor Suite Dataset for Autonomous Driving,2021,ITSC,https://arxiv.org/pdf/2112.12610v1,https//scale.com/open-av-datasets/pandaset,dataset produced by a complete; high-precision autonomous vehicle sensor kit with a no-cost commercial license; dataset was collected using one 360x360 mechanical spinning LiDAR; one forward-facing; long-range LiDRAR; and 6 cameras,more than 100 scenes; each of which is 8 seconds long; and provides 28 types of labels for object classification and 37 types of annotations for semantic segmentation,outdoor,,,,3D Object Detection; Autonomous Driving,43,0,,
Pano3D,Pano3D: A Holistic Benchmark and a Solid Baseline for 360o Depth Estimation,2021,CVPRW,https://arxiv.org/pdf/2109.02749v1,https//vcl3d.hub.io/Pano3D/,benchmark for depth estimation from spherical panoramas,7170/1527 train/test Matterport3D samples; and 2740; 6999; 3284; 21203 GibsonV2 tiny; medium; fullplus; and full split samples ,indoor,,,,Domain Adaption; Depth Estimation; Out-of-Distribution Detection; 3D Reconstruction; Surface Normals Estimation; 3D Depth Estimation; Zero-Shot Learning+Domain Generalization; Zero-Shot Out-of-Domain Detection,2,0,,
PanoContext,PanoContext: A Whole-Room 3D Context Model for Panoramic Scene Understanding,2014,ECCV,https://panocontext.cs.princeton.edu/paper.pdf,https//panocontext.cs.princeton.edu,A Whole-Room 3D Context Model for Panoramic Scene Understanding,We collected 700 full-view panoramas for home environments from SUN360 database; including 418 bedrooms and 282 living rooms. We split our dataset into two halves for training and testing respectively.,indoor,,,,3D Room Layouts From A Single RGB Panorama,41,1,,
Panoptic nuScenes,Panoptic nuScenes: A Large-Scale Benchmark for LiDAR Panoptic Segmentation and Tracking,2021,RA-L,https://arxiv.org/pdf/2109.03805v3,https//www.nuscenes.org/panoptic?externalData=all&mapData=all&modalities=Any,benchmark dataset that extends the popular nuScenes dataset with point-wise groundtruth annotations for semantic segmentation; panoptic segmentation; and panoptic tracking tasks,extension of the nuScenes dataset which contains 1000 scenes collected across multiple cities; 32 semantic classes; with 23 thing and 9 stuff classes; 1.1B LiDAR points annotated with one of 32 semantic labels,outdoor,,,,Panoptic Segmentation,5,0,,
PARIS,PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects,2023,ICCV,https://arxiv.org/pdf/2308.07391v1,https//3dlg-hcvc.hub.io/paris/,PARIS = PArt-level Reconstruction and motion analysIS for articulated objects; Synthetic dataset,models from the PartNet-Mobility dataset; they selected 10 / 46 categories; For each articulation state; they randomly sampled 64-100 views covering the upper hemisphere of the object to simulate capturing in the real world,indoor,synthetic,,,3D Shape Modeling,2,0,,
Paris-Lille 3D,Paris-Lille-3D: a large and high-quality ground truth urban point cloud dataset for automatic segmentation and classification,2018,IJRR,https://arxiv.org/abs/1712.00032,Datahttps//npm3d.fr/paris-lille-3d,Dataset and a Benchmark on Point Cloud Classification,50 different classes; around 2km of Mobile Laser System point cloud acquired in two cities in France (Paris and Lille),outdoor,,,50,Semantic Segmentation; Few-Shot Learning; 3D Semantic Segmentation; LIDAR Semantic Segmentation,14,1,,
PartNet,PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding,2019,CVPR,https://arxiv.org/pdf/1812.02713v1,https//partnet.cs.stanford.edu,consistent; large-scale dataset of 3D objects annotated with fine-grained; instance-level; and hierarchical 3D part information,573;585 part instances over 26;671 3D models covering 24 object categories,indoor,real,,,(3D) Semantic Segmentation; (3D) Instance Segmentation,136,3,,
PartNet-Mobility,SAPIEN: A SimulAted Part-based Interactive ENvironment,2020,CVPR,https://arxiv.org/pdf/2003.08515v1,https//sapien.ucsd.edu,Dataset produced for the SAPIEN (SimulAted Part-based Interactive Environment) simulation environment; large-scale 3D interactive model datase,over 14K articulated parts over 2;346 object models from 46 common indoor object categories; All models are collected from 3D Warehouse and organized as in ShapeNet and PartNet; 3 types of motions: hinge; slider; and screw; where hinge indicates rotation around an axis (e.g. doors); slider indicates translation along an axis (e.g. drawers); and screw indicates a combined hinge and slider (e.g. bottle caps; swivel chairs),indoor,synthetic,,,3D Pose Estimation; 3D Shape Modeling; 3D Feature Matching; Articulated Object Modelling,62,1,,
PASCAL3D+,Beyond PASCAL: A benchmark for 3D object detection in the wild,2014,WACV,https://ieeexplore.ieee.org/document/6836101,https//cvgl.stanford.edu/projects/pascal3d.html,multi-view dataset consists of images in the wild,12 categories of rigid objects selected from the PASCAL VOC 2012 dataset; These objects are annotated with pose information (azimuth; elevation and distance to camera); Pascal3D+ also adds pose annotated images of these 12 categories from the ImageNet dataset.,mixed,synthetic,CAD models from Google 3D Warehouse; images from PASCAL and ImageNet,12,Object Detection; Pose Estimation; Keypoint Detection; Viewpoint Estimation,235,1,,c
PASCAL Context,The Role of Context for Object Detection and Semantic Segmentation in the Wild,2014,CVPR,https://openaccess.thecvf.com/content_cvpr_2014/papers/Mottaghi_The_Role_of_2014_CVPR_paper.pdf,https//cs.stanford.edu/~roozbeh/pascal-context/,extension of the PASCAL VOC 2010 detection challenge; contains pixel-wise labels for all training images,more than 400 classes (including the original 20 classes plus backgrounds from PASCAL VOC segmentation); divided into 3 categories (objects; stuff; and hybrids); Many of the object categories of this dataset are too sparse and; therefore; a subset of 59 frequent classes are usually selected for use,mixed,,,,Semantic Segmentation; Zero-Shot Learning; Saliency Detection; Surface Normals Estimation; Human Parsing; Boundary Detection,297,6,,
Pascal Panoptic Parts,Part-aware Panoptic Segmentation,2021,CVPR,https://arxiv.org/pdf/2106.06351v1,https//hub.com/pmeletis/panoptic_parts,The Pascal Panoptic Parts dataset consists of annotations for the part-aware panoptic segmentation task on the PASCAL VOC 2010 dataset; It is created by merging scene-level labels from PASCAL-Context with part-level labels from PASCAL-Part,PASCAL-Context + partl-level labels from PASCAL-PART,mixed,,,,Scene Understanding; Panoptic Segmentation; Image Segmentation; Human Part Segmentation; Part-aware Panoptic Segmentation,9,2,,
PASCAL VOC (PASCAL Visual Object Classes Challenge),The PASCAL Visual Object Classes (VOC) Challenge,2021,ICCV,https://arxiv.org/pdf/2012.07131v2,http//host.robots.ox.ac.uk/pascal/VOC/,The PASCAL Visual Object Classes (VOC) 2012 dataset contains 20 object categories; This dataset has been widely used as a benchmark for object detection-semantic segmentation-classification tasks, images were collected from the flickr photo-sharing web-site; 20 object categories (incl. vehicles; household; animals; and other: aeroplane; bicycle; boat; bus; car; motorbike; train; bottle; chair; dining table; potted plant; sofa; TV/monitor; bird; cat; cow; dog; horse; sheep; person); Each image in this dataset has pixel-level segmentation annotations-bounding box annotations-object class annotations;  split into three subsets: 1464 images for training - 1449 images for validation - private testing set,mixed,real,images collected from flickr ,20,Object Detection; Semantic Segmentation; Node Classification; Object Counting; Graph Matching…,184,22,,c
PedX,PedX: Benchmark Dataset for Metric 3D Pose Estimation of Pedestrians in Complex Urban Intersections,2019,RA-L,https://arxiv.org/pdf/1809.03605v1,https//umfordav.squarespace.com/projects/pedestrian-perception,large-scale multi-modal collection of pedestrians at complex urban intersections,more than 5;000 pairs of high-resolution (12MP) stereo images and LiDAR data along with providing 2D and 3D labels of pedestrians,outdoor,,,,(3D) Pose Estimation; Autonomous Driving,5,0,,
PhotoShape,PhotoShape: Photorealistic Materials for Large-Scale Shape Collections,2018,TOG,https://arxiv.org/pdf/1809.09761v1,https//photoshape.hub.io,dataset consists of photorealistic; relightable; 3D shapes produced by the work proposed in the work of Park et al. (2021); 3 types of datasets used: shape; photo; and texture collections; focus on chairs,a) Shapes used: 5;740 3D models from ShapeNet and 90 models from Herman Miller; b) Photos used: 40;927 product photos that were collected from the Herman Miller website (1820 images) and image search engines (Google and Bing; 39107 images); c) Textures used:  48 leathers; 154 fabrics; 105 woods; 86 metals; and 60 plastics,indoor,,,,Novel View Synthesis,21,1,,
PhraseCut,PhraseCut: Language-based Image Segmentation in the Wild,2020,CVPR,https://arxiv.org/pdf/2008.01187v1,https//people.cs.umass.edu/~chenyun/publication/phrasecut/,problem of segmenting image regions given a natural language phrase,77;262 images and 345;486 phrase-region pairs; dataset is collected on top of the Visual Genome dataset and uses the existing annotations,mixed,,,,Semantic Segmentation; Referring Expression Segmentation,26,1,,
PIE (Pedestrian Intention Estimation),PIE: A Large-Scale Dataset and Models for Pedestrian Intention Estimation and Trajectory Prediction,2019,ICCV,https://openaccess.thecvf.com/content_ICCV_2019/papers/Rasouli_PIE_A_Large-Scale_Dataset_and_Models_for_Pedestrian_Intention_Estimation_ICCV_2019_paper.pdf,https//data.nvision2.eecs.yorku.ca/PIE_dataset/,dataset for studying pedestrian behavior in traffic,over 6 hours of footage recorded in typical traffic scenes with on-board camera; provides accurate vehicle information from OBD sensor (vehicle speed; heading direction and GPS coordinates) synchronized with video footage; spatial and behavioral annotations are available for pedestrians and vehicles that potentially interact with the ego-vehicle as well as for the relevant elements of infrastructure (traffic lights; signs and zebra crossings); over 300K labeled video frames with 1842 pedestrian samples ,outdoor,real,,,Trajectory Prediction; Multi-future Trajectory Prediction; Trajectory Forecasting; Pedestrian Trajectory Prediction,6,2,,
Pix3D,Pix3D: Dataset and Methods for Single-Image 3D Shape Modeling,2018,CVPR,https://arxiv.org/pdf/1804.04610v1,http//pix3d.csail.mit.edu,large-scale benchmark of diverse image-shape pairs with pixel-level 2D-3D alignment; wide applications in shape-related tasks including reconstruction; retrieval; viewpoint estimation; etc.,395 3D shapes of nine object categories. Each shape associates with a set of real images; capturing the exact object in diverse environments. Further; the 10;069 image-shape pairs have precise 3D annotations; giving pixel-level alignment between shapes and their silhouettes in the images,,,,,Pose Estimation; 3D Shape Reconstruction; 3D Shape Modelling; 3D Shape Classification,133,5,,
PointCleanNet,POINTCLEANNET: Learning to Denoise and Remove Outliers from Dense Point Clouds,2019,CGF,https://arxiv.org/pdf/1901.01060v3,https//www.lix.polytechnique.fr/Labo/Marie-Julie.RAKOTOSAONA/pointcleannet.html,data-driven method for removing outliers and reducing noise in unordered point clouds,28 different shapes; which we split into 18 training shapes and 10 test shapes,,,,,Outliers Removal; Denoising,1,0,,
PointCloud-C,Benchmarking and Analyzing Point Cloud Classification under Corruptions,2022,PMLR,https://arxiv.org/pdf/2202.03377v3,https//pointcloud-c.hub.io/home.html,test-suite for point cloud robustness analysis under corruptions,Two sets: ModelNet-C for point cloud classification and ShapeNet-C for part segmentation; Real-world corruption sources; ranging from object-; senor-; and processing-levels; 7 types of corruptions; each with 5 severity levels; benchmark with more than 20 point cloud recognition algorithms; Methods ranging from architecture design; augmentations; and pre-training,,,,,Point Cloud Classification; Point Cloud Segmentation,21,2,,
PolyU-BPCoMa (HK PolyU Backpack Colorized Mapping),PolyU-BPCoMa: A dataset and benchmark towards mobile colorized mapping using a backpack multisensorial system,2022,Int. J. Appl. Earth Obs. Geoinf.,https://arxiv.org/pdf/2206.07468v2,https//hub.com/chenpengxin/PolyU-BPCoMa,A Dataset and Benchmark Towards Mobile Colorized Mapping Using a Backpack Multisensorial System,surveyed areas include outdoor terrace; indoor office; corridor; staircase and underground tunnel;  two data sequences with different traverse paths and walking speeds for each area;  data formats: a) ROS bag; b) LiDAR data -> single 360° sweep; c) Image data; d) GNSS/IMNU data,outdoor,,,,Simultaneous Localization and Mapping,1,0,,
Pomposa Abbey (IT), A hierarchical machine learning approach for multi-level and multi-resolution 3D point cloud classification,2020, Remote Sens.,https://www.mdpi.com/2072-4292/12/16/2598/pdf,,heritage scenario; features a large amount of 3D points; a great diversity of geometries and styles; a richness in decoration and a lack in the regularity of the architectural elements,Basilica; typical Ravenna style; external areas: TOF (Leica C10); 21 aqcuisitions; 161 mil. 3D points; 5.2 mm mean resolution; 2.5 cm subsampling; internal spaces: TOF Faro Focus X120; 31 aqcuisitions; 580 mil. 3D points; 10 mm mean res.; 2.5 cm subsampling,outdoor,,,,point cloud classification,?,,,
PreSIL (Precise Synthetic Image and LiDAR),Precise Synthetic Image and LiDAR (PreSIL) Dataset for Autonomous Vehicle Perception,2019,IV,https://arxiv.org/pdf/1905.00160v2,https//uwaterloo.ca/waterloo-intelligent-systems-engineering-lab/projects/precise-synthetic-image-and-lidar-presil-dataset-autonomous,dataset for autonomous vehicle perception; precise LiDAR simulator within GTA V which collides with detailed models for all entities no matter the type or position,over 50;000 frames; includes high-definition images with full resolution depth information; semantic segmentation (images); point-wise segmentation (point clouds); and detailed annotations for all vehicles and people,outdoor,synthetic,,,(3D) Object Detection;  Autonomous Vehicles,13,0,,
PSI (Pedestrian Situated Intent),PSI: A Pedestrian Behavior Dataset for Socially Intelligent Autonomous Car,2021,arXiv,https://arxiv.org/pdf/2112.02604v2,http//pedestriandataset.situated-intent.net,Pedestrian Behavior Dataset for Socially Intelligent Autonomous Car,110 representative vehicle-pedestrian encounters that happened at mid-block road locations with potential conflicts (sampled from the TASI 110-car naturalistic driving study) -> 110 encounters in mid-block road locations;  two types of labels in PSI: visual and cognitive annotations,outdoor,,,,pedestrian intent prediction; pedestrian trajectory prediction; driver reasoning explanation,5,0,,
R2R (Room-to-Room),Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments,2018,CVPR,https://arxiv.org/pdf/1711.07280,https//bringmeaspoon.org,dataset for visually-grounded natural language navigation in real buildings. The dataset requires autonomous agents to follow human-generated navigation instructions in previously unseen buildings,22k instructions are available; with an average length of 29 words; Visual imagery for the Matterport3D simulator comes from the Matterport3D dataset; The main advantage of our framework over synthetic environments is that all pixel observations come from natural images of real scenes,indoor,real,natural language instruction; (RGB-D) images for the simulator from Matterport3D,0,Visual Navigation; Vision-Language Navigation,162,2,,c
RadarScenes,RadarScenes: A Real-World Radar Point Cloud Data Set for Automotive Applications,2021,FUSION,https://arxiv.org/pdf/2104.02493v1,https//radar-scenes.com,real-world radar point cloud dataset for automotive applications,over 4 hours and 100 km of driving at a total of 158 different sequences with over 7500 manually annotated unique road users from 11 different object classes,outdoor,,,,Autonomous Vehicles,24,0,,
RADIATE (RAdar Dataset In Adverse weaThEr),RADIATE: A Radar Dataset for Automotive Perception in Bad Weather,2020,ICRA,https://arxiv.org/pdf/2010.09076v3,https//hub.com/marcelsheeny/radiate_sdk,automotive dataset created by Heriot-Watt University which includes Radar; Lidar; Stereo Camera and GPS/IMU,radar images are annotated in 7 different scenarios: Sunny (Parked); Sunny/Overcast (Urban); Overcast (Motorway); Night (Motorway); Rain (Suburban); Fog (Suburban); Snow (Suburban); dataset contains 8 different types of objects (car; van; truck; bus; motorbike; bicycle; pedestrian and group of pedestrians),outdoor,,,,(2D) Object Detection; Scene Understanding; Multiple Object Tracking,21,2,,
RCooper (Roadside Cooperative Perception Dataset),RCooper: A Real-world Large-scale Dataset for Roadside Cooperative Perception,2024,CVPR,https://arxiv.org/pdf/2403.10145v2,https//hub.com/AIR-THU/DAIR-Rcooper,real-world; large-scale Roadside Cooperative Perception Dataset for practical applications,More than 50k images and 30k point clouds manually annotated with 3D bounding boxes and trajectories for ten semantic classes,outdoor,,,,3D Object Detection; 3D Object Tracking,3,0,,
Real 3D-AD (3D-Anomaly Detection),Real3D-AD: A Dataset of Point Cloud Anomaly Detection,2023,NeurIPS,https://arxiv.org/pdf/2309.13226v3,https//hub.com/M-3LAB/Real3D-AD,point cloud anomaly detection dataset for industrial products,1;254 samples that are distributed across 12 distinct categories; These categories include Airplane; Car; Candybar; Chicken; Diamond; Duck; Fish; Gemstone; Seahorse; Shell; Starfish; and Toffees,,,,,Anomaly Detection; 3D Anomaly Detection ,4,2,,
RefCOCO,ReferItGame: Referring to Objects in Photographs of Natural Scenes,2014,EMNLP,https://aclanthology.org/D14-1086.pdf,https//hub.com/lichengunc/refer,referring expression generation (REG) dataset used for tasks related to understanding natural language expressions that refer to specific objects in images,"We build our dataset of referring expressions
on top of the ImageCLEF IAPR image retrieval
dataset (collection 20000 of freely available images)depicting a variety of aspects of everyday life (e.g.sports; animals; cities; landscapes; SAIAPR TC-12 expansion includes segmentations -> 238 different object categories are labeled (e.g. animals; people; buildings; objects; background elements like grass or sky); (i) RefCOCO: Contains 142209 refer expressions for 50000 objects across 19994 images; (ii) RefCOCO+: Includes 141564 expressions for 49856 objects in 19992 images; (iii) RefCOCOg: This variant has 25799 images; 95010 referring expressions; and 49822 object instances",mixed,real,"(RGB) images from ImageCLEF IAPR image retrieval
dataset",238,Semantic Segmentation; Referring Expression Segmentation; Visual Reasoning; Referring Expression Comprehension; Visual Grounding; Region Proposal,393,21,,c
ReferIt3D,ReferIt3D: Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes,2020,ECCV,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460409.pdf,https//referit3d.hub.io,Neural Listeners for Fine-Grained 3D Object Identification in Real-World Scenes,two large-scale and complementary visio-linguistic datasets: i) Sr3D; which contains 83.5K template-based utterances leveraging spatial relations among fine-grained object classes to localize a referred object in a scene; and ii) Nr3D which contains 41.5K natural; free-form; utterances collected by deploying a 2-player object reference game in 3D scenes,indoor,,,,3D visual grounding; 3D dense captioning,56,0,,
RefMatte (Referring Image Matting),Referring Image Matting,2023,CVPR,https://arxiv.org/pdf/2206.05149v3,https//hub.com/jizhiziLi/rim,large-scale challenging dataset under the task referring image matting,230 object categories; 47;500 images; 118;749 expression-region entities; and 474;996 expressions,,,,,Referring Image Matting (Expression-based; Keyword-based; RefMatte-RW100),6,3,,
Rel3D,Rel3D: A Minimally Contrastive Benchmark for Grounding Spatial Relations in 3D,2020,NeurIPS,https://arxiv.org/pdf/2012.01634v1,https//hub.com/princeton-vl/Rel3D,large-scale; human-annotated dataset for grounding spatial relations (e.g.; “laptop on table”) in 3D,9;990 3D scenes ( 4;995 positive; 4;995 negative) and 27336 images;  objects come from 67 categories; with 30 different spatial predicates,mixed,,,,Spatial Relation Recognition,6,1,,
Rent3D,Rent3D: Floor-Plan Priors for Monocular Layout Estimation,2015,CVPR,https://openaccess.thecvf.com/content_cvpr_2015/papers/Liu_Rent3D_Floor-Plan_Priors_2015_CVPR_paper.pdf,https//www.cs.toronto.edu/~fidler/projects/rent3D.html,The goal of this work is to enable a 3D ``virtual-tour'' of an apartment given a small set of monocular images of different rooms; as well as a 2D floor plan,215 apartments; 1570 images; avg. 6 rooms per apt; avg. 31 walls per apt; avg. 6 windows per apt; avg. 9 doors per apt; Floor-plans have annotations for: Room types; Walls (represented with lines and dimension in real world); Windows; Doors; Each photo of a room has the following ground-truth: Scene type (e.g.; kitchen; bedroom; outdoor); Room layout; Windows; Doors; Ground-truth alignment of each photo within floor-plan,indoor,,,,Semantic Segmentation; 3D Room Layouts From A Single RGB Panorama; Room Layout Estimation,5,0,,
Replica,The Replica Dataset: A Digital Replica of Indoor Spaces,2019,arXiv,https://arxiv.org/pdf/1906.05797v1,https//hub.com/facebookresearch/Replica-Dataset,dataset of high quality reconstructions of a variety of indoor spaces,18 highly photo-realistic 3D indoor scene reconstructions at room and building scale; Each scene consists of a dense mesh; highresolution high-dynamic-range (HDR) textures; per-primitive semantic class and instance information; and planar mirror and glass reflectors,indoor,synthetic,time-aligned raw IMU; RGB; IR; wideangle greyscale sensor data,88,Semantic Segmentation; Image Generation; Domain Adaption; Visual Navigation; Scene Generation; 3D Open-Vocabulary Instance Segmentation; Efficient Exploration,372,4,,c
(Washington) RGB-D Scenes,A Large-Scale Hierarchical Multi-View RGB-D Object Dataset,2014,ICRA,https://ieeexplore.ieee.org/abstract/document/5980382?casa_token=vXGHjcdQsQEAAAAA:QTmhQ3Kj1GGuYJTQoANNUI8aP5pe9E6evWYrDij0-Dgp35zV0pLk8_p0t7KF0VwzSDS9B4fZiTzL,https//rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/,The RGB-D Scenes Dataset contains 8 scenes with tabletop objects in kitchen and office environments,8 video sequences of natural scenes;  300 objects organized into 51 categories (WordNet),indoor,real,prototype RGB-D camera manufactured by PrimeSense; firewire camera from Point Grey Research,51,semantic scene understanding; scene labeling,0,0,,c
(Washington) RGB-D Scenes v2,Unsupervised feature learning for 3D scene labeling,2014,ICRA,https://ieeexplore.ieee.org/document/6907298,https//rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/,Dataset consists of 14 scenes containing furniture and a subset of the objects in the original RGB-D Object Dataset ,14 scenes containing furniture (chair; coffee table; sofa; table) and a subset of the objects in the RGB-D Object Dataset (bowls; caps; cereal boxes; coffee mugs; and soda cans),indoor,real,augments RGB-D v1 scenes,51,semantic scene understanding; scene labeling,0,0,,c
RF100 (Roboflow 100),Roboflow 100: A Rich; Multi-Domain Object Detection Benchmark,2022,arXiv,https://arxiv.org/pdf/2211.13523v3,https//www.rf100.org,collection of 100 crowdsourced object detection (OD) datasets; specifically constructed by Roboflow users to represent a chosen subject of study,100 datasets; 7 imagery domains; 224;714 images; and 805 class labels with over 11;170 labelling hours. We derived RF100 from over 90;000 public datasets; 60 million public images ,,,,,Image Classification; (2D) Object Detection; (Visual) Object Tracking; Object Localization; Object Counting…,5,1,,
RoadTextVQA,Reading Between the Lanes: Text VideoQA on the Road,2023,ICDAR,https://arxiv.org/pdf/2307.03948v1,https//cvit.iiit.ac.in/research/projects/cvit-projects/roadtextvqa,dataset for the task of video question answering (VideoQA) in the context of driver assistance,3;222 driving videos collected from multiple countries; annotated with 10;500 questions; all based on text or road signs present in the driving videos,outdoor,,,,Visual Question Answering (VQA); Video Question Answering,1,1,,
RobotriX,The RobotriX: An eXtremely Photorealistic and Very-Large-Scale Indoor Dataset of Sequences with Robot Trajectories and Interactions,2019,IROS,https://arxiv.org/pdf/1901.06514v1,https//hub.com/3dperceptionlab/therobotrix,Photorealistic synthetic indoor dataset designed to enable the application of deep learning techniques to a wide variety of robotic vision problems,we were able to generate a dataset of 38 semantic classes totaling 8M stills recorded at +60 frames per second with full HD resolution,indoor,synthetic,,,Semantic Segmentation; Robotic Grasping; 3D Scene Reconstruction,1,0,,
Robo-VLN (Robotics Vision-and-Language Navigation),Hierarchical Cross-Modal Agent for Robiotics Vision-and-Language Navigation,2021,ICRA,https://github.com/zubair-irshad/zubair-irshad.github.io/blob/master/projects/resources/HCM_ICRA21.pdf,https//zubair-irshad.hub.io/projects/robo-vln.html,continuous control formulation of the VLN-CE dataset by Krantz et al ported over from Room-to-Room (R2R) dataset created by Anderson et al.,Robo-VLN is built upon Matterport3D dataset; Robo-VLN provides 3177 trajectories; each trajectory is associated with 3 instructions annotated by humans ported over from the R2R Dataset; Overall; the dataset comprises 9533 expert instruction-trajectory pairs with an average trajectory length of 326 steps compared to 55.8 in VLN-CE and 5 in R2R,indoor,,,,Vision and Language Navigation,3,1,,
Robot@Home2,Robot@Home; a robotic dataset for semantic mapping of home environments,2017,IJRR,https://journals.sagepub.com/doi/pdf/10.1177/0278364917695640,https//zenodo.org/records/10928908,collection of raw and processed sensory data from domestic settings aimed at serving as a benchmark for semantic mapping algorithms through the categorization of objects and/or rooms -> relational database that states the contextual information and data links; compatible with Standard Query Language (+ Python package for managing the database + learning resources in the form of Jupyter notebooks),87;000+ time-stamped observations gathered by a mobile robot endowed with a rig of four RGB-D cameras and a 2D laser scanner.,indoor,,,,Semantic Mapping,1,0,,
RoomR (Room Rearrangement),Visual Room Rearrangement,2021,CVPR,https://arxiv.org/pdf/2103.16544v1,https//hub.com/allenai/robothor-challenge#dataset,The task of Room Rearrangement consists on an agent exploring a room and recording objects' initial configurations. The agent is removed and the poses and states (e.g.; open/closed) of some objects in the room are changed. The agent must restore the initial configurations of all objects in the room.,6;000 distinct rearrangement settings involving 72 different object types in 120 scenes,indoor,,,,Room Rearrangement,8,0,,
RTMV,RTMV: A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis,2022,ECCV,https://arxiv.org/pdf/2205.07058v2,https//www.cs.umd.edu/~mmeshry/projects/rtmv/,A Ray-Traced Multi-View Synthetic Dataset for Novel View Synthesis,?300k images rendered from nearly 2000 complex scenes using high-quality ray tracing at high resolution (1600 × 1600 pixels); scenes from four different environments; namely Google Scanned Objects; ABC; Bricks and Amazon Berkeley,indoor,synthetic,,,Novel View Synthesis,13,1,,
RxR (Room-across-Room),Room-Across-Room: Multilingual Vision-and-Language Navigation with Dense Spatiotemporal Grounding,2020,EMNLP,https://arxiv.org/pdf/2010.07954v1,https//hub.com/google-research-datasets/RxR,multilingual dataset for Vision-and-Language Navigation (VLN) for Matterport3D environments,126k navigation instructions in English; Hindi and Telugu; and 126k navigation following demonstrations,indoor,,,,Vision and Language Navigation,50,1,,
S.MID (SeMantic InDustry),SFPNet: Sparse Focal Point Network for Semantic Segmentation on General LiDAR Point Clouds,2024,arXiv,https://arxiv.org/pdf/2407.11569v1,https//www.semanticindustry.top,dataset designed to advance the field of LiDAR semantic segmentation; specifically for robotic applications and large-scale industrial scene; dataset is based on a hybrid-solid LiDAR (Livox Mid-360),total of 38;904 frames of LiDAR data at a rate of 10 Hz across various substations; LiDAR point clouds are annotated into 25 categories under professional guidance (14 categories for single frame segmentation task),outdoor,,,,LIDAR Semantic Segmentation,1,0,,
S3DIS (Stanford 3D Indoor Scene Set),3D Semantic Parsing of Large-Scale Indoor Spaces,2016,CVPR,https://openaccess.thecvf.com/content_cvpr_2016/papers/Armeni_3D_Semantic_Parsing_CVPR_2016_paper.pdf,http//buildingparser.stanford.edu/dataset.html,new dataset of several buildings with a covered area of over 6; 000m2 and over 215 million points,over 6000m2; over 215 million points; 6 large-scale indoor areas with 271 rooms; Each point in the scene point cloud is annotated with one of the 13 semantic categories,indoor,real,colored point cloud (Matterport scanner); RGB-D images ,13,(3D) Semantic Segmentation; 3D Object Detection; (3D) Panoptic Segmentation; 3D Instance Segmentation; Few-shot 3D Point Cloud Semantic Segmentation,471,10,,c
S3E,S3E: A Mulit-Robot Multimodal Dataset for Collaborative SLAM,2022,arXiv,https://arxiv.org/pdf/2210.13723v7,https//hub.com/PengYu-Team/S3E,large-scale multimodal dataset captured by a fleet of unmanned ground vehicles along four designed collaborative trajectory paradigms,7 outdoor and 5 indoor scenes that each exceed 200 seconds; consisting of well synchronized and calibrated high-quality stereo camera; LiDAR; and high-frequency IMU data,outdoor,,,,Simultaneous Localization and Mapping,2,0,,
Scan2CAD,Scan2CAD: Learning CAD Model Alignment in RGB-D Scans,2019,CVPR,https://arxiv.org/pdf/1811.11187v1,https//hub.com/skanti/Scan2CAD,data-driven method that learns to align clean 3D CAD models from a shape database to the noisy and incomplete geometry of an RGBD scan,Scan2CAD dataset builds upon the 3D scans from ScanNet and CAD models from ShapeNet; alignment dataset based on 1506 ScanNet scans with 97607 annotated keypoints pairs between 14225 (3049 unique) CAD models from ShapeNet and their counterpart objects in the scans; top 3 annotated model classes: chairs; tables and cabinets; number of objects aligned per scene ranges from 1 to 40 with an average of 9.3,indoor,mixed,,,3D Reconstruction,66,1,,
SCAND (Socially CompliAnt Navigation Dataset),Socially CompliAnt Navigation Dataset (SCAND): A Large-Scale Dataset Of Demonstrations For Social Navigation,2022,RA-L,https://arxiv.org/pdf/2203.15041v2,https//www.cs.utexas.edu/~xiao/SCAND/SCAND.html,large-scale; first-person-view dataset of socially compliant navigation demonstrations,8.7 hours; 138 trajectories; 25 miles of socially compliant; human tele-operated driving demonstrations that comprises multi-modal data streams including 3D lidar; joystick commands; odometry; visual and inertial information; collected on two morphologically different mobile robots (a Boston Dynamics Spot and a Clearpath Jackal) by four different human demonstrators in both indoor and outdoor environments.,mixed,,,,Social Navigation,14,0,,
ScanEnts3D (Scan Entitities in 3D),ScanEnts3D: Exploiting Phrase-to-3D-Object Correspondences for Improved Visio-Linguistic Models in 3D Scenes,2024,WACV,https://arxiv.org/pdf/2212.06250v2,https//scanents3d.hub.io,connecting natural language to real-world 3D scenes; extension of ScanRefer and ReferIt3D,dataset provides explicit correspondences between 369k objects across 84k referential sentences; covering 705 real-world scenes,NLP,,,,neural listening; language generation,1,0,,
ScanNet,ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes,2017,CVPR,https://arxiv.org/pdf/1702.04405v2,http//www.scan-net.org, instance-level indoor RGB-D dataset that includes both 2D and 3D data,RGB-D video dataset containing 2.5 million views in 1513 annotated scans with an approximate 90% surface coverage,indoor,,,,Semantic Segmentation; 3D Object Detection; Depth Estimation; Panoptic Segmentation; 3D Reconstruction; Scene Segmentation; Scene Recognition; 3D Instance Segemtnation; 3D Point Cloud Classification; 3D Semantic Instance Segementation…,1369,20,,
ScanNet++,ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes,2023,ICCV,https://kaldir.vc.in.tum.de/scannetpp/,,large scale dataset with 450+ 3D indoor scenes containing sub-millimeter resolution laser scans; registered 33-megapixel DSLR images; and commodity RGB-D streams from iPhone,ScanNet++ contains 460 scenes; 280;000 captured DSLR images; and over 3.7M iPhone RGBD frames,indoor,,,,3D Semantic Segmentation,10,2,,
ScanNet200,Language-Grounded Indoor 3D Semantic Segmentation in the Wild,2022,ECCV,https://arxiv.org/pdf/2204.07761v2,https//kaldir.vc.in.tum.de/scannet_benchmark/,The ScanNet200 benchmark studies 200-class 3D semantic segmentation; The source of scene data is identical to ScanNet; but parses a larger vocabulary for semantic and instance segmentation,The source of scene data is identical to ScanNet; but parses a larger vocabulary for semantic and instance segmentation,indoor,,,,3D Semantic Segmentation; 3D Instance Segmentation; 3D Open-Vocabulary Instance Segmentation,33,3,,
ScanNet V2,ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes,2018,arXiv,https://arxiv.org/pdf/1702.04405v2,http//www.scan-net.org, instance-level indoor RGB-D dataset that includes both 2D and 3D data; ScanNet v2 = newest version of ScanNet,RGB-D video dataset containing 2.5 million views in 1513 annotated scans with an approximate 90% surface coverage,indoor,,,,Semantic Segmentation; 3D Object Detection; Depth Estimation; Panoptic Segmentation; 3D Reconstruction; Scene Segmentation; Scene Recognition; 3D Instance Segemtnation; 3D Point Cloud Classification; 3D Semantic Instance Segementation…,1369,20,,
ScanObjectNN,Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data,2019,ICCV,https://arxiv.org/pdf/1908.04616v2,https//hkust-vgd.hub.io/scanobjectnn/,Benchmark Dataset and Classification Model on Real-World Data,real-world dataset comprising of 2902 3D objects in 15 categories;  challenging point cloud classification datasets due to the background; missing parts and deformations,indoor,real,using scenes from SceneNN and ScanNet; point cloud data,15, 3D Point Cloud Classification; Zero-Shot  3D Point Cloud Classification;  3D Point Cloud Data Augmentation…,319,10,,c
ScanRefer,ScanRefer: 3D Object Localization in RGB-D Scans using Natural Language,2020,ECCV,https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123650205.pdf,hubhttps//hub.com/daveredrum/ScanRefer,task of 3D object localization in RGB-D scans using natural language descriptions; input: point cloud of a scanned 3D scene along with a free-form description of a specified target object; ScanRefer to address this task,dataset contains 51;583 descriptions of 11;046 objects from 800 ScanNet scenes,mixed,,,,Object Detection; Object Localization; Sentence Embeddings,48,0,,
SceneNet,SceneNet: Understanding Real World Indoor Scenes With Synthetic Data,2016,CVPR,https://arxiv.org/pdf/1511.07041v2,https//robotvault.bitbucket.io,dataset of labelled synthetic indoor scenes,11 Bedroom scenes with 428 objects; 15 Office scenes with 1;203 objects; 11 Kitchen scenes with 797 objects; 10 Living Room scenes with 715 objects; 10 Bathrooms with 556 objects,indoor,synthetic,,,Object Detection; Semantic Segmentation; Scene Understanding; 3D Reconstruction; Single-View 3D Reconstruction; Surface Reconstruction,17,0,,
SceneNN,SceneNN: a Scene Meshes Dataset with aNNotations,2016,3DV,https://ieeexplore.ieee.org/document/7785081,http//103.24.77.34/scenenn/home/,RGB-D scene dataset consisting of more than 100 indoor scenes,SceneNN; an RGB-D scene dataset consisting of 100 scenes,indoor,,,, 3D Instance Segmentation,59,1,,
ScribbleKITTI,Scribble-Supervised LiDAR Semantic Segmentation,2022,CVPR,https://arxiv.org/pdf/2203.08537v2,https//hub.com/ouenal/scribblekitti,scribble-annotated dataset for LiDAR semantic segmentation,based on SemanticKITTI (10 sequences; 19130 scans; 2349 million points); ScribbleKITTI contains 189 million labeled points corresponding to only 8.06% of the total point count;  19 classes (as in SemanticKITTI),outdoor,,,,(Semi-Supervised / Weakly-Supervised / 3D / LiDAR) Semantic Segmentation; Weakly Supervised 3D Point Cloud Segmentation,17,2,,
Semantic3D,Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark,2017,ISPRS,https://arxiv.org/pdf/1704.03847v1,http//www.semantic3d.net,point cloud dataset of scanned outdoor scenes ,over 3 billion points; 15 training and 15 test scenes annotated with 8 class labels; covers a range of diverse urban scenes: churches; streets; railroad tracks; squares; villages; soccer fields; castles…,outdoor,,,,Semantic Segmentation,62,1,,
SemanticKITTI,SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences,2019,ICCV,https://arxiv.org/pdf/1904.01416v3,https//www.semantic-kitti.org,large-scale outdoor-scene dataset for point cloud semantic segmentation; derived from the KITTI Vision Odometry Benchmark which it extends with dense point-wise annotations for the complete 360 field-of-view of the employed automotive LiDAR,22 sequences; 23201 point clouds for training and 20351 for testing,outdoor,,,,(3D) Semantic Segmentation; Panoptic Segmentation; 3D Semantic Scene Completion…,582,10,,
SemanticKITTI-C,Robo3D: Towards Robust and Reliable 3D Perception against Corruptions,2023,ICCV,https://arxiv.org/pdf/2303.17597v4,https//ldkong.com/Robo3D,Semantic-KITTI + simulated physically-principled corruptions on the val set; corruptions: 1) Severe weather conditions: fog; rain; snow -> back-scattering; attenuation; reflections; 2) External disturbances: bumpy surfaces; dust; insects... -> motion blur; LiDAR beam missing issues; 3) Internal sensor failure: incomplete echo; miss detection of instances with a dark color (e.g.; black car); crosstalk among multiple sensors -> deteriorates the 3D perception accuracy,SemanticKITTI + corruption,outdoor,synthetic,,,(Robust) 3D Semantic Segementation,23,1,,
SemanticPOSS,SemanticPOSS: A Point Cloud Dataset with Large Quantity of Dynamic Instances,2020,IV,https://arxiv.org/pdf/2002.09147v1,http//www.poss.pku.edu.cn/semanticposs.html,dataset for 3D semantic segmentation,2988 various and complicated LiDAR scans with large quantity of dynamic instances; The data is collected in Peking University and uses the same data format as SemanticKITTI,outdoor,,,,(3D) Semantic Segmentation; Autonomous Driving; Weakly supervised Semantic Segmentation,61,2,,
SemanticSpray,Energy-Based Detection of Adverse Weather Effects in LiDAR Data,2023,RA-L,https://ieeexplore.ieee.org/document/10143263,https//semantic-spray-dataset.hub.io,This dataset provides semantic labels for a subset of the Road Spray dataset; which contains scenes of vehicles traveling at different speeds on wet surfaces; creating a trailing spray effect,semantic labels for over 200 dynamic scenes; labeling each point in the LiDAR point clouds as background (road; vegetation; buildings; ...); foreground (moving vehicles); and noise (spray; LiDAR artifacts),outdoor,,,,Semantic Segmentation,2,0,,
SemanticSTF,3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds,2023,CVPR,https://arxiv.org/pdf/2304.00690v1,https//hub.com/xiaoaoran/SemanticSTF,adverse-weather point cloud dataset that provides dense point-level annotations and allows to study 3DSS under various adverse weather conditions; extends the STF Detection Benchmark by providing point-wise annotations of 21 semantic categories,contains 2;076 scans captured by a Velodyne HDL64 S3D LiDAR sensor from STF that cover various adverse weather conditions including 694 snowy; 637 dense-foggy; 631 light-foggy; and 114 rainy (all rainy LiDAR scans in STF); 21 semantic categories,outdoor,,,,(3D) Semantic Segmentation; LiDAR Semantic Segmentation,8,1,,
SensatUrban,Towards Semantic Segmentation of Urban-Scale 3D Point Clouds: A Dataset; Benchmarks and Challenges,2021,CVPR,https://arxiv.org/pdf/2009.03137v3,https//hub.com/QingyongHu/SensatUrban,urban-scale photogrammetric point cloud dataset,nearly three billion richly annotated points; large areas from two UK cities; covering about 6 km^2 of the city landscape; each 3D point is labeled as one of 13 semantic classes; such as ground; vegetation; car; etc..,outdoor,,,,Semantic Segmentation; Scene Understanding; 3D Semantic Segmentation,25,1,,
ShapeGlot,ShapeGlot: Learning Language for Shape Differentiation,2019,ICCV,https://arxiv.org/pdf/1905.02925v1,https//ai.stanford.edu/~optas/shapeglot/,grounding referential language in the shape of common objects,CiC (Chairs in Context); comprised of 4;511 unique chairs from ShapeNet and 78;789 referential utterances. In CiC chairs are organized into 4;054 sets of size 3 (representing contrastive communication contexts) and each utterance is intended to distinguish a chair in context,indoor,,,,Neural Language Understanding (listening); Neural Language Production (speaking),13,0,,
ShapeNet,ShapeNet: An Information-Rich 3D Model Repository,2015,arXiv,https://arxiv.org/pdf/1512.03012v1,https//shapenet.org,large scale repository for 3D CAD models developed by researchers from Stanford University,more than 3000000 models; 220000 models out of which are classified into 3135 categories (WordNet synsets); ShapeNet Parts subset contains 31693 meshes categorised into 16 common object classes (i.e. table; chair; plane etc.); Each shapes ground truth contains 2-5 parts (with a total of 50 part classes).,indoor,synthetic,raw 3D model data  from public online repositories and existing research datasets,3135,Semantic Segmentation; 3D Reconstruction; Novel View Synthesis; Single-View 3D Reconstruction; Point Cloud Generation; 3D Object Reconstruction; Point Cloud Completion; 3D Part Segmentation,1880,13,,c
ShapeNet-ViPC (View-guided Point Cloud Completion),View-Guided Point Cloud Completion,2021,CVPR,https://arxiv.org/pdf/2104.05666v2,https//hub.com/Hydrogenion/ViPC,A large-scale dataset for the point cloud completion task on the ShapeNet dataset, 38;328 objects from 13 categories; Each object has 24 sets of ground-truth data consisting of two incomplete point clouds (produced under two typical data acquisition scenarios; a view-aligned image and a complete ground-truth point cloud),indoor,,,,Point Cloud Completion,6,1,,
ShapeNetCore,ShapeNet: An Information-Rich 3D Model Repository,2015,arXiv,https://arxiv.org/pdf/1512.03012v1,https//shapenet.org,subset of the full ShapeNet dataset with single clean 3D models and manually verified category and alignment annotations,55 common object categories with about 51300 unique 3D models; The 12 object categories of PASCAL 3D+; a popular computer vision 3D benchmark dataset; are all covered by ShapeNetCore,indoor,synthetic,raw 3D model data from public online repositories and existing research datasets,55,3D Reconstruction; Single-View 3D Reconstruction; 3D Classification,174,1,,c
ShapeTalk,ShapeTalk: A Language Dataset and Framework for 3D Shape Edits and Deformations,2023,CVPR,https://openaccess.thecvf.com//content/CVPR2023/papers/Achlioptas_ShapeTalk_A_Language_Dataset_and_Framework_for_3D_Shape_Edits_CVPR_2023_paper.pdf,https//changeit3d.hub.io/#dataset, discriminative utterances produced by contrasting the shapes of common 3D objects for a variety of object classes and degrees of similarity,discriminative utterances for a total of 36;391 shapes; across 30 object classes; Overall; ShapeTalk contains 73;799 distinct contexts; and a total of 536;596 utterances,indoor,,,,Retrieval,4,0,,
SHREC'19 ,FARM: Functional Automatic Registration Method for 3D Human Bodies,2019,CGF,https://profs.scienze.univr.it/~marin/shrec19/,,benchmark to evaluate the performance of point-to-point matching pipelines when the shapes to be matched have different connectivity,We consider the concurrent presence of 1) different meshing; 2) rigid transformation in 3D space; 3) non-rigid deformations; 4) different vertex density; ranging from 5K to more than 50K; and 5) topological changes induced by mesh gluing in areas of contact,other,synthetic,,,3D Dense Shape Correspondence,18,1,,
SHREC'20 ,SHREC’20: Shape correspondence with non-isometric deformations,2020,3DOR,http://robertodyke.com/shrec2020/index2.html,,dataset of highly non-isometric non-rigid quadruped shapes with consensus-based ground-truth correspondences; they identified a set of synthetic models and real-world scans of 3D shapes and produced a set of ground truth correspondences,14 (animal -> mammal) models ,other,mixed,,,matching pairs of full-to-full and partial-to-full models,1,0,,
SLAM2REF,BIM-SLAM: Integrating BIM Models in Multi-session SLAM for Lifelong Mapping using 3D LiDAR,2023,ISARC,https://arxiv.org/pdf/2408.15870v1,https//mediatum.ub.tum.de/1459256?show_id=1743877,This dataset comprehends the 3D building information model (in IFC and Revit formats); manually elaborated based on the terrestrial laser scanner of the sequence 2 of ConSLAM; and the refined ground truth (GT) poses (in TUM format) of sessions 2; 3; 4; and 5 of the open-access ConSLAM dataset (which provides camera; LiDAR; and IMU measurements),This dataset comprehends the 3D building information model (in IFC and Revit formats); manually elaborated based on the terrestrial laser scanner of the sequence 2 of ConSLAM; and the refined ground truth (GT) poses (in TUM format) of sessions 2; 3; 4; and 5 of the open-access ConSLAM dataset (which provides camera; LiDAR; and IMU measurements).,indoor,,,,(2D / 3D / 6D) Pose Estimation; Visual Odometry; Camera Localization; Camera Relocalization; Pose Retrieval; Object SLAM; Semantic Slam…,2,0,,
SNARE (ShapeNet Annotated with Referring Expressions),Language Grounding with 3D Objects,2021,PMLR,https://arxiv.org/pdf/2107.12514v2,https//hub.com/snaredataset/snare,ShapeNet Annotated with Referring Expressions,To construct SNARE; we select a subset of 7;897 ACRONYM object models from ShapeNetSem. We obtain over 50;000 natural language referring expressions in English for these object models via Amazon Mechanical Turk (AMT),indoor,,,,ground language to 3D object models,7,0,,
SODA-A,Towards Large-Scale Small Object Detection: Survey and Benchmarks,2022,TPAMI,https://arxiv.org/pdf/2207.14096v4,https//shaunyuan22.hub.io/SODA/, large-scale benchmark specialized for small object detection task under aerial scenes,800203 instances with oriented rectangle box annotation across 9 classes; 2510 high-resolution images extracted from Google Earth,outdoor,,,,Object Detection in Aerial Images; Small Object Detection; Oriented Object Detection,3,0,,
SPACE,SPACE: A SIMULATOR FOR PHYSICAL INTERACTIONS AND CAUSAL LEARNING IN 3D,2021,ICCV,https://arxiv.org/pdf/2108.06180v1,https//hub.com/jiafei1224/SPACE,simulator for physical Interactions and causal learning in 3D environments; SPACE dataset; a synthetic video dataset in a 3D environment,There are 15;000 unique scene instances generated for the SPACE simulator; with 5;000 scenes for each category of physical interactions. From there; we collect 15;000 videos lasting 3 seconds with a frame rate of 50 frames per second (FPS); which total up to 2;250;000 frames. Besides the RGB frames; we also provide the segmentation map; optical flow map; depth map and surface normal vector map,other,synthetic,,,Semantic Object Interaction Classification,6,1,,
Spatial Commonsense Graph Dataset,Spatial Commonsense Graph for Object Localisation in Partial Scenes,2022,CVPR,https://arxiv.org/pdf/2203.05380v2,https//fgiuliari.hub.io,Dataset built from partial reconstructions of real-world indoor scenes using RGB-D sequences from ScanNet; aimed at estimating the unknown position of an object (e.g. where is the bag?) given a partial 3D scan of a scene; based on ScanNet sequences,based on ScanNet sequences; 24896 partial scenes with 19461 partial scenes to be used for training and validation; and 5435 partial scenes for testing,indoor,,,,Object Localization,1,0,,
SQA3D (Situated Question Answering in 3D Scenes),SQA3D: SITUATED QUESTION ANSWERING IN 3D SCENES,2023,ICLR,https://arxiv.org/pdf/2210.07474v5,https//sqa3d.hub.io,dataset for embodied scene understanding; where an agent needs to comprehend the scene it situates from an first person's perspective and answer questions,3 different modalities to represent a 3D scene: 3D scan; egocentric video and BEV picture; Based upon 650 scenes from ScanNet; around 6.8k unique situations; along with 20.4k descriptions and 33.4k diverse reasoning questions for these situations,indoor,,,,Question Answering; Referring Expression,23,2,,
SSCBench,SSCBench: A Large-Scale 3D Semantic Scene Completion Benchmark for Autonomous Driving,2023,arXiv,https://arxiv.org/pdf/2306.09001v2,https//hub.com/ai4ce/SSCBench,large-scale SSC benchmark in street views that facilitates the training of robust and generalizable SSC models,three subsets; including 38;562 frames for training; 15;798 frames for validation; and 12;553 frames for testing respectively; amounting totally to 66;913 frames,outdoor,,,,3D Semantic Scene Completion (from a single RGB image),3,0,,
Stanford Models (e.g. Stanford Bunny) [The Stanford 3D Scanning Repository],/,,/,/,https//graphics.stanford.edu/data/3Dscanrep/,In recent years; the number of range scanners and surface reconstruction algorithms has been growing rapidly. Many researchers; however; do not have access to scanning facilities or dense polygonal models. The purpose of this repository is to make some range data and detailed reconstructions available to the public.,6 scanned objecs (e.g. Standford Bunny),other,,,,Surface reconstruction,2,0,,
STPLS3D,STPLS3D: A Large-Scale Synthetic and Real Aerial Photogrammetry 3D Point Cloud Dataset,2022,BMVC,https://arxiv.org/pdf/2203.09065v3,https//www.stpls3d.com,large-scale aerial photogrammetry dataset with synthetic and real annotated 3D point clouds for semantic and instance segmentation tasks, richly-annotated synthetic 3D aerial photogrammetry point cloud dataset; more than 16 km^2 of landscapes and up to 18 fine-grained semantic categories; parallel dataset collected from four areas in the real environment,outdoor,mixed,,,(3D) Semantic Segmentation; (3D) Instance Segmentation; 3D Open-Vocabulary Instance-Segmentation,35,3,,
StreetLearn,The StreetLearn Environment and Dataset,2019,arXiv,https://arxiv.org/pdf/1903.01292v1,http//streetlearn.cc,An interactive; first-person; partially-observed visual environment that uses Google Street View for its photographic content and broad coverage; and give performance baselines for a challenging goal-driven navigation task,regions in New York City and Pittsburgh; Manhattan: 56K panoramic images; covering an area of 31.6 km2; Pittsburgh: 58K images; covering an area of 8.9 km by 3.9 km,outdoor,,,,Vision and Language Navigation,28,0,,
Structured3D,Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling,2020,ECCV,https://arxiv.org/pdf/1908.00222v3,https//hub.com/bertjiazheng/Structured3D,synthetic dataset with the aim of providing largescale photo-realistic images with rich 3D structure annotations for a wide spectrum of structured 3D modeling tasks,3;500 house designs with 21;835 rooms (hand-crafted by professional designers),indoor,synthetic,,,Semantic Segmentation; Image Generation; Metric Learning; Visual Localization; Room Layout Estomatimation; Indoor Localization,71,1,,
SUM,SUM: A Benchmark Dataset of Semantic Urban Meshes,2021,ISPRS,https://arxiv.org/pdf/2103.00355v2,https//3d.bk.tudelft.nl/projects/meshannotation/,benchmark dataset of semantic urban meshes which covers about 4 km2 in Helsinki (Finland),covers about 4 km2 in Helsinki (Finland); six classes: Ground; Vegetation; Building; Water; Vehicle; and Boat; based on Helsinki 3D textured meshes (covers about 12 km2) ,outdoor,,,,Semantic segmentation ,8,0,,
SUNCG,Semantic Scene Completion from a Single Depth Image,2017,CVPR,https://arxiv.org/pdf/1611.08974v1,https//sscnet.cs.princeton.edu,large-scale dataset of synthetic 3D scenes with dense volumetric annotations,45622 different scenes with realistic room and furniture layouts that are manually created though the Planner5D platform; 49884 valid floors; with contain 404058 rooms and 5697217 object instances from 2644 unique object meshes covering 84 categories,indoor,synthetic,scenes are manually created using Planner5D,84,Semantic Segmentation; Depth Estimation; Visual Navigation,184,0,,c
SUN3D ,SUN3D: A Database of Big Spaces Reconstructed Using SfM and Object Labels,2014,ICCV,https://openaccess.thecvf.com/content_iccv_2013/papers/Xiao_SUN3D_A_Database_2013_ICCV_paper.pdf,https//sun3d.cs.princeton.edu,large-scale RGB-D video database,8 annotated sequences; 415 sequences captured for 254 different spaces; in 41 different buildings; includes camera poses; number of object classes not specified,indoor,real,"RGB-D sensor (ASUS
Xtion PRO LIVE sensor)",-1,Semantic Segmentation; Pose Estimation; Depth Estimation,123,0,,c
SUN RGB-D,SUN RGB-D: A RGB-D Scene Understanding Benchmark Suite,2015,CVPR,https://openaccess.thecvf.com/content_cvpr_2015/papers/Song_SUN_RGB-D_A_2015_CVPR_paper.pdf,http//rgbd.cs.princeton.edu,The SUN RGBD dataset contains 10335 real RGB-D images of room scenes,10335 RGB-D images (similar scale as PASCAL VOC); Each RGB image has a corresponding depth and segmentation map; 700 object categories are labeled; training and testing sets contain 5285 and 5050 images; respectively; whole dataset is densely annotated and includes 146;617 2D polygons and 64;595 3D bounding boxes with accurate object orientations; as well as a 3D room layout and scene category for each image,indoor,,,,(3D) Object Detection; Semantic Segmentation; Panoptic Segmentation; Depth Estimation; Scene Segmentation; Scene Recognition; Scene Classification…,435,14,,
Swiss3DCities,Semantic Segmentation on Swiss3DCities: A Benchmark Study on Aerial Photogrammetric 3D Pointcloud Dataset,2021,Pattern Recognit. Lett.,https://arxiv.org/pdf/2012.12996v1,https//hub.com/NomokoAG/Swiss3DCities,outdoor urban 3D pointcloud dataset,total area of 2.7 km2 ; sampled from three Swiss cities with different characteristics,outdoor,,,,(3D) Semantic Segmentation; Autonomous Driving,4,0,,
Sydney Urban Objects,Unsupervised Feature Learning for Classification of Outdoor 3D Scans,2013,ACFR,https://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml,,dataset contains a variety of common urban road objects scanned with a Velodyne HDL-64E LIDAR; collected in the CBD of Sydney; Australia,631 individual scans of objects across classes of vehicles; pedestrians; signs and trees,outdoor,,,,3D Point Cloud Classification,4,1,,
SynLiDAR,Transfer Learning from Synthetic to Real LiDAR Point Cloud for Semantic Segmentation,2022,AAAI,https://arxiv.org/pdf/2107.05399v2,https//hub.com/xiaoaoran/SynLiDAR,large-scale synthetic LiDAR sequential point cloud dataset with point-wise annotations,13 sequences of LiDAR point cloud with around 20k scans (over 19 billion points and 32 semantic classes) are collected from virtual urban cities; suburban towns; neighborhood; and harbor,outdoor,synthetic,,,Unsupervised Domain Adaption; 3D Semantic Segmentation; 3D Unsupervised Domain Adaption,13,1,,
Synscapes,Synscapes: A Photorealistic Synthetic Dataset for Street Scene Parsing,2018,arXiv,https://arxiv.org/pdf/1810.08705v1,https//7dlabs.com/synscapes-overview,synthetic dataset for street scene parsing created using photorealistic rendering techniques,Synscapes consists of 25;000 RGB images in PNG format at 1440×720 resolution; Synscapes was designed to be similar in structure and content to the Cityscapes dataset -> includes all 19 of its training classes for semantic segmentation,outdoor,synthetic,,,Object Detection; Semantic Segmentation; Domain Adaption; Instance Segmentation; Image-to-Image Translation,44,1,,
SYNTHIA (SYNTHetic Collection of Imagery and Annotations),The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes,2016,CVPR,https://openaccess.thecvf.com/content_cvpr_2016/papers/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.pdf,https//synthia-dataset.net,SYNTHIA = SYNTHetic Collection of Imagery and Annotations; dataset for semantic segmentation of driving scenes,more than 213;400 synthetic images including both; random snapshots and video sequences in a virtual city;  Images are generated simulating different seasons; weather and illumination conditions from multiple view-points; Frames include pixel-level semantic annotations and depth; SYNTHIA was used to train DCNNs for the semantic segmentation of 11 (now 13) common classes in driving scenes,outdoor,synthetic,,,Semantic Segmentation; Domain Adaption; Image-to-Image Translation; Novel View Synthesis; Synthetic-to-real Translation…,516,11,,
SYNTHIA -PANO,Semantic Segmentation of Panoramic Images Using a Synthetic Dataset,2019,SPIE,https://arxiv.org/pdf/1909.00532v1,https//hub.com/Francis515/SYNTHIA-PANO,panoramic version of SYNTHIA dataset,Five sequences are included: Seqs02-summer; Seqs02-fall; Seqs04-summer; Seqs04-fall and Seqs05-summer; Panomaramic images with fine annotation for semantic segmentation,outdoor,,,,Semantic Segmentation,1,0,,
T-LESS (Texture-Less),T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects,2017,WACV,https://arxiv.org/pdf/1701.05498v1,https//cmp.felk.cvut.cz/t-less/,dataset for estimating the 6D pose; i.e. translation and rotation; of texture-less rigid objects,3 synchronized sensors: a structured-light and a time-of-flight RGB-D sensor and a high-resolution RGB camera; approximately 39K training and 10K test images from each sensor; Additionally; two types of 3D models are provided for each object; i.e. a manually created CAD model and a semi-automatically reconstructed one,other,synthetic,,,6D Pose Estimation using RGB / RGBD,83,2,,
Talk2Nav,Talk2Nav: Long-Range Vision-and-Language Navigation with Dual Attention and Spatial Memory,2020,IJRR,https://arxiv.org/pdf/1910.02029v3,https//www.trace.ethz.ch/publications/2019/talk2nav/index.html, large-scale dataset with verbal navigation instructions,10; 714 routes (navigation routes at city levels); A route from a source node to a destination node is composed of densely sampled atomic unit nodes. Each node contains a) a street-view panoramic image; b) the GPS coordinates of that location; c) the bearing angles to connecting edges (roads),outdoor,,,,Autonomous Driving; Visual Navigation; Visual and Language Navigation,2,0,,
Talk the Walk,TALK THE WALK: NAVIGATING GRIDS IN NEW YORK CITY THROUGH GROUNDED DIALOGUE,2019,ICLR,https://arxiv.org/pdf/1807.03367v3,https//hub.com/facebookresearch/talkthewalk,large-scale dialogue dataset grounded in action and perception. The task involves two agents (a “guide” and a “tourist”) that communicate via natural language in order to achieve a common goal: having the tourist navigate to a given target location,over 10k successful dialogues,outdoor,,,,Visual Navigation; Vision-Language Navigation,11,0,,
TEACh (Task-driven Embodied Agents that Chat),TEACh: Task-driven Embodied Agents that Chat,2022,AAAI,https://arxiv.org/pdf/2110.00534v3,https//hub.com/alexa/teach,dataset of human-human interactive dialogues to complete tasks in a simulated household environment,over 3;000 human--human; interactive dialogues to complete household tasks in simulation (A Commander with access to oracle information about a task communicates in natural language with a Follower. The Follower navigates through and interacts with the environment to complete tasks varying in complexity from Make Coffee to Prepare Breakfast; asking questions and getting additional information from the Commander),indoor,synthetic,,,Vision-Language Navigation,28,0,,
Text-to-3D House Model,Intelligent Home 3D: Automatic 3D-House Design from Linguistic Descriptions Only,2020,CVPR,https://arxiv.org/pdf/2003.00397v1,https//hub.com/chenqi008/HPGM,They formulate home design as a language conditioned visual content generation problem that is further divided into a floor plan generation and an interior texture (such as floor and wall) synthesis task,2;000 houses; 13;478 rooms and 873 texture images with corresponding natural language descriptions; average length of the description is 173.73 and there are 193 unique words; experiments: a) building layout generation: 1;600 pairs for training; 400 for testing; b)  texture synthesis: 503 data for training; 370 data for testing ,indoor,,,,Home Design,3,0,,
Text2Shape,Text2Shape: Generating Shapes from Natural Language by Learning Joint Embeddings,2018,arXiv,https://arxiv.org/abs/1803.08495,http//text2shape.stanford.edu,large dataset of natural language descriptions for physical 3D objects in the ShapeNet dataset; method for generating colored 3D shapes from natural language,A) they created a realistic dataset with real 3D objects and natural language descriptions -> ShapeNet table and chair object categories (with 8;447 and 6;591 instances); 75;344 natural language descriptions (5 descriptions on average per shape); B) they created a dataset of 3D geometric primitives with corresponding text descriptions -> voxelizing 6 types of primitives (cuboids; ellipsoids; cylinders; cones; pyramids; and tori) in 14 color variations and 9 size variations; generating 10 samples from each of 756 possible primitive configurations; thus creating 7560 voxelized shapes;  In total; we generate 192;602 descriptions; for an average of about 255 descriptions per primitive configuration,,,,,generating colored 3D shapes from natural language,1,0,,
TextVQA,Towards VQA Models That Can Read,2019,CVPR,https://arxiv.org/pdf/1904.08920v2,https//textvqa.org,dataset to benchmark visual reasoning based on text in images,28;408 images from OpenImages; 45;336 questions; 453;360 ground truth answers,indoor,,,,Question Answering; Visual Question Answering (VQA),265,2,,
Tinto,Tinto: Multisensor Benchmark for 3D Hyperspectral Point Cloud Segmentation in the Geosciences,2023,TGRS,https://arxiv.org/pdf/2305.09928v2,https//rodare.hzdr.de/record/2256,multi-sensor benchmark digital outcrop dataset designed to facilitate the development and validation of deep learning approaches for geological mapping; especially for non-structured 3D data like point clouds,The point cloud is dense and contains 3;242;964 labeled points,outdoor,,,,Classification; Semantic Segmentation,1,0,,
Tongji-3D (Tongji Computer Graphic (TCG)),A synthetic dataset for Visual SLAM evaluation,2020,Robot. Auton. Syst.,https://www.sciencedirect.com/science/article/pii/S0921889019301009,/,synthetic dataset for VSLAM evaluation,multiple sequences in different indoor environments; more than 15k images in total; sequence contents: 1. Camera 6-DOF pose in world coordinate system; 2. undistorted wide-angle images and fisheye images; 3. Bounding boxes for significant objects +  objects’ relative transformation in camera coordinate system; 4. Depth for each pixel in each frame; 5. Semantic/instance segmentation ; 6. Edge map of object borders; 7. Optical flow from frame FN to the next frame FN+1; 8. Camera intrinsic parameters for pinhole camera model + fisheye model; 9. Stereo image pairs for wide-angle images + fisheye images. 10. Images with motion blur effect and depth-of-field effect,outdoor,synthetic,,,VSLAM evaluation,?,,,
Toronto3D,Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of Urban Roadways,2020,CVPR,https://arxiv.org/pdf/2003.08284v3,https//hub.com/WeikaiTan/Toronto-3D,large-scale urban outdoor point cloud dataset acquired by an MLS system in Toronto; Canada; for semantic segmentation,dataset covers approximately 1 km of road; consists of about 78.3 million points; Point clouds has 10 attributes and classified in 8 labelled object classes,outdoor,,,,Semantic Segmentation; Scene Understanding; Autonomous Driving,21,1,,
TOSCA,Numerical geometry of non-rigid shapes,2008,Springer,https://tosca.cs.technion.ac.il/book/resources_data.html,,Hi-resolution three-dimensional nonrigid shapes in a variety of poses for non-rigid shape similarity and correspondence experiments,total of 80 objects; including 11 cats; 9 dogs; 3 wolves; 8 horses; 6 centaurs; 4 gorillas; 12 female figures; and two different male figures; containing 7 and 20 poses; Typical vertex count is about 50;000,,synthetic,,,3D Dense Shape Correspondence,4,0,,
Touchdown,TOUCHDOWN: Natural Language Navigation and Spatial Reasoning in Visual Street Environments,2019,CVPR,https://arxiv.org/pdf/1811.12354v7,https//hub.com/lil-lab/touchdown,corpus for executing navigation instructions and resolving spatial descriptions in visual real-world environments; The task is to follow instruction to a goal position and there find a hidden object; Touchdown the bear,For the full task and navigation-only; TOUCHDOWN includes 9;326 examples with 6;526 in the training set; 1;391 in the development set; and 1;409 in the test set. For the SDR task; TOUCHDOWN includes 9;326 unique descriptions and 25;575 examples with 17;880 for training; 3;836 for development; and 3;859 for testing,outdoor,,,,Vision and Language Navigation; Style Transfer; Text Style Transfer,19,1,,
TrajNet,An Evaluation of Trajectory Prediction Approaches and Notes on the TrajNet Benchmark,2018,arXiv,https://arxiv.org/pdf/1805.07663v6,http//trajnet.stanford.edu,TrajNet Challenge represents a large multi-scenario forecasting benchmark -> challenge consists on predicting 3161 human trajectories; observing for each trajectory 8 consecutive ground-truth values (3.2 seconds) in world plane coordinates and forecasting the following 12 (4.8 seconds); ,TrajNet is a superset of diverse datasets ->  1) BIWI Hotel (orthogonal bird’s eye flight view; moving people); 2) Crowds UCY (3 datasets; tilted bird’s eye view; camera mounted on building or utility poles; moving people); 3) MOT PETS (multisensor; different human activities) and 4) Stanford Drone Dataset (8 scenes; high orthogonal bird’s eye flight view; different agents as people; cars etc. ); for a total of 11448 trajectories,outdoor,,,,Trajectory Prediction; Trajectory Forecasting; Decision Making,12,2,,
UCY,Crowds by Example,2007,CGF,https://onlinelibrary.wiley.com/doi/10.1111/j.1467-8659.2007.01089.x,https//graphics.cs.ucy.ac.cy/research/downloads/crowd-data.html,real pedestrian trajectories with rich multi-human interaction scenarios captured at 2.5 Hz (?t=0.4s) -> real scenes used to generate synthetic crowds,three sequences (Zara01; Zara02; and UCY); taken in public spaces from top-view,outdoor,synthetic,,,Trajectory Prediction; Trajectory Forecasting; Motion Prediction,162,1,,
ULS labeled data,Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination,2023,Zenodo,https://proceedings.neurips.cc/paper_files/paper/2023/file/9708c7d3a0fef3710f33ba05a74e10b3-Paper-Conference.pdf,https//zenodo.org/records/8398853,UAV Laser Scanning data collected over neotropical forest (Paracou French Guiana); Semantic segmentation of sparse irregular point clouds for leaf/wood discrimination,Four flights of ULS data were collected at various times over the same site; with each flight covering 14;000 m² of land and consisting of approximately 10 million points; yielding a density of around 1;000 pts/m²,outdoor,,,,LIDAR Semantic Segmentation,1,1,,
UrbanLoco,UrbanLoco: A Full Sensor Suite Dataset for Mapping and Localization in Urban Scenes,2020,ICRA,https://arxiv.org/pdf/1912.09513v2,https//advdataset2019.wixsite.com/urbanloco,mapping/localization dataset collected in highly-urbanized environments with a full sensor-suite,13 trajectories collected in San Francisco and Hong Kong; covering a total length of over 40 kilometers,outdoor,,,,Point Cloud Registration; Autonomous Driving,14,0,,
UrbanScene3D,Capturing; Reconstructing; and Simulating: the UrbanScene3D Dataset,2022,ECCV,https://arxiv.org/pdf/2107.04286v3,https//vcc.tech/UrbanScene3D,large scale urban scene dataset associated with a handy simulator based on Unreal Engine 4 and AirSim; which consists of both man-made and real-world reconstruction scenes in different scales,10 synthetic and 6 real scenes; 128k high-resolution images covering 16 scenes including largescale real urban regions and synthetic cities with 136 km2 area in total,outdoor,mixed,,,Simultaneous Localization and Mapping; Depth Estimation; Autonomous Navigation; Novel View Synthesis,13,0,,
V2V4Real,V2V4Real: A Real-world Large-scale Dataset for Vehicle-to-Vehicle Cooperative Perception,2023,CVPR,https://arxiv.org/pdf/2303.07601v2,https//hub.com/ucla-mobility/v2v4real,large-scale real-world multi-modal dataset for V2V (Vehicle-to-Vehicle) perception,covers a driving area of 410 km comprising 20K LiDAR frames; 40K RGB frames; 240K annotated 3D bounding boxes for 5 classes; and HDMaps that cover all the driving routes,outdoor,,,,Domain Adaption; 3D Object Detection,20,0,,
VBR,VBR: A Vision Benchmark in Rome,2024,ICRA,https://arxiv.org/pdf/2404.11322v1,https//rvp-group.net/slam-dataset.html,This dataset presents a vision and perception research dataset collected in Rome; featuring RGB data; 3D point clouds; IMU; and GPS data + benchmark targeting visual odometry and SLAM; to advance the research in autonomous robotics and computer vision,6 datasets split into different sequences -> a) Spagna: urban; 2.045 km; 1827 s; b) Colosseum: urban; 2.159 km; 1383 s; c) Pincio: park; 2.541 km; 2064 s; d) DIAG: outdoor / indoor; 1.480 km; 1458 s; e) Campus: urban; 11.455 km; 2290 s; f) Ciampino: urban / traffic; 21.064 km; 3688 s,mixed,,,,(3D) Pose Estimation; Visual Place Recognition; Point Cloud Registration; 3D Reconstruction; Visual Tracking; Monocular Depth Estimation; Novel View Synthesis; Visual Odometry…,2,0,,
VideoNavQA,VideoNavQA: Bridging the Gap between Visual and Embodied Question Answering,2019,BMVC,https://arxiv.org/pdf/1908.04950v1,https//hub.com/catalina17/VideoNavQA,dataset contains pairs of questions and videos generated in the House3D environment,approximately 101;000 pairs of videos and questions; 28 types of questions belonging to 8 categories; with 70 possible answers,indoor,,,,Question Answering; Visual Question Answering (VQA); Scene Understanding,4,0,,
VIENA2,VIENA2: A Driving Anticipation Dataset,2018,ACCV,https://arxiv.org/pdf/1810.09044v2,https//sites.google.com/view/viena2-project/home,Large-scale driving anticipation dataset,Covers 5 generic driving scenarios; with a total of 25 distinct action classes; more than 15K full HD; 5s long videos acquired in various driving conditions; weathers; daytimes and environments; complemented with a common and realistic set of sensor measurements; more than 2.25M frames; each annotated with an action label; corresponding to 600 samples per action class,outdoor,,,,Action Anticipation; Motion Prediction; Human Motion Prediction,4,0,,
VIPeR (Viewpoint Invariant Pedestrian Recognition),Evaluating appearance models for recognition; reacquisition; and tracking,2007,PETS,https://github.com/KaiyangZhou/deep-person-reid/blob/master/torchreid/data/datasets/image/viper.py,,Dataset for viewpoint invariant pedestrian recognition (VIPeR),VIPeR dataset includes 632 people and two outdoor cameras under different viewpoints and light conditions. Each person has one image per camera and each image has been scaled to be 128×48 pixels. It provides the pose angle of each person as 0° (front); 45°; 90° (right); 135°; and 180° (back),outdoor,,,,Person Re-Identification; Metric Learning; Patch Matching,133,0,,
VIRDO,VIRDO: Visio-tactile Implicit Representations of Deformable Objects,2022,ICRA,https://arxiv.org/pdf/2202.00868v2,https//hub.com/MMintLab/VIRDO/,implicit; multi-modal; and continuous representation for deformable-elastic objects,144 deformation scenes from 6 different objects generated through MATLAB,indoor,,,,Surface Reconstruction; Deformable Object Manipulation,1,0,,
Visual Genome,Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations,2017,IJCV,https://arxiv.org/pdf/1602.07332v1,https//homes.cs.washington.edu/~ranjay/visualgenome/index.html,Visual Question Answering data in a multi-choice setting,"The Visual Genome dataset consists of all 108249 images from the intersection of MS-COCO’s 328000 images and YFCC’s 100 million images -> real-world,
non-iconic images that were uploaded onto Flickr by
users; We collected the WordNet synsets into which our 108249 images can be categorized using the
same method as ImageNet -> Visual Genome images cover 972 synsets; 1.7 million QA pairs; 17 questions per image on average; balanced distribution over 6 question types (What; Where; When; Who; Why; How); 108K images with densely annotated objects; attributes and relationships",mixed,Real,(RGB) images from Flickr,"76,340",Object Detection; Visual Question Answering (VQA); Layout-to-Image Generation; Scene Graph Generation; Visual Relationship Detection…,1226,19,,c
VIsual PERception (VIPER),Playing for Benchmarks,2017,ICCV,https://arxiv.org/pdf/1709.07322v1,https//playing-for-benchmarks.org/overview/,benchmark suite for visual perception;  benchmark is based on more than 250K high-resolution video frames; all annotated with ground-truth data for both low-level and high-level vision tasks; including optical flow; semantic instance segmentation; object detection and tracking; object-level 3D scene layout; and visual odometry,more than 250K high-resolution video frames; all annotated with ground-truth data; data was collected while driving; riding; and walking a total of 184 kilometers in diverse ambient conditions in a realistic virtual world,outdoor,,,,optical flow; semantic instance segmentation; object detection and tracking; object-level 3D scene layout; visual odometry,6,0,,
VizWiz(-VQA),VizWiz Grand Challenge: Answering Visual Questions from Blind People,2018,CVPR,https://arxiv.org/pdf/1802.08218v4,https//vizwiz.org/tasks-and-datasets/vqa/,dataset originates from a natural visual question answering setting where blind people each took an image and recorded a spoken question about it, VizWiz consists of over 31;000 visual questions originating from blind people who each took a picture using a mobile phone and recorded a spoken question about it; together with 10 crowdsourced answers per visual question,,,,,Visual Question Answering (VQA); Image Captioning,182,7,,
VLN-CE (Vision-and-Language Navigation in Continuous Environments),Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments,2020,ECCV,https://arxiv.org/pdf/2004.02857v2,https//jacobkrantz.hub.io/vlnce/#overview,Vision and Language Navigation in Continuous Environments (VLN-CE) is an instruction-guided navigation task with crowdsourced instructions; realistic environments; and unconstrained agent navigation,4475 trajectories converted from Room-to-Room train and validation splits; For each trajectory; multiple natural language instructions from Room-to-Room and a pre-computed shortest path are provided following the waypoints via low-level actions,indoor,,,,Image Generation,41,1,,
V-PCCD (simulated Visual Point Cloud Change Detection dataset),Point Cloud Change Detection With Stereo V-SLAM:Dataset; Metrics and Baseline,2022,RA-L,https://arxiv.org/pdf/2207.00246v2,https//lnexenl.hub.io/PPCA-VINS/,A simulated dataset built in Unreal Engine 4 with AirSim; Designed for visual point cloud change detection; Including GT point clouds before changes and after changes; Besides; 4 trajectories with stereo camera and IMU data are recorded for change detection task,we build a dataset using the simulated scene in Unreal Engine 4 with the AirSim plugin; We have built three scenes in good light condition; namely the original scene and two changed scenes,outdoor,synthetic,,,Change Detection,1,0,,
WADS (Winter Adverse Driving dataSet),DSOR: A Scalable Statistical Filter for Removing Falling Snow from LiDAR Point Clouds in Severe Winter Weather,2021,arXiv,https://arxiv.org/pdf/2109.07078v2,https//bitbucket.org/autonomymtu/wads/src/master/,multi-modal dataset featuring dense point-wise labeled sequential LiDAR scans collected in severe winter weather,Over 26 TB of multi modal data has been collected of which over 7 GB of LiDAR point clouds (3.6 billion points) have been labeled (semanticKITTI format); dataset introduces falling_snow and accumulated_snow along with all the semanticKITTI classes to further AV tasks like semantic and panoptic segmentation; object detection and tracking; and localization and mapping in conditions of moderate to severe snow,outdoor,,,,Object Detection; Autonomous Driving,8,0,,
Washington RGB-D,A large-scale hierarchical multi-view RGB-D object dataset,2011,ICRA,https://ieeexplore.ieee.org/document/5980382,https//rgbd-dataset.cs.washington.edu,widely used testbed in the robotic community,41;877 RGB-D images organized into 300 instances divided in 51 classes of common indoor objects (e.g. scissors; cereal box; keyboard etc),indoor,,,,Object Detection; (3D) Object Recognition,15,0,,
WaterScenes,WaterScenes: A Multi-Task 4D Radar-Camera Fusion Dataset and Benchmarks for Autonomous Driving on Water Surfaces,2024,T-ITS,https://arxiv.org/pdf/2307.06505v3,https//waterscenes.hub.io,4D Radar-Camera Fusion Dataset for Autonomous Driving on Water Surfaces description of the dataset,54;120 sets of RGB images; radar point clouds; GPS and IMU data; covering over 200;000 objects,outdoor,,,,Object Detection; (2D) Semantic Segmentation; Instance Segmentation; Panoptic Segmentation; Line Segment Detection; Point Cloud Segmentation,9,2,,
Waymo,Scalability in Perception for Autonomous Driving: Waymo Open Dataset,2020,CVPR,https://arxiv.org/pdf/1912.04838v7,http//www.waymo.com/open, large-scale multimodal camera-LiDAR dataset,1150 scenes; each span 20 seconds; consisting of synchronized and calibrated high quality LiDAR and camera data captured across a range of urban and suburban geographies; covers 76km2 when considering the diluted ego poses at a visibility of 150 meters,outdoor,real,5 LiDAR; 5 high-resolution pinhole cameras,4,(3D) Object Detection; 3D Human Post Estimation;M Multiple Object Tracking; Autonomous Driving; Video Object Detection; 3D Multi-Object Tracking,451,15,,c
WikiScenes,Towers of Babel: Combining Images; Language; and 3D Geometry for Learning Multimodal Vision,2021,ICCV,https://arxiv.org/pdf/2108.05863v1,https//www.cs.cornell.edu/projects/babel/,paired images and language descriptions capturing world landmarks and cultural sites; with associated 3D models and camera poses; derived from the massive public catalog of freely-licensed crowdsourced data in the Wikimedia Commons project,26K images of cathedrals were successfully registered in 3D,outdoor,,,,Image Captioning,3,0,,
Wild-Places,Wild-Places: A Large-Scale Dataset for Lidar Place Recognition in Unstructured Natural Environments,2023,ICRA,https://arxiv.org/pdf/2211.12732v3,https//csiro-robotics.hub.io/Wild-Places/,large-scale dataset for lidar place recognition in unstructured; natural environments,63K undistorted lidar submaps along with accurate 6DoF ground truth,outdoor,,,,3D Place Recognition; Sequential Place Recognition,11,0,,
WOD-C,Robo3D: Towards Robust and Reliable 3D Perception against Corruptions,2023,ICCV,https://arxiv.org/pdf/2303.17597v4,https//ldkong.com/Robo3D,Waymo + simulated physically-principled corruptions on the val set; corruptions: 1) Severe weather conditions: fog; rain; snow -> back-scattering; attenuation; reflections; 2) External disturbances: bumpy surfaces; dust; insects... -> motion blur; LiDAR beam missing issues; 3) Internal sensor failure: incomplete echo; miss detection of instances with a dark color (e.g.; black car); crosstalk among multiple sensors -> deteriorates the 3D perception accuracy,Waymo + corruption,outdoor,synthetic,,,3D Object Detection; (Robust) 3D Semantic Segmentation,5,1,,
WPC (Waterloo Point Cloud),Perceptual Quality Assessment of Colored 3D Point Clouds,2021,TVCG,https://arxiv.org/pdf/2111.05474v1,https//hub.com/qdushl/Waterloo-Point-Cloud-Database,dataset for subjective and objective quality assessment of point clouds,20 high quality; realistic and omnidirectional dense point clouds with diverse geometric and textural complexity; voxelized with an average number of 1.35M points and a standard deviation of 656K; We then apply downsampling; Gaussian noise; and three types of compression algorithms to create 740 distorted point clouds,,,,,Point Cloud Quality Assessment,24,1,,
XA Bin-Picking,A Convolutional Neural Network for Point Cloud Instance Segmentation in Cluttered Scene Trained by Synthetic Data Without Color,2020,IEEE Access,https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9025047,Datahttps//drive.google.com/drive/folders/1KCDS8_ZHxav5NZKhBzgEX4srf5xg7vW0,point-cloud dataset comprising both simulated and real-world scenes with three industrial parts,Synthesized scenes: 1000 training samples; real scenes as test samples; manually annotated ground truth instance labels; 20 to 30 identical types of parts randomly piled up in a scene; Each scene contains about 60;000 boundary points; Each point in the scene has instance annotations; The parts are texture-less and have no discernible color; training samples and test samples only contain the boundary points of parts,,mixed,,,3D Instance Segmentation,1,0,,
XL-R2R (Cross-lingual Room2Room),Cross-Lingual Vision-Language Navigation,2020,arXiv,https://paperswithcode.com/paper/cross-lingual-vision-language-navigation-1,https//hub.com/zzxslp/XL-VLN,built upon the R2R dataset and extends it with Chinese instructions,reserves the same splits as in R2R and thus consists of train; val-seen; and val-unseen splits with both English and Chinese instructions; and test split with English instructions only,indoor,,,,Zero-Shot Learning; Domain Adaption; Vision-Language Navigation,2,0,,
ZInd (Zillow Indoor Dataset),Zillow Indoor Dataset: Annotated Floor Plans With 360deg Panoramas and 3D Room Layouts,2021,CVPR,https://openaccess.thecvf.com//content/CVPR2021/papers/Cruz_Zillow_Indoor_Dataset_Annotated_Floor_Plans_With_360deg_Panoramas_and_CVPR_2021_paper.pdf,https//hub.com/zillow/zind,The Zillow Indoor Dataset (ZInD) provides extensive visual data that covers a real world distribution of unfurnished residential homes. It consists of primary 360º panoramas with annotated room layouts; windows; doors and openings (W/D/O); merged rooms; secondary localized panoramas; and final 2D floor plans,71;474 panoramas from 1;524 real unfurnished homes; annotations of 3D room layouts; 2D and 3D floor plans; panorama location in the floor plan; and locations of windows and door,indoor,,,,Camera Localization; Visual Localization; Room Layout Estimation; Indoor Localization,17,0,,
ZOD (Zenseact Open Dataset),Zenseact Open Dataset: A large-scale and diverse multimodal dataset for autonomous driving,2023,ICCV,https://arxiv.org/pdf/2305.02008v2,https//zod.zenseact.com,large-scale and diverse multi-modal autonomous driving (AD) dataset; created by researchers at Zenseact; It was collected over a 2-year period in 14 different European counties; using a fleet of vehicles equipped with a full sensor suite; The dataset consists of three subsets: Frames; Sequences; and Drives; designed to encompass both data diversity and support for spatiotemporal learning; sensor fusion; localization; and mapping; released under the permissive CC BY-SA 4.0 license; allowing for both commercial and non-commercial use,collected over a 2-year period in 14 different European counties; Frames consist of 100k curated camera images with two seconds of other supporting sensor data; while the 1473 Sequences and 29 Drives include the entire sensor suite for 20 seconds and a few minutes,outdoor,,,,spatiotemporal learning; sensor fusion: localization; mapping,6,0,,
